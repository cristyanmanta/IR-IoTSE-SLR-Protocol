@article{10.1145/3284316,
author = {Kolbe, Niklas and Kubler, Sylvain and Robert, J\'{e}r\'{e}my and Le Traon, Yves and Zaslavsky, Arkady},
title = {Linked Vocabulary Recommendation Tools for Internet of Things: A Survey},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3284316},
doi = {10.1145/3284316},
abstract = {The Semantic Web emerged with the vision of eased integration of heterogeneous, distributed data on the Web. The approach fundamentally relies on the linkage between and reuse of previously published vocabularies to facilitate semantic interoperability. In recent years, the Semantic Web has been perceived as a potential enabling technology to overcome interoperability issues in the Internet of Things (IoT), especially for service discovery and composition. Despite the importance of making vocabulary terms discoverable and selecting the most suitable ones in forthcoming IoT applications, no state-of-the-art survey of tools achieving such recommendation tasks exists to date. This survey covers this gap by specifying an extensive evaluation framework and assessing linked vocabulary recommendation tools. Furthermore, we discuss challenges and opportunities of vocabulary recommendation and related tools in the context of emerging IoT ecosystems. Overall, 40 recommendation tools for linked vocabularies were evaluated, both empirically and experimentally. Some of the key findings include that (i) many tools neglect to thoroughly address both the curation of a vocabulary collection and effective selection mechanisms, (ii) modern information retrieval techniques are underrepresented, and (iii) the reviewed tools that emerged from Semantic Web use cases are not yet sufficiently extended to fit today’s IoT projects.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {127},
numpages = {31},
keywords = {Internet of things, Linked vocabularies, linked open data, ontologies, open ecosystems, semantic web}
}

@inproceedings{10.1145/2430475.2430500,
author = {Wei, Qiang and Jin, Zhi},
title = {Service discovery for internet of things: a context-awareness perspective},
year = {2012},
isbn = {9781450318884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430475.2430500},
doi = {10.1145/2430475.2430500},
abstract = {In Internet of Things (IoT), functionalities of devices are encapsulated as real-world services to interact with other things or traditional web services to realize the seamless integration physical world with information world, where service discovery plays an important role. However, the resource-constrained and mobility natures of devices and intermittent disconnection of wireless network result in services in IoT having highly dynamic environment, which asks for different requirements for context-aware service discovery approach than traditional web services and brings new challenges for service discovery in IoT. In this paper, we analyze the role of context and relations with entities in IoT firstly; then combing the characteristics of data in IoT, a common ontology based context model with the ability to handle uncertainty and temporal aspects of context is proposed and Dynamic Bayesian networks (DBN) is adopted to reason about these contexts for supporting to sense current situation. Finally, based on the context model and reasoning approach proposed, we present a context-aware service discovery architecture for IoT to provide an efficient infrastructure to support user-centric and environment-aware service provision.},
booktitle = {Proceedings of the Fourth Asia-Pacific Symposium on Internetware},
articleno = {25},
numpages = {6},
keywords = {Dynamic Bayesian networks, context-aware, internet of things, service discovery},
location = {Qingdao, China},
series = {Internetware '12}
}

@inproceedings{10.1145/3365871.3365901,
author = {Bienz, Simon and Ciortea, Andrei and Mayer, Simon and Gandon, Fabien and Corby, Olivier},
title = {Escaping the Streetlight Effect: Semantic Hypermedia Search Enhances Autonomous Behavior in the Web of Things},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365901},
doi = {10.1145/3365871.3365901},
abstract = {The integration of systems of autonomous agents in Web of Things (WoT) environments is a promising approach to provide and distribute intelligence in world-wide pervasive systems. A central problem then is to enable autonomous agents to discover heterogeneous resources in large-scale, dynamic WoT environments. This is true in particular if an environment relies on open-standards and evolves rapidly requiring agents to adapt their behavior to achieve their goals. To this end, we developed a search engine for the WoT that allows autonomous agents to perform approximate search queries in order to find relevant resources in their environment in (weak) real time. The search engine crawls dynamic WoT environments to discover and index device metadata described with the W3C WoT Thing Description, and exposes a SPARQL endpoint that agents can use for approximate search. To demonstrate the feasibility of our approach, we implemented a prototype application for the maintenance of industrial robots in world-wide manufacturing systems. The prototype demonstrates that our semantic hypermedia search engine enhances the flexibility and agility of autonomous agents in the WoT.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {28},
numpages = {8},
keywords = {Autonomous Agents, Hypermedia, Search, Semantic Web, Web of Things},
location = {Bilbao, Spain},
series = {IoT '19}
}

@article{10.1145/3284763,
author = {Tran, Nguyen Khoi and Sheng, Quan Z. and Babar, M. Ali and Yao, Lina and Zhang, Wei Emma and Dustdar, Schahram},
title = {Internet of things search engine},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3284763},
doi = {10.1145/3284763},
abstract = {Tracing the complicated yet still relatively unripe area of the Internet of Things search engine---from concepts, to classification, and open issues.},
journal = {Commun. ACM},
month = {jun},
pages = {66–73},
numpages = {8}
}

@inproceedings{10.1145/3019612.3019698,
author = {Cabrera, Christian and Palade, Andrei and Clarke, Siobh\'{a}n},
title = {An evaluation of service discovery protocols in the internet of things},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019698},
doi = {10.1145/3019612.3019698},
abstract = {The IoT environment surfaces challenging requirements for service discovery, such as: services heterogeneity, mobility, scalability, security, QoS support and context management. Different protocols have been proposed to facilitate service discovery, but it is difficult to assess how well these protocols meet the IoT requirements. This paper presents an evaluation of commonly used service discovery protocols for the IoT, CoAP-SD, DNS-SD, mDNS-SD, and DDS-SD, performed against both qualitative and quantitative metrics, on a physical experimental setup. The results show the limitations and strengths of the protocols, and future research directions are discussed.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {469–476},
numpages = {8},
keywords = {CoAP-SD, DDS-SD, DNS-SD, IoT protocols, evaluation, internet of things, mDNS-SD, service discovery, service oriented computing},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/1806338.1806368,
author = {Tomassen, Stein L. and Strasunskas, Darijus},
title = {An ontology-driven approach to web search: analysis of its sensitivity to ontology quality and search tasks},
year = {2009},
isbn = {9781605586601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806338.1806368},
doi = {10.1145/1806338.1806368},
abstract = {An increasing number of recent information retrieval systems makes use of ontologies to help the users to detail queries and to come up with semantic representations of documents. A particular concern here is user-friendliness (usability) and scalability of those approaches for Web search purposes. In this paper, we present an approach where entities in an ontology are associated with domain terminology by feature vectors (FV). A FV reflects the semantic and linguistic neighbourhoods of a particular entity. The semantic neighbourhood is derived from an ontology and is based on related entities and specified properties, while linguistic neighbourhood is based on co-location of terms in a text corpus. Later, during the search process the FVs are used to filter and rerank the search results of the underlying search engine and thereby increasing the precision of the result.We elaborate on the approach and describe how the FVs are constructed. Then we report on a conducted evaluation where we analyse the sensitivity of the approach w.r.t. ontology quality and search tasks. Results indicate that the proposed approach and implemented prototype are able to improve the search results of a standard Web search engine. Furthermore, the analysis of the experiment data shows that the level of ontology specification is important for the quality of the FVs.},
booktitle = {Proceedings of the 11th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {130–138},
numpages = {9},
keywords = {feature vector construction, ontology, ontology quality, semantic search, web search},
location = {Kuala Lumpur, Malaysia},
series = {iiWAS '09}
}

@inproceedings{10.1145/3470482.3479629,
author = {Dantas, Paulo Filipe and Maia, Jos\'{e} Gilvan Rodrigues and Viana, Windson},
title = {Point and Control it! Using Computer Vision for Service Discovery to Control Smart Objects},
year = {2021},
isbn = {9781450386098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470482.3479629},
doi = {10.1145/3470482.3479629},
abstract = {The widespread of smart objects in our daily lives request the creation and analysis of new service discovery mechanisms and interaction techniques. In this work, we designed and evaluated a pointing-based interaction mechanism based on a Convolutional Neural Network classification method. We called it ViSCo (View, Scan, and Control it), which extends the openHAB service discovery mechanism of smart objets. ViSCo aggregates the users' field of view, captured by the camera of their smartphones, to reduce the service discovery results. 17 users evaluated the final solution remotely, in an environment with virtual devices. Participants used the ViSCo approach to find and control virtual devices by pointing to real objects in their homes (e.g., their TVs). System Usability Scale (SUS) survey about ViSCo results showed a good level of acceptance, with an average score of 83.97.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {153–160},
numpages = {8},
keywords = {CNN, IoT, Smart Home, computer vision, object classification},
location = {Belo Horizonte, Minas Gerais, Brazil},
series = {WebMedia '21}
}

@inproceedings{10.1145/2016551.2016558,
author = {Thoelen, Klaas and Michiels, Sam and Joosen, Wouter},
title = {On-demand attribute-based service discovery for mobile WSANs},
year = {2011},
isbn = {9781450305600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016551.2016558},
doi = {10.1145/2016551.2016558},
abstract = {Adequate service discovery is required to support dynamic composition as offered by recent light-weight service platforms for wireless sensor and actuator networks (WSANs). To provide flexible and precise service discovery in such dynamic environments, the operational context of services should be taken into account. This context can be inferred by reusing the information collected by the sensor nodes' environmental and resource monitoring services. In order to be usable, a light-weight abstraction of the contextual parameters is required to provide standardized context descriptions. Furthermore, the service discovery process itself needs to be adapted to handle the flexibility of this approach. In this paper, we present a service discovery mechanism that supports the expression of contextual constraints through the concepts of attributes and dictionaries. We report on the operation of such a service discovery solution and show its feasibility in a proof-of-concept implementation on SunSPOT. In this implementation, we integrate service and route discovery into a single process hereby reducing the required network overhead.},
booktitle = {Proceedings of the 5th International Conference on Communication System Software and Middleware},
articleno = {7},
numpages = {6},
location = {Verona, Italy},
series = {COMSWARE '11}
}

@article{10.1145/3150975,
author = {Koskela, Markus and Luukkonen, Petri and Ruotsalo, Tuukka and Sj\"{O}berg, Mats and Flor\'{e}en, Patrik},
title = {Proactive Information Retrieval by Capturing Search Intent from Primary Task Context},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3150975},
doi = {10.1145/3150975},
abstract = {A significant fraction of information searches are motivated by the user’s primary task. An ideal search engine would be able to use information captured from the primary task to proactively retrieve useful information. Previous work has shown that many information retrieval activities depend on the primary task in which the retrieved information is to be used, but fairly little research has been focusing on methods that automatically learn the informational intents from the primary task context. We study how the implicit primary task context can be used to model the user’s search intent and to proactively retrieve relevant and useful information. Data comprising of logs from a user study, in which users are writing an essay, demonstrate that users’ search intents can be captured from the task and relevant and useful information can be proactively retrieved. Data from simulations with several datasets of different complexity show that the proposed approach of using primary task context generalizes to a variety of data. Our findings have implications for the design of proactive search systems that can infer users’ search intent implicitly by monitoring users’ primary task activities.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {jul},
articleno = {20},
numpages = {25},
keywords = {Task-based information retrieval, proactive search, user intent modeling}
}

@inproceedings{10.1145/3605098.3635942,
author = {Vaidhyanathan, Karthik and Caporuscio, Mauro and Florio, Stefano and Muccini, Henry},
title = {ML-enabled Service Discovery for Microservice Architecture: a QoS Approach},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3635942},
doi = {10.1145/3605098.3635942},
abstract = {Microservice architectures have gained enormous popularity due to their ability to be dynamically added/removed, replicated, and updated according to run-time needs. However, the dynamic nature of microservices introduces uncertainty, which in turn can affect the provided Quality of Service (QoS). This calls for novel service discovery mechanisms able to adapt to the variability of the QoS attributes and further perform effective service discovery and selection. To this end, this paper combines machine learning and self-adaptation techniques to perform service discovery and selection by trading off different QoS attributes. The results of our validation on a state-of-the-art microservices exemplar show that our ML-enabled approach can perform service discovery with 35% higher effectiveness with respect to existing baselines.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {1193–1200},
numpages = {8},
keywords = {self-adaptation, machine learning, service discovery},
location = {Avila, Spain},
series = {SAC '24}
}

@inproceedings{10.1145/3109761.3158401,
author = {Potiopa, Piotr and Karwatowski, Micha\l{} and Duda, Jerzy and Sasor, Pawe\l{} and Wielgosz, Maciej and Muzykiewicz, Bart\l{}omiej},
title = {Semantic search extension based on polish wordnet relations in business document exploration},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3158401},
doi = {10.1145/3109761.3158401},
abstract = {This paper addresses the problem of building a specialized semantic search engine for documents collected in small or medium-sized enterprises. It presents the results of a project that brought together computer scientists and entrepreneurs for the purpose of providing a common perspective regarding the implementation in company practice of a search engine based on the Polish version of Word-Net semantic relations. The core functionality of the search engine module is provided along with a discussion on how to arrange semantic similarity structures so as to ensure the efficient generation of relevant search engine results. Some patterns and similarity coefficients for hyperonymy, hyponymy, holonymy and meronymy relations are presented and analyzed for the purpose of producing relationship structures. Finally, the architecture of the system that can be implemented in a company is outlined.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {34},
numpages = {7},
keywords = {NLP, information retrieval, semantic search engine, semantic similarity, wordnet},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@inproceedings{10.1145/2379756.2379757,
author = {Mayer, Simon and Guinard, Dominique and Wilde, Erik},
title = {Third International Workshop on the Web of Things (WoT 2012)},
year = {2012},
isbn = {9781450316033},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379756.2379757},
doi = {10.1145/2379756.2379757},
abstract = {Continuing the successful Web of Things workshop series, the goal of this workshop is to further explore the use of technologies and principles at the core of the Web to provide methods for a seamless integration of physical devices. In particular, our goal is to foster discussion on systems towards a real-time Web of Things and the discovery, search, and composition of services provided by Web-enabled things. This document contains all submissions to WoT 2012 that were accepted after a double-blind peer-reviewing process and cover current research in the fields of Web-based service discovery and semantic descriptions, case studies and toolkits for the Web of Things, and architectures that demonstrate the possibilities which arise when combining Web patterns and technologies with metadata and machine learning approaches to create smart environments.},
booktitle = {Proceedings of the Third International Workshop on the Web of Things},
articleno = {1},
numpages = {3},
keywords = {HTTP, REST, internet of things, web architecture, web of things},
location = {Newcastle, United Kingdom},
series = {WOT '12}
}

@article{10.1145/2814568,
author = {Leung, Kenneth Wai-Ting and Jiang, Di and Lee, Dik Lun and Ng, Wilfred},
title = {Constructing Maintainable Semantic Relation Network from Ambiguous Concepts in Web Content},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/2814568},
doi = {10.1145/2814568},
abstract = {The semantic network is a form of knowledge that represents various relationships between concepts with ambiguity. The knowledge can be employed to identify semantically related objects. It helps, for example, a recommender system to generate effective recommendations to the users. We propose to study a new semantic network, namely, the Concept Relation Network (CRN), which is efficiently constructed and maintained using existing web search engines. CRN tackles the uncertainty and dynamics of web content, and thus is optimized for many important web applications, such as social networks and search engines. It is a large semantic network for the collection, analysis, and interpretation of web content, and serves as a cornerstone for applications such as web search engines, recommendation systems, and social networks that can benefit from a large-scale knowledge base. In this article, we present two applications for CRN: (1) search engine and web analytic and (2) semantic information retrieval. Experimental results show that CRN effectively enhances these applications by considering the heterogenous and polysemous nature of web content.},
journal = {ACM Trans. Internet Technol.},
month = {feb},
articleno = {6},
numpages = {23},
keywords = {Concept network, query suggestion, search engine, semantic network, web analytic, web search}
}

@inproceedings{10.1145/3018896.3056773,
author = {Obidallah, Waeal J. and Raahemi, Bijan},
title = {A survey on web service discovery approaches},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3056773},
doi = {10.1145/3018896.3056773},
abstract = {With the growth and emergence of the Internet of Things (IoT) and API economy, the number of APIs and web services increased rapidly over the web. Web services play a significant role in today's business computing environment for the development of distributed applications across different networks. The process of finding the right web services that meet users' functional and nonfunctional requirements is crucial. Several web service discovery approaches have been proposed to facilitate the discovery process. These approaches are different, operate in multiple layers, and use various techniques to meet users' requirements. In this paper, we present a survey of different web service discovery approaches. The survey includes context-ware, user-side, clustering and recommender-based web service discovery approaches.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {156},
numpages = {8},
keywords = {clustering, context, recommendation, survey, web service discovery, web service discovery approaches, web services},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/2908446.2908496,
author = {Younan, Mina and Khattab, Sherif and Bahgat, Reem},
title = {WoTSF: A Framework for Searching in the Web of Things},
year = {2016},
isbn = {9781450340625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908446.2908496},
doi = {10.1145/2908446.2908496},
abstract = {A key challenge in the emerging Web of Things (WoT) paradigm is how the human users and machines look for meaningful and readable information in huge and dynamic datasets in real-time, whereby the datasets are presented in different formats. This paper presents a technique to construct efficient, hierarchical web indices that are efficiently kept up-to-date. Also, a framework for searching in the WoT, namely WoTSF, is proposed and experimentally evaluated using a prototype. The proposed framework was shown to present a tradeoff between search speed and result accuracy as compared to the Dyser WoT search engine.},
booktitle = {Proceedings of the 10th International Conference on Informatics and Systems},
pages = {278–285},
numpages = {8},
keywords = {Internet of Things (IoT), Web Indexing, Web Search, Web of Things (WoT)},
location = {Giza, Egypt},
series = {INFOS '16}
}

@article{10.14778/2777598.2777600,
author = {Anciaux, Nicolas and Lallali, Saliha and Popa, Iulian Sandu and Pucheral, Philippe},
title = {A scalable search engine for mass storage smart objects},
year = {2015},
issue_date = {May 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2777598.2777600},
doi = {10.14778/2777598.2777600},
abstract = {This paper presents a new embedded search engine designed for smart objects. Such devices are generally equipped with extremely low RAM and large Flash storage capacity. To tackle these conflicting hardware constraints, conventional search engines privilege either insertion or query scalability but cannot meet both requirements at the same time. Moreover, very few solutions support document deletions and updates in this context. In this paper, we introduce three design principles, namely Write-Once Partitioning, Linear Pipelining and Background Linear Merging, and show how they can be combined to produce an embedded search engine reconciling high insert/delete/update rate and query scalability. We have implemented our search engine on a development board having a hardware configuration representative for smart objects and have conducted extensive experiments using two representative datasets. The experimental results demonstrate the scalability of the approach and its superiority compared to state of the art methods.},
journal = {Proc. VLDB Endow.},
month = {may},
pages = {910–921},
numpages = {12}
}

@inproceedings{10.1145/3341325.3342015,
author = {Tran, Hien and Menouer, Tarek and Darmon, Patrice and Doucoure, Abdoulaye and Binder, Fran\c{c}ois},
title = {Smart Contracts Search Engine in Blockchain},
year = {2019},
isbn = {9781450371636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341325.3342015},
doi = {10.1145/3341325.3342015},
abstract = {Recently blockchain technology has attracted increasing attention. It provides a distributed peer-to-peer network. It also allows to enlarge the contracting space using smart contracts. Smart contract is a numeric protocol which define a promises between parties. In a blockchain platform, we can find millions of smart contracts which are created by different users (developers). The smarts contracts created by users and saved in a blockchain can be similar in functionality, even if different users use different wording. This paper presents a new search engine in the blockchain. The novelty of our search engine is to help users to checks their smart contracts by referencing some similar existing smart contracts created and saved in a blockchain platform. Our search engine allows to give each user an adaptive set of similar smart-contracts based on the user's similarity needs. Our search engine is developed in the Ethereum blockchain platform. Experiments demonstrate the potential of our search engine system under different scenarios.},
booktitle = {Proceedings of the 3rd International Conference on Future Networks and Distributed Systems},
articleno = {24},
numpages = {5},
keywords = {Blockchain technology, Elasticsearch, Engine search, Similarity, Smart contracts},
location = {Paris, France},
series = {ICFNDS '19}
}

@article{10.1145/3483382.3483394,
author = {Cantador, Iv\'{a}n and Chevalier, Max and Melucci, Massimo and Mothe, Josiane},
title = {CIRCLE 2020: the first joint conference of the information retrieval communities in Europe},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3483382.3483394},
doi = {10.1145/3483382.3483394},
abstract = {The Joint Conference of the Information Retrieval Communities in Europe (CIRCLE 2020) is the first joint conference of the French, Italian, Spanish, and Swiss information retrieval communities. Although these communities had conceived the CIRCLE conference as a meeting and networking venue, because of the COVID-19 pandemic, they had to make the conference as fully virtual event. Nonetheless, the three days of conference gathered interesting studies and research work on a wide range of topics on information retrieval, such as topic and document modelling, query and ranking refinement, information retrieval in e-government, social media, recommender systems, information retrieval evaluation, indexing and annotation, user profiling and interaction, frameworks and systems, and semantic extraction.},
journal = {SIGIR Forum},
month = {aug},
articleno = {9},
numpages = {9}
}

@inproceedings{10.1145/3079628.3079687,
author = {Hashemi, Seyyed Hadi and Kamps, Jaap},
title = {Where To Go Next? Exploiting Behavioral User Models in Smart Environments},
year = {2017},
isbn = {9781450346351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079628.3079687},
doi = {10.1145/3079628.3079687},
abstract = {There is a growing interest in using the Internet of Things (IoT) to create smart environments, which hold the promise to provide personalized experience based on the trail of user interactions with smart devices. We experiment with behavioral user models based on interactions with smart devices in a museum, and investigate the personalized recommendation of what to see after visiting an initial set of Point of Interests (POIs), a key problem in personalizing museum visits or tour guides. We have logged users' onsite physical information interactions of visits in a museum. Moreover, to have a better understanding of users' information interaction behaviors and their preferences, we have collected and studied query logs of a search engine of the same collection, and we have found similarities between users' online digital and onsite physical information interaction behaviors. We exploit user modeling based on users' different information interaction behaviors and experiment with a novel approach to a critical one-shot POI recommendation using deep neural multilayer perceptron based on explicitly given users' contextual information, and set-based extracted features using users' physical information interaction behaviors and similar users' digital information interaction behaviors. Experimental results indicates that our proposed behavioral user modeling, using both physical and digital user information interaction behaviors, improves the onsite POI recommendation baselines' performances in all common Information Retrieval evaluation metrics. Our proposed approach provides an effective way to achieve a high precision at rank 1 in onsite critical one-shot POI recommendation problem.},
booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {50–58},
numpages = {9},
keywords = {behavioral user models, human information interaction, internet of things, onsite logs, poi recommendation},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@article{10.1145/3415151,
author = {Liang, Haoran and Wu, Jun and Zheng, Xi and Zhang, Mengshi and Li, Jianhua and Jolfaei, Alireza},
title = {Fog-based Secure Service Discovery for Internet of Multimedia Things: A Cross-blockchain Approach},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3415151},
doi = {10.1145/3415151},
abstract = {The Internet of Multimedia Things (IoMT) has become the backbone of innumerable multimedia applications in various fields. The wide application of IoMT not only makes our life convenient but also brings challenges to service discovery. Service discovery aims to leverage location information and trust evidence scattered in a variety of multimedia applications to find trusted IoMT devices that can provide specific service in target areas. However, the eavesdropping and tampering to these sensitive IoMT data during the trust propagation process invalidate the service discovery process. To address these challenges, we propose Secure Service Discovery (SSD) for IoMT using cross-blockchain-enabled fog computing. To resist the tampering and eavesdropping during the trust propagation process, a scalable cross-blockchain structure consisting of multiple parallel blockchains is first proposed based on fog, in which different parallel blockchains can be orchestrated to propagate encrypted location information and trust evidence of different applications. Moreover, to enable a cross-blockchain structure to leverage encrypted location information and trust evidence to find trusted IoMT devices in preset areas, a novel privacy-preserving range query is proposed to query and aggregate trust evidence. Security analysis and simulations are carried out to demonstrate the effectiveness and security of the proposed SSD.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {dec},
articleno = {96},
numpages = {23},
keywords = {Internet of Multimedia Things (IoMT), blockchain, fog computing, privacy-preserving}
}

@inproceedings{10.1145/3582197.3582217,
author = {Xu, Yanting and Li, Zhi and Wang, Xindan},
title = {Specific Search Engine Identification Model Based on Improved TF-IDF and SVM},
year = {2023},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582197.3582217},
doi = {10.1145/3582197.3582217},
abstract = {Search engines have become one of the indispensable tools on the Internet. A specific search engine identification model can effectively identify the types of search engines. The model constructs website feature vectors according to the already mastered website samples and common website samples. And the text feature vectors calculated according to the improved TF-IDF algorithm are used to build a support vector machine (SVM)-based search engine recognition model. Finally, 2,000 website data were randomly selected from the 300,000 search engine-related website source data. After verification, the model has a high recognition rate for specific types of search engines.},
booktitle = {Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
pages = {122–126},
numpages = {5},
keywords = {Search Engine Identification, improved TF-IDF algorithm, website feature},
location = {Shanghai, China},
series = {ICIT '22}
}

@inproceedings{10.1145/2133601.2133634,
author = {Shi, Jie and Sim, Darren and Li, Yingjiu and Deng, Robert},
title = {SecDS: a secure EPC discovery service system in EPCglobal network},
year = {2012},
isbn = {9781450310918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2133601.2133634},
doi = {10.1145/2133601.2133634},
abstract = {In recent years, the Internet of Things (IOT) has drawn considerable attention from the industrial and research communities. Due to the vast amount of data generated through IOT devices and users, there is an urgent need for an effective search engine to help us make sense of this massive amount of data. With this motivation, we begin our initial works on developing a secure and efficient search engine (SecDS) based on EPC Discovery Services (EPCDS) for EPCglobal network, an integral part of IOT. SecDS is designed to provide a bridge between different partners of supply chains to share information while enabling them to find who is in possession of an item. The most important property of SecDS is: while efficiently processing user's search, it is also secure. In order to prevent unauthorized access to SecDS, an extended attribute-based access control model is proposed and implemented such that information belonging to different companies can be protected using different policies.},
booktitle = {Proceedings of the Second ACM Conference on Data and Application Security and Privacy},
pages = {267–274},
numpages = {8},
keywords = {EPC discovery service, EPCglobal network, access control, internet of things},
location = {San Antonio, Texas, USA},
series = {CODASPY '12}
}

@article{10.1145/3582900.3582913,
author = {Tamine, Lynda and Amig\'{o}, Enrique and Mothe, Josiane},
title = {Report on the 2nd Joint Conference of the Information Retrieval Communities in Europe (CIRCLE 2022)},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3582900.3582913},
doi = {10.1145/3582900.3582913},
abstract = {The Joint Conference of the Information Retrieval Communities in Europe (CIRCLE 2020) is the second joint conference of the French, Italian, and Spanish information retrieval communities. The three days of conference gathered interesting studies and research work on a wide range of topics on information retrieval, such as topic and document modeling, web search, information retrieval in e-government, social media, recommender systems, information retrieval evaluation, indexing and annotation, user profiling and interaction, frameworks and systems, and semantic extraction.It was hosted by Universit\'{e} de Toulouse, France in a holiday resort at Samatan.Date: 4--7 July, 2022.Website: https://www.irit.fr/CIRCLE/.},
journal = {SIGIR Forum},
month = {jan},
articleno = {9},
numpages = {10}
}

@inproceedings{10.1145/2379756.2379758,
author = {Butt, Talal Ashraf and Phillips, Iain and Guan, Lin and Oikonomou, George},
title = {TRENDY: an adaptive and context-aware service discovery protocol for 6LoWPANs},
year = {2012},
isbn = {9781450316033},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379756.2379758},
doi = {10.1145/2379756.2379758},
abstract = {We propose, trendy, a new registry-based Service Discovery protocol with context awareness. It uses CoAP-based RESTful web services to provide a standard interoperable interface which can be easily translated from HTTP. In addition, trendy introduces an adaptive timer and grouping mechanism to minimise control overhead and energy consumption. trendy's grouping is based on location tags to localise status maintenance traffic and to compose and offer new group based services. Our simulation results show that trendy techniques reduce the control traffic considerably and also reduce the energy consumption, while offering the optimal service selection.},
booktitle = {Proceedings of the Third International Workshop on the Web of Things},
articleno = {2},
numpages = {6},
location = {Newcastle, United Kingdom},
series = {WOT '12}
}

@article{10.1145/3092695,
author = {Tran, Nguyen Khoi and Sheng, Quan Z. and Babar, Muhammad Ali and Yao, Lina},
title = {Searching the Web of Things: State of the Art, Challenges, and Solutions},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092695},
doi = {10.1145/3092695},
abstract = {Technological advances allow more physical objects to connect to the Internet and provide their services on the Web as resources. Search engines are the key to fully utilize this emerging Web of Things, as they bridge users and applications with resources needed for their operation. Developing these systems is a challenging and diverse endeavor due to the diversity of Web of Things resources that they work with. Each combination of resources in query resolution process requires a different type of search engine with its own technical challenges and usage scenarios. This diversity complicates both the development of new systems and assessment of the state of the art. In this article, we present a systematic survey on Web of Things Search Engines (WoTSE), focusing on the diversity in forms of these systems. We collect and analyze over 200 related academic works to build a flexible conceptual model for WoTSE. We develop an analytical framework on this model to review the development of the field and its current status, reflected by 30 representative works in the area. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and an ideal WoTSE.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {55},
numpages = {34},
keywords = {Internet of Things, Search, Web of Things, discovery, retrieval}
}

@article{10.1145/2811264,
author = {Liu, Lu and Antonopoulos, Nick and Zheng, Minghui and Zhan, Yongzhao and Ding, Zhijun},
title = {A Socioecological Model for Advanced Service Discovery in Machine-to-Machine Communication Networks},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/2811264},
doi = {10.1145/2811264},
abstract = {The new development of embedded systems has the potential to revolutionize our lives and will have a significant impact on future Internet of Thing (IoT) systems if required services can be automatically discovered and accessed at runtime in Machine-to-Machine (M2M) communication networks. It is a crucial task for devices to perform timely service discovery in a dynamic environment of IoTs. In this article, we propose a Socioecological Service Discovery (SESD) model for advanced service discovery in M2M communication networks. In the SESD network, each device can perform advanced service search to dynamically resolve complex enquires and autonomously support and co-operate with each other to quickly discover and self-configure any services available in M2M communication networks to deliver a real-time capability. The proposed model has been systematically evaluated and simulated in a dynamic M2M environment. The experiment results show that SESD can self-adapt and self-organize themselves in real time to generate higher flexibility and adaptability and achieve a better performance than the existing methods in terms of the number of discovered service and a better efficiency in terms of the number of discovered services per message.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {mar},
articleno = {38},
numpages = {26},
keywords = {Machine-to-machine communication networks, service discovery, social-ecological model}
}

@inproceedings{10.1145/3322431.3325103,
author = {Morris, Joshua and Lin, Dan and Squicciarini, Anna},
title = {FriendGuard: A Friend Search Engine with Guaranteed Friend Exposure Degree},
year = {2019},
isbn = {9781450367530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322431.3325103},
doi = {10.1145/3322431.3325103},
abstract = {With the prevalence of online social networking, a large amount of studies have focused on online users' privacy. Existing work has heavily focused on preventing unauthorized access of one's personal information (e.g. locations, posts and photos). Very little research has been devoted into protecting the friend search engine, a service that allows people to explore others' friend lists. Although most friend search engines only disclose a partial view of one's friend list (e.g., k friends) or offer the ability to show all or no friends, attackers may leverage the combined knowledge from views obtained from different queries to gain a much larger social network of a targeted victim, potentially revealing sensitive information of a victim. In this paper, we propose a new friend search engine, namely FriendGuard, which guarantees the degree of friend exposure as set by users. If a user only allows k of his/her friends to be disclosed, our search engine will ensure that any attempts of discovering more friends of this user through querying the user's other friends will be a failure. The key idea underlying our search engine is the construction of a unique sub social network that is capable of satisfying query needs as well as controlling the degree of friend exposure. We have carried out an extensive experimental study and the results demonstrate both efficiency and effectiveness in our approach.},
booktitle = {Proceedings of the 24th ACM Symposium on Access Control Models and Technologies},
pages = {37–48},
numpages = {12},
keywords = {friend search, privacy preservation, social network},
location = {Toronto ON, Canada},
series = {SACMAT '19}
}

@inproceedings{10.1145/3277593.3277597,
author = {Sahlmann, Kristina and Schwotzer, Thomas},
title = {Ontology-based virtual IoT devices for edge computing},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277597},
doi = {10.1145/3277593.3277597},
abstract = {An IoT network may consist of hundreds heterogeneous devices. Some of them may be constrained in terms of memory, power, processing and network capacity. Manual network and service management of IoT devices are challenging. We propose a usage of an ontology for the IoT device descriptions enabling automatic network management as well as service discovery and aggregation. Our IoT architecture approach ensures interoperability using existing standards, i.e. MQTT protocol and Semantic Web technologies. We herein introduce virtual IoT devices and their semantic framework deployed at the edge of network. As a result, virtual devices are enabled to aggregate capabilities of IoT devices, derive new services by inference, delegate requests/responses and generate events. Furthermore, they can collect and pre-process sensor data. These tasks on the edge computing overcome the shortcomings of the cloud usage regarding siloization, network bandwidth, latency and speed. We validate our proposition by implementing a virtual device on a Raspberry Pi.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {15},
numpages = {7},
keywords = {M2M, MQTT, edge computing, internet of things, oneM2M ontology, semantic interoperability},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@article{10.14778/3476311.3476386,
author = {Stoddard, Josh and Mustafa, Adam and Goela, Naveen},
title = {Tanium reveal: a federated search engine for querying unstructured file data on large enterprise networks},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476386},
doi = {10.14778/3476311.3476386},
abstract = {Tanium Reveal is a federated search engine deployed on large-scale enterprise networks that is capable of executing data queries across billions of private data files within 60 seconds. Data resides at the edge of networks, potentially distributed on hundreds of thousands of endpoints. The anatomy of the search engine consists of local inverse indexes on each endpoint and a global communication platform called Tanium for issuing search queries to all endpoints. Reveal enables asynchronous parsing and indexing on endpoints without noticeable impact to the endpoints' primary functionality. The engine harnesses the Tanium platform, which is based on a self-organizing, fault-tolerant, scalable, linear chain communication scheme. We demonstrate a multi-tier workflow for executing search queries across a network and for viewing matching snippets of text on any endpoint. We analyze metrics for federated indexing and searching in multiple environments including a production network with 1.05 billion searchable files distributed across 4236 endpoints. While primarily focusing on Boolean, phrase, and similarity query types, Reveal is compatible with further automation (e.g., semantic classification based on machine learning). Lastly, we discuss safeguards for sensitive information within Reveal including cryptographic hashing of private text and role-based access control (RBAC).},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3096–3109},
numpages = {14}
}

@inproceedings{10.1145/3397271.3401467,
author = {Zhang, Weinan and Zhao, Xiangyu and Zhao, Li and Yin, Dawei and Yang, Grace Hui and Beutel, Alex},
title = {Deep Reinforcement Learning for Information Retrieval: Fundamentals and Advances},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401467},
doi = {10.1145/3397271.3401467},
abstract = {Information retrieval (IR) techniques, such as search, recommendation and online advertising, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. Since the widely use of mobile applications, more and more information retrieval services have provided interactive functionality and products. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information retrieval techniques, which could continuously update the information retrieval strategies according to users' real-time feedback, and optimize the expected cumulative long-term satisfaction from users. Our workshop aims to provide a venue, which can bring together academia researchers and industry practitioners (i) to discuss the principles, limitations and applications of DRL for information retrieval, and (ii) to foster research on innovative algorithms, novel techniques, and new applications of DRL to information retrieval.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2468–2471},
numpages = {4},
keywords = {deep reinforcement learning, information retrieval},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/1460412.1460487,
author = {Ostermaier, Benedikt and Elahi, B. Maryam and R\"{o}mer, Kay and Fahrmair, Michael and Kellerer, Wolfgang},
title = {Dyser: towards a real-time search engine for the web of things},
year = {2008},
isbn = {9781595939906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1460412.1460487},
doi = {10.1145/1460412.1460487},
abstract = {The increasing penetration of the real world with embedded and globally networked sensors enables the formation of a Web of Things (WoT), where high-level state information derived from sensors is embedded into Web representations of real-world entities (e.g. places, objects, creatures). A key service for the WoT is searching for entities which exhibit a certain dynamic state at the time of the query, which is a challenging problem due to the dynamic nature of the sought state information and due to the potentially huge scale of the WoT. Below we report on our initial efforts to construct such a search engine and the underlying WoT.},
booktitle = {Proceedings of the 6th ACM Conference on Embedded Network Sensor Systems},
pages = {429–430},
numpages = {2},
keywords = {real-time search engine, sensors, web of things},
location = {Raleigh, NC, USA},
series = {SenSys '08}
}

@inproceedings{10.1145/2999508.2999509,
author = {Leng, Xiaoming and Yan, Ying and Chen, Yang and Karlsson, B\"{o}rje F. and Moscibroda, Thomas},
title = {Therenow: what is happening over there, right now?},
year = {2016},
isbn = {9781450345514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2999508.2999509},
doi = {10.1145/2999508.2999509},
abstract = {Imagine you need to know what is happening at a certain location. How could you achieve that? A search engine might be a choice, but the information may be outdated since the page was crawled. Radio and TV news can broadcast in real-time, but they focus only on hot events. Social networks can also be real-time, but how to request what you want and quickly spread your question?},
booktitle = {SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications},
articleno = {21},
numpages = {1},
keywords = {computer vision, crowdsourcing, location-based services, web of things},
location = {Macau},
series = {SA '16}
}

@inproceedings{10.1109/CCGrid.2015.111,
author = {Gupta, Smrati and Muntes-Mulero, Victor and Matthews, Peter and Dominiak, Jacek and Omerovic, Aida and Aranda, Jordi and Seycek, Stepan},
title = {Risk-driven framework for decision support in cloud service selection},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.111},
doi = {10.1109/CCGrid.2015.111},
abstract = {The growth in the number of cloud computing users has led to the availability of a variety of cloud based services provided by different vendors. This has made the task of selecting a suitable set of services quite difficult. There has been a lot of research towards the development of suitable decision support system (DSS) to assist users in making an optimal selection of cloud services. However, existing decision support systems cannot address two crucial issues: firstly, the involvement of both business and technical perspectives in decision making simultaneously and, secondly, the multiple-clouds services based selection using a single DSS. In this paper, we tackle these issues in the light of solving the problem of cloud service discovery. In particular, we present the following novel contributions: Firstly, we present a critical analysis of the state-of-the-art in decision support systems. Based on our analysis, we identify critical shortcomings in the existent tools and develop the set of requirements which should be met by a potential DSS. Secondly, we present a new holistic framework for the development of DSS which allows a pragmatic description of user requirements. Additionally, the data gathering and analysis is studied as an integral part of the proposed DSS and therefore, we present concrete algorithms to assess the data for an optimal service discovery. Thirdly, we assess our framework for applicability to cloud service selection using an industrial case study. We also demonstrate the implementation and performance of our proposed framework using a prototype which serves as a proof of concept. Overall, this paper provides a novel and holistic framework for development of a multiple cloud service discovery based decision support system.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {545–554},
numpages = {10},
keywords = {cloud computing, decision support systems, risk modeling},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3267955.3269019,
author = {Zhang, Zhiyi and Lu, Edward and Li, Yanbiao and Zhang, Lixia and Yu, Tianyuan and Pesavento, Davide and Shi, Junxiao and Benmohamed, Lotfi},
title = {NDNoT: a framework for named data network of things},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3269019},
doi = {10.1145/3267955.3269019},
abstract = {The Named Data Networking (NDN) architecture provides simple solutions to the communication needs of Internet of Things (IoT) in terms of ease-of-use, security, and content delivery. To utilize the desirable properties of NDN architecture in IoT scenarios, we are working to provide an integrated framework, dubbed NDNoT, to support IoT over NDN. NDNoT provides solutions to auto configuration, service discovery, data-centric security, content delivery, and other needs of IoT application developers. Utilizing NDN naming conventions, NDNoT aims to create an open environment where IoT applications and different services can easily cooperate and work together. This poster introduces the basic components of our framework and explains how these components function together.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {200–201},
numpages = {2},
keywords = {NDN, internet of things},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@article{10.1145/3132732,
author = {Manta-Caro, Cristyan and Fern\'{a}ndez-Luna, Juan M.},
title = {Modeling and Simulating the Web of Things from an Information Retrieval Perspective},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3132732},
doi = {10.1145/3132732},
abstract = {Internet and Web technologies have changed our lives in ways we are not yet fully aware of. In the near future, Internet will interconnect more than 50 billion things in the real world, nodes will sense billions of features and properties of interest, and things will be represented by web-based, bi-directional services with highly dynamic content and real-time data. This is the new era of the Internet and the Web of Things. Since the emergence of such paradigms implies the evolution and integration of the systems with which they interact, it is essential to develop abstract models for representing and simulating the Web of Things in order to establish new approaches. This article describes a Web of Things model based on a structured XML representation. We also present a simulator whose ultimate goal is to encapsulate the expected dynamics of the Web of Things for the future development of information retrieval (IR) systems. The simulator generates a real-time collection of XML documents containing spatio-temporal contexts and textual and sensed information of highly dynamic dimensions. The simulator is characterized by its flexibility and versatility for representing real-world scenarios and offers a unique perspective for information retrieval. In this article, we evaluate and test the simulator in terms of its performance variables for computing resource consumption and present our experimentation with the simulator on three real scenarios by considering the generation variables for the IR document collection.},
journal = {ACM Trans. Web},
month = {nov},
articleno = {6},
numpages = {27},
keywords = {Discrete-event systems, information retrieval, simulation, web of things}
}

@article{10.1145/3274784.3274788,
author = {Culpepper, J. Shane and Diaz, Fernando and Smucker, Mark D.},
title = {Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018)},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3274784.3274788},
doi = {10.1145/3274784.3274788},
abstract = {The purpose of the Strategic Workshop in Information Retrieval in Lorne is to explore the long-range issues of the Information Retrieval field, to recognize challenges that are on - or even over - the horizon, to build consensus on some of the key challenges, and to disseminate the resulting information to the research community. The intent is that this description of open problems will help to inspire researchers and graduate students to address the questions, and will provide funding agencies data to focus and coordinate support for information retrieval research.},
journal = {SIGIR Forum},
month = {aug},
pages = {34–90},
numpages = {57}
}

@inproceedings{10.1145/2948992.2949000,
author = {Lopes, Cl\'{a}udio and Cabral, Bruno and Bernardino, Jorge},
title = {Personalization using Big Data Analytics Platforms},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949000},
doi = {10.1145/2948992.2949000},
abstract = {Personalization can be defined as the customization of the outputs of a system based on the collected personal information of its users. Personalization techniques rely on user information, such as interests, preferences, geographic location, etc. The data being collected is used to create a profile, and improve the relevance of the outputs presented to the user. Google's search engine, or Facebook's suggestions are examples of personalization. This paper intent to provide an overview of the concept, and pretend to answer the question: Which level of personalization can Big Data Analytics Platforms support?},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {131–132},
numpages = {2},
keywords = {Big Data, Big Data Analytics Platforms, Personalization},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@article{10.1145/3282517.3282523,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2019},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3282517.3282523},
doi = {10.1145/3282517.3282523},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {13–20},
numpages = {8}
}

@inproceedings{10.1145/3603781.3603874,
author = {Wang, Tao and Huang, Haitao and Tian, Ting and Zhou, Zhengda},
title = {A Novel Elasticsearch Encryption Scheme for Intelligent Transportation System Applications},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603874},
doi = {10.1145/3603781.3603874},
abstract = {Elasticsearch is a popular open-source search engine widely used storing, indexing, retrieving and analysing transportation data for intelligent transportation system applications. As the insufficient data protection mechanisms, the information systems and services applying Elasticsearch take greater risk of data breaching. While cryptographic techniques are widely used as an efficient method for ensuring data security, the implementation on Elasticsearch poses a significant challenge. We present a novel Elasticsearch encryption scheme for intelligent transportation system applications and propose some implementation techniques to eliminate or alleviate side effects introduces data encryption. We have implemented a prototype of our proposed scheme to assess feasibility and effectiveness. The experimental results indicate that the scheme is feasible and effective.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {531–535},
numpages = {5},
keywords = {Data encryption, Data security, Elasticsearch, Intelligent transportation system applications},
location = {Xiamen, China},
series = {CNIOT '23}
}

@inproceedings{10.1145/3368691.3368736,
author = {Albalas, Firas and Al-Soud, Majd and Mardini, Wail},
title = {A scalable and battery: aware tuning approach for constrained devices in internet of things},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368736},
doi = {10.1145/3368691.3368736},
abstract = {Service Discovery (RD) and finding Resource Directory (RD) for services in Internet of Things (IoT) is one of the fundamental tasks in providing services for constrained devices in IoT networks. Providing these services faces serious problems in the power consumption and the increasing number of devices in this kind of networks. Many approaches and efforts have nbeen made to address the problem of power consumption and when tested on a large number of nodes these approaches faced serious problems. To handle this problem, this paper proposes an approach with implementation to increasing the number of devices in an Internet of Things (IoT) providing scalable approach and at the same time keeps the network lifetime with minimal affect. The experiment shows that the proposed scalable approach improves the network lifetime with increasing the number of nodes.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {45},
numpages = {3},
keywords = {CoAP, internet of things (IoT), power consumption, scalability and COOJA, sensors},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/1871437.1871563,
author = {Punera, Kunal and Merugu, Srujana},
title = {The anatomy of a click: modeling user behavior on web information systems},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871563},
doi = {10.1145/1871437.1871563},
abstract = {The ultimate goal of information retrieval science continues to be providing relevant information to users while placing minimal cognitive load on them. The retrieval and presentation of relevant information (say, search results) as well as any dynamic system behavior (e.g., search engine re-ranking) depends acutely on estimating user intent. Hence, it is critical to use all the available information about user behavior at any stage of a search-session to accurately infer the user intent. However, the simplistic interfaces provided by search engines in order to minimize the user cognitive effort, and intrinsic limits imposed by privacy concerns, latency requirements, and other web instrumentation challenges, result in only a subset of user actions that are predictive of the search intent being captured.In this paper, we present a dynamic Bayesian network (DBN) that models user interaction with general web information systems, taking into account both observed (clicks etc.) as well as hidden (result examinations etc.) user actions. Our model goes beyond the ranked list information access paradigm and gives a solution where arbitrary context information can be incorporated in a principled fashion. To account for heterogeneity in user behavior as well as information access tasks, we further propose a bi-clustering algorithm that partitions users and tasks, and learns separate models for each bicluster. We instantiate this general DBN model for a typical static search interface comprising of a single query box and a ranked list of search results using a set of seven common user actions and various predictive state attributes. Experimental results on real-world web search log data indicate that one can obtain superior predictive performance on various session properties (such as click positions and reformulations) compared to simpler instantiations of the DBN.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {989–998},
numpages = {10},
keywords = {hidden action sequences, latent user intents, search sessions, user interaction modeling},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}

@article{10.1145/3539814.3539816,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3539814.3539816},
doi = {10.1145/3539814.3539816},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {5–10},
numpages = {6}
}

@proceedings{10.1145/3615890,
title = {GeoSearch '23: Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
year = {2023},
isbn = {9798400703522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In this 2nd International Workshop on Searching and Mining Large Collections of Geospatial Data (GeoSearch 2023), we built on the success of the previous edition and continued to bring together the art of search engine construction with both geospatial data modeling, data processing, and management to provide a forum for researchers and practitioners interested in the general topic of GeoSearch.},
location = {Hamburg, Germany}
}

@article{10.1145/3402127.3402130,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3402127.3402130},
doi = {10.1145/3402127.3402130},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {6–11},
numpages = {6}
}

@inproceedings{10.1145/3617695.3617702,
author = {Le, Quang Ba Minh and Vo, Chau Thi Ngoc},
title = {Patient Information Retrieval Based on BERT Variants and Clinical Texts in Electronic Medical Records},
year = {2023},
isbn = {9798400708015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617695.3617702},
doi = {10.1145/3617695.3617702},
abstract = {Information retrieval is a task related to search a database for the most relevant similar objects to a given query object. In medicine, patient information retrieval is important to get the patients that are the most similar to a patient being considered. The resulting data can be further used for physicians to produce adaptive treatment plans as well as for other applications such as disease classification, re-admission prediction, and stay-length prediction. Due to its significance, several traditional information retrieval approaches were applied on medical databases. However, there is still a growing need for an effective solution to patient information retrieval in the context where more and more electronic medical records and their clinical texts are captured. In this paper, we focus on this task and propose to perform local learning on the BERT-based embeddings from clinical texts of patients to achieve an effective solution. The advanced properties of BERT variants help better represent each patient using the clinical texts instead of other data types like medication codes and demographic data. The experimental results on MIMIC III database have confirmed the effectiveness of our proposed solution. Above all, the better differences between our solution and the others in F-measure are statistically significant in all the cases.},
booktitle = {Proceedings of the 2023 7th International Conference on Big Data and Internet of Things},
pages = {188–194},
numpages = {7},
location = {Beijing, China},
series = {BDIOT '23}
}

@inproceedings{10.1145/1967486.1967652,
author = {Rodriguez, B. Helena and Moissinac, Jean-Claude and Demeure, Isabelle},
title = {Multimodal instantiation of assistance services},
year = {2010},
isbn = {9781450304214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967486.1967652},
doi = {10.1145/1967486.1967652},
abstract = {Our research focuses in a multimodal model of services for the dynamic composition of interactive features in multimedia. These services primarily do complex multimedia compositions in pervasive semantics systems. We propose a new protocol to enhance UPnP [14] and Bonjour [15] device descriptions with semantic multimodal descriptions and to extend the service discovery to high-level SOA services. This protocol bases on the upper-level ontology DOLCE+DnS Ultralite in conjunction with OWL-S and provides a flexible mean for multimodal rich media composition and context-aware presentation in a SOA architecture based on W3C's Multimodal interaction framework (MMIF).},
booktitle = {Proceedings of the 12th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {934–937},
numpages = {4},
keywords = {DOLCE, MMIF, context aware, domain ontologies, multimodal architecture, pervasive computing, semantic services},
location = {Paris, France},
series = {iiWAS '10}
}

@inproceedings{10.1145/2486001.2491691,
author = {Biswas, Trisha and Chakraborti, Asit and Ravindran, Ravishankar and Zhang, Xinwen and Wang, Guoqiang},
title = {Contextualized information-centric home network},
year = {2013},
isbn = {9781450320566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486001.2491691},
doi = {10.1145/2486001.2491691},
abstract = {We deploy information-centric networks (ICN) to serve several applications including content distribution, vehicle-to-vehicle communication (V2V), home networks (homenet), and sensor networks. These applications require policy and context-based interaction between service producers and consumers. We visualize the ICN service layer as a contextualized information-centric bus (CIBUS), over which diverse sets of service producers and consumers co-exist. We develop a prototype and demonstrate several desirable features of ICN for homenets such as contextual service publishing and subscription, zero-configuration based node and service discovery, policy based routing and forwarding with name-based firewall, and device-to-device communication. Furthermore the prototype is applicable to both ad hoc and infrastructure settings, and can deal with diverse devices and services.},
booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
pages = {461–462},
numpages = {2},
keywords = {content centric networking, home networks, information-centric networks, named data networks, node discovery, policy based routing, service discovery, zero-configuration},
location = {Hong Kong, China},
series = {SIGCOMM '13}
}

@inproceedings{10.1145/2930056.2933327,
author = {Cozzolino, Vittorio},
title = {Exploiting Scattered Data in Smart Systems},
year = {2016},
isbn = {9781450343312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2930056.2933327},
doi = {10.1145/2930056.2933327},
abstract = {The Internet of Things (IoT) is slowly, but steadily, changing the way we interact with our surrounding. Smart cities, smart environments, smart buildings are just a few macroscopic examples of how smart ecosystems are increasingly involved in our daily life, each one offering a different set of information. This information's decentralization and scattering can be exploited, optimizing mobile nodes on-demand information retrieval process. We propose an approach focused on defining competence domains in smart systems where the responsibility of providing a specific information to a mobile node is defined by spatial constraints. By exploiting the interplay and duality of Cloud Computing and Fog Computing we introduce an approach to exploit data spatial allocation in smart systems to optimize mobile nodes information retrieval.},
booktitle = {Proceedings of on MobiSys 2016 PhD Forum},
pages = {19–20},
numpages = {2},
keywords = {cloud computing, fog computing, internet of things, resource virtualization, smart systems},
location = {Singapore, Singapore},
series = {Ph.D. Forum '16}
}

@inproceedings{10.1109/CCGrid.2015.98,
author = {Li, Wenbo and Wang, Peixia and Yang, Kaifei},
title = {Visualizing city events on search engine: tword the search infrustration for smart city},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.98},
doi = {10.1109/CCGrid.2015.98},
abstract = {City big data play the central role in the whole smart city system architecture, where the search engine acts as the entrance to these big data. In this paper, we focus on the topic of events management in the city. By visualizing the search results beyond the traditional page-list manner, we can provide more valuable insight of the important events occurring in the city. Our development comprises three aspects: (i) the elementary representations of one city event. Here, two manners are proposed: one for the emergency event real time detecting &amp; tracing, the other continuously aggregating the data to describe the event completely; (ii) the high order city event(s) representations, which extend along three directions: event summary, event drill-down and multi-events; (iii) the intelligent methods behind events visualization in the circumstance of heterogeneous data from IOT (internet of things) and web.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {1019–1026},
numpages = {8},
keywords = {big data, internet of things, search engine, smart city, visualization},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/253168.253185,
author = {Doszkocs, Tamas E.},
title = {IR, NLP, AI and UFOS: or IR-relevance, natural language problems, artful intelligence and user-friendly online systems},
year = {1986},
isbn = {0897911873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/253168.253185},
doi = {10.1145/253168.253185},
abstract = {User Friendly Online Searching is examined in the context of Natural Language Processing in Information Retrieval and Artificial Intelligence. Opportunities for synergetic R &amp; D are identified as the basis for Intelligent Information Retrieval and Artificial Retrieval Intelligence.},
booktitle = {Proceedings of the 9th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {49–57},
numpages = {9},
location = {Palazzo dei Congressi, Pisa, Italy},
series = {SIGIR '86}
}

@inproceedings{10.1145/3395351.3401696,
author = {Celosia, Guillaume and Cunche, Mathieu},
title = {Venom: a visual and experimental bluetooth low energy tracking system},
year = {2020},
isbn = {9781450380065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395351.3401696},
doi = {10.1145/3395351.3401696},
abstract = {The Bluetooth Low Energy (BLE) protocol is being included in mobile devices such as smartphones, headphones and smartwatches. As part of the BLE service discovery mechanism, devices announce their presences by broadcasting radio signals called advertisement packets that can be collected with off-the-shelf hardware and software. To avoid the risk of tracking based on those messages, BLE features an address randomization mechanism substituting the device MAC address with random temporary pseudonyms. However, the payload of advertisement packets still contains fields that can negate the randomization mechanism by exposing static identifiers.In this paper, we present Venom (Visual and ExperimeNtal Bluetooth Low Energy tracking systeM), an experimental tracking platform aiming to raise public awareness about physical tracking technologies and experiment privacy-preserving mechanisms. Venom tracks users by collecting advertisement packets broadcasted by their BLE-enabled devices, and displays related information.},
booktitle = {Proceedings of the 13th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {346–348},
numpages = {3},
keywords = {address randomization, bluetooth low energy, internet of things, privacy, tracking},
location = {Linz, Austria},
series = {WiSec '20}
}

@inproceedings{10.1145/2979779.2979846,
author = {Pandya, Hiren D. and Mulchandani, Girish and Patalia, Tejas P.},
title = {SearchAutomaton: Searching mechanism for multi-format data by combining indexing tools and techniques},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979846},
doi = {10.1145/2979779.2979846},
abstract = {In present days, I.T. Industry is rapidly growing there for its operational data increase significantly. Any large organization review its data effectively for performance evaluation and day to day monitoring. To provide such functionality Search Engine is required. Sometimes requirement of multi-format data retrieval is also important with compare to text retrieval. Oracle, Lucene and Terrier all having its own indexing mechanism to deal with such kind of retrieval. All these have its own limitations too. By combining oracle and Lucene or terrier indexing mechanism formulation of multi-format searching mechanism can be built and these is the main gist of these research. In SearchAutomaton master index is built by combining Oracle 11g index data and Terrier index data. With the help of Top-k algorithm and modified ranking model different indexing structures were analyzed as a master index entries. Effective retrieval is being possible through trained ranking model which give priority ranking for database item and file system item.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {67},
numpages = {4},
keywords = {Full-text Searching, Indexing, Information Retrieval, Lucene and Terrier, Oracle 11g},
location = {Bikaner, India},
series = {AICTC '16}
}

@inproceedings{10.1145/3486611.3486647,
author = {Villca-Rocha, Andrew and Zheng, Max and Duan, Chengzhu and Wang, Hongning},
title = {Towards semantic search in building sensor data},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3486647},
doi = {10.1145/3486611.3486647},
abstract = {This paper presents a search engine system for sensor time series data and metadata in the context of building management. It takes natural language queries as input and retrieves sensor time series data, ranks them with respect to their relevance to a given query, and visualizes the results as graphs. In addition, the system allows users to interact with the search results: they can define events of interest in the visualized results and search across sensor data for time series with similar shape, i.e. the search by example scheme. We leverage both a feature based cosine similarity model and DTW to find similar time series and rank them by relevance. Our quantitative evaluations and user studies demonstrate the value of this system for managing building sensor data.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {164–167},
numpages = {4},
keywords = {building search engine, natural language query, query by example},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3372938.3372958,
author = {Fagroud, Fatima Zahra and Ben Lahmar, El Habib and Amine, Mohamed and Toumi, Hicham and El Filali, Sanaa},
title = {What does mean search engine for IOT or IOT search engine},
year = {2020},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372958},
doi = {10.1145/3372938.3372958},
abstract = {One of the emerging and evolving area in computer science in the last years is internet of things (IOT), which offer great opportunities to researchers and industrialists around the world. It is a technological revolution specially an evolution of internet that can be applied in different areas such as Education, agriculture and Industry. IOT solutions are characterized by innovation and creativity aspect and serves to improve the quality of life and will make the environment intelligent (smart) and conducive to all human activity by offering advanced services based on existing or evolving technologies. With the development of IOT and the increase of the number of IOT devices, learning IOT, searching the information about IOT, track and list of the connected devices has become a necessity, which became possible with the appearance of a new type of search engine entitled IOT search engines. Our study in this paper are focalized on the issues of IOT which concern the search of connected objects and information related to internet of things. For this, we present a review on IOT search engines (Search engines for internet of things) when we define IOT search engines and the steps for it developement, their advantages and issues, and their impact in the aim to solve the issues of internet of things.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {20},
numpages = {7},
keywords = {IOT, IOT devices, search, search engine, shodan},
location = {Rabat, Morocco},
series = {BDIoT '19}
}

@inproceedings{10.1145/2905055.2905102,
author = {More, Sunil and Nighot, Mininath},
title = {AgroSearch: A Web Based Search Tool for Pomegranate Diseases and Pests Detection Using Image Processing},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905102},
doi = {10.1145/2905055.2905102},
abstract = {Uneven climate change demands improved and modern methods for agriculture domain. Also increased demand for food to accommodate global population, farmers try to multiply food production. Automation and intelligent decision system helps to accomplish this mission. AgroSearch is web based search tool help farmers to identify diseases and pests for pomegranate as well as provides remedy. User gives the query in the form of text, image and visual image click and gets results like search engine. It follows image preprocessing, feature extraction, training and testing respectively. Features extracted from images are color, color coherence vector and morphology. Clustering is done by applying K-Means algorithm. Finally classification of image using Multi-class Support Vector Machine into one of the class. Morphology feature gives better results. Accuracy of system is 84% and validated experimentally.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {44},
numpages = {6},
keywords = {Color, Color Coherence Vector, K-Means clustering, Morphology, Multi-class Support Vector Machine},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3196494.3196498,
author = {Erickson, Jeremy and Chen, Qi Alfred and Yu, Xiaochen and Lin, Erinjen and Levy, Robert and Mao, Z. Morley},
title = {No One In The Middle: Enabling Network Access Control Via Transparent Attribution},
year = {2018},
isbn = {9781450355766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196494.3196498},
doi = {10.1145/3196494.3196498},
abstract = {Commodity small networks typically rely on NAT as a perimeter defense, but are susceptible to a variety of well-known intra-network attacks, such as ARP spoofing. With the increased prevalence of oft-compromised Internet-of-Things (IoT) devices now taking up residence in homes and small businesses, the potential for abuse has never been higher. In this work, we present a novel mechanism for strongly attributing local network traffic to its originating principal, fully-compatible with existing legacy devices. We eliminate Man-in-the-Middle attacks at both the link and service discovery layers, and enable users to identify and block malicious devices from direct attacks against other endpoints. Despite the prevalence of prior work with similar goals, previous solutions have either been unsuited to non-Enterprise environments or have broken compatibility with existing network devices and therefore failed to be adopted. Our prototype imposes negligible performance overhead, runs on an inexpensive commodity router, and retains full compatibility with modern and legacy devices.},
booktitle = {Proceedings of the 2018 on Asia Conference on Computer and Communications Security},
pages = {651–658},
numpages = {8},
keywords = {arp spoofing, checkpoint, dreamcatcher, name poisoning, vnic, wpa},
location = {Incheon, Republic of Korea},
series = {ASIACCS '18}
}

@inproceedings{10.1145/3510858.3510950,
author = {Zhuang, Wei},
title = {Architecture of Knowledge Extraction System based on NLP},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510950},
doi = {10.1145/3510858.3510950},
abstract = {Knowledge extraction is to extract useful structured text information from messy free text. Under the current massive information background, it has attracted extensive attention. This paper analyzes the concept of NLP and the application process of NLP algorithm, discusses web information retrieval system, information extraction based on natural language processing and text relationship extraction, and tests the pipeline performance. The results show that the pipeline time is not linearly correlated with the size of the novel, but positively correlated.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {294–297},
numpages = {4},
location = {Changsha, China},
series = {ICASIT 2021}
}

@article{10.1145/3672089.3672090,
author = {Neumann, Peter G.},
title = {ACM Risks Forum Quarterly Summary},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3672089.3672090},
doi = {10.1145/3672089.3672090},
abstract = {This is an annotated summary of the main items contributed to the ACM Risks Forum in the most recent quarter, orgnanized categorically. References (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Marshall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {3–12},
numpages = {10}
}

@inproceedings{10.1145/2800835.2801655,
author = {Wanigasekara, Nirandika},
title = {A semi lazy bandit approach for intelligent service discovery in IoT applications},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2801655},
doi = {10.1145/2800835.2801655},
abstract = {Smart devices with incompatible protocols and APIs are increasing in everyday environments. Adaptive middleware techniques have enabled smart phones to become smart gateways between these incompatible devices by providing protocol translation services at runtime. Thus, mobile apps can easily use remote services to execute the app logic, which typically integrates services in a distributed mobile environment. However, as the number of services increase, and the user's context changes, it is challenging for mobile apps to discover relevant device services due to incompatible protocols and unknown devices. A smart agent is needed to guide service composition in mobile apps with respect to the user's changing context, so that mobile apps can seamlessly integrate services from different networks and execute complex tasks. To this end, I present a novel approach for context-aware service discovery based on semi-lazy bandit algorithms, and, present a functioning prototype as a microservice, which shows encouraging first results.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {503–508},
numpages = {6},
keywords = {context-aware service discovery, contextual bandits},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/3512729.3533011,
author = {Ribiero, Ricardo and Trifan, Alina and Neves, Antonio J. R.},
title = {MEMORIA: A Memory Enhancement and MOment RetrIeval Application for LSC 2022},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533011},
doi = {10.1145/3512729.3533011},
abstract = {Research on retrieving data and analyzing lifelogs revealed to be a very complex task, and the interdisciplinary challenges to be tackled have boosted increasing attention from the scientific community in information retrieval and lifelogging. The Lifelog Search Challenge is an international competition for lifelog retrieval in which researchers propose their approaches and compete to solve lifelog retrieval challenges and evaluate the effectiveness of their systems. In this paper, we present the MEMORIA computational tool to participate for the first time in the Lifelog Search Challenge 2022. The information retrieval is based on the search of keywords and time periods and several computer vision methods are used to process visual lifelogs, from pre-processing algorithms to feature extraction methods, in order to enrich the annotation of the lifelogs. Preliminary experimental results of the user interaction with our retrieval module are presented, confirming the effectiveness of the proposed approach and showing the most relevant functionalities of the system.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {8–13},
numpages = {6},
keywords = {data retrieval, image annotation, image processing, information systems, lifelog, lifelogging, machine learning, object detection},
location = {Newark, NJ, USA},
series = {LSC '22}
}

@article{10.1145/3458553.3458556,
author = {Olteanu, Alexandra and Garcia-Gathright, Jean and de Rijke, Maarten and Ekstrand, Michael D. and Roegiest, Adam and Lipani, Aldo and Beutel, Alex and Olteanu, Alexandra and Lucic, Ana and Stoica, Ana-Andreea and Das, Anubrata and Biega, Asia and Voorn, Bart and Hauff, Claudia and Spina, Damiano and Lewis, David and Oard, Douglas W. and Yilmaz, Emine and Hasibi, Faegheh and Kazai, Gabriella and McDonald, Graham and Haned, Hinda and Ounis, Iadh and van der Linden, Ilse and Garcia-Gathright, Jean and Baan, Joris and Lau, Kamuela N. and Balog, Krisztian and de Rijke, Maarten and Sayed, Mahmoud and Panteli, Maria and Sanderson, Mark and Lease, Matthew and Ekstrand, Michael D. and Lahoti, Preethi and Kamishima, Toshihiro},
title = {FACTS-IR: fairness, accountability, confidentiality, transparency, and safety in information retrieval},
year = {2021},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3458553.3458556},
doi = {10.1145/3458553.3458556},
abstract = {The purpose of the SIGIR 2019 workshop on Fairness, Accountability, Confidentiality, Transparency, and Safety (FACTS-IR) was to explore challenges in responsible information retrieval system development and deployment. To this end, the workshop aimed to crowd-source from the larger SIGIR community and draft an actionable research agenda on five key dimensions of responsible information retrieval: fairness, accountability, confidentiality, transparency, and safety. Such an agenda can guide others in the community that are interested in pursuing FACTS-IR research, as well as inform potential funders about relevant research avenues. The workshop brought together a diverse set of researchers and practitioners interested in contributing to the development of a technical research agenda for responsible information retrieval.},
journal = {SIGIR Forum},
month = {mar},
pages = {20–43},
numpages = {24}
}

@inproceedings{10.1145/3397482.3450621,
author = {Fu, Zuohui and Xian, Yikun and Zhang, Yongfeng and Zhang, Yi},
title = {IUI 2021 Tutorial on Conversational Recommendation Systems},
year = {2021},
isbn = {9781450380188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397482.3450621},
doi = {10.1145/3397482.3450621},
abstract = {Recent years have witnessed the emerging of conversational systems, including both physical devices and mobile-based applications. Both the research community and industry believe that conversational systems will have a major impact on human-computer interaction, and specifically, the CHI/IR/DM/RecSys communities have begun to explore Conversational Recommendation Systems. Conversational recommendation aims at finding or recommending the most relevant information (e.g., web pages, answers, movies, products) for users based on textual- or spoken-dialogs, through which users can communicate with the system more efficiently using natural language conversations. Due to users’ constant need to look for information to support both work and daily life, conversational recommendation system will be one of the key techniques towards an intelligent web. The tutorial focuses on the foundations and algorithms for conversational recommendation, as well as their applications in real-world systems such as search engine, e-commerce and social networks. The tutorial aims at introducing and communicating conversational recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
booktitle = {Companion Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {1–2},
numpages = {2},
keywords = {Conversational Recommendation, Dialog Systems, Human-in-the-Loop AI, Intelligent Interface, Natural Language Processing},
location = {College Station, TX, USA},
series = {IUI '21 Companion}
}

@inproceedings{10.1145/3545729.3545748,
author = {Liu, Chen and Wang, William Yu Chung and Khan, Gohar},
title = {Digital Public Health Surveillance with Online Health Consultation Data: An Example of HIV Monitoring},
year = {2022},
isbn = {9781450396301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545729.3545748},
doi = {10.1145/3545729.3545748},
abstract = {There is increasing interest in obtaining electronic information from sources other than official public health organizations for public health surveillance. The potential of novel digital data sources, such as internet news, search engine, social media, mobile apps and wearable devices in improving the speed, scope and temporal precision of disease surveillance have been demonstrated. However, the value of Online Health Consultation (OHC) data in public health surveillance is ignored. This study aims to assess the predictive value of OHC data in public health surveillance. This study constructs a public health surveillance system for HIV with OHC data based on the medical entities extraction and disease prediction two-module framework. This research outcome will contribute to closing the knowledge gap in the research area of digital public health surveillance and provide inspiration for relevant scholars to make better use of OHC data in their research.},
booktitle = {Proceedings of the 6th International Conference on Medical and Health Informatics},
pages = {89–93},
numpages = {5},
keywords = {digital surveillance, online health consultation, public health surveillance},
location = {Virtual Event, Japan},
series = {ICMHI '22}
}

@inproceedings{10.1145/3383313.3411548,
author = {Fu, Zuohui and Xian, Yikun and Zhang, Yongfeng and Zhang, Yi},
title = {Tutorial on Conversational Recommendation Systems},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3411548},
doi = {10.1145/3383313.3411548},
abstract = {Recent years have witnessed the emerging of conversational systems, including both physical devices and mobile-based applications. Both the research community and industry believe that conversational systems will have a major impact on human-computer interaction, and specifically, the RecSys community has begun to explore Conversational Recommendation Systems. Conversational recommendation aims at finding or recommending the most relevant information (e.g., web pages, answers, movies, products) for users based on textual- or spoken-dialogs, through which users can communicate with the system more efficiently using natural language conversations. Due to users’ constant need to look for information to support both work and daily life, conversational recommendation system will be one of the key techniques towards an intelligent web. The tutorial focuses on the foundations and algorithms for conversational recommendation, as well as their applications in real-world systems such as search engine, e-commerce and social networks. The tutorial aims at introducing and communicating conversational recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {751–753},
numpages = {3},
keywords = {Conversational Recommendation, Dialog Systems},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/1993966.1993984,
author = {Bian, Li and Shilkrot, Roy},
title = {PalimPost: information convergence using sticky notes},
year = {2011},
isbn = {9781450306249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993966.1993984},
doi = {10.1145/1993966.1993984},
abstract = {In today's world, the digital information retrieval experience is inherently a sparse device-centric activity. Users rely on the ability of the currently used device to supply the requested information, in some disconnection from past activities on other devices. There is a growing need to develop new methods of connecting cross-context information retrieval sessions. We present PalimPost, a converged system for storing, searching, and sharing digital and physical world information using sticky notes and mobile devices. PalimPost extracts contextual cues from a user's physical environment and activities, and connects them to the user's digital world research. Subsequently, the system presents systematically categorized information that is relevant to the moment of interaction in a just-in-time manner. PalimPost uses physical sticky notes with embedded QR codes, as well as virtual sticky notes on mobile devices. The system incorporates Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), and Natural Language Processing (NLP) techniques for understanding and categorizing the content.},
booktitle = {Proceedings of the Second International Workshop on Web of Things},
articleno = {13},
numpages = {6},
keywords = {Internet of Things, augmented reality, cloud computing, embedded information technology, tangible user interfaces},
location = {San Francisco, California, USA},
series = {WoT '11}
}

@inproceedings{10.1145/3437963.3441661,
author = {Fu, Zuohui and Xian, Yikun and Zhang, Yongfeng and Zhang, Yi},
title = {WSDM 2021 Tutorial on Conversational Recommendation Systems},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441661},
doi = {10.1145/3437963.3441661},
abstract = {Recent years have witnessed the emerging of conversational systems, including both physical devices and mobile-based applications. Both the research community and industry believe that conversational systems will have a major impact on human-computer interaction, and specifically, the IR/DM/RecSys communities have begun to explore Conversational Recommendation Systems. Conversational recommendation aims at finding or recommending the most relevant information (e.g., web pages, answers, movies, products) for users based on textual- or spoken-dialogs, through which users can communicate with the system more efficiently using natural language conversations. Due to users' constant need to look for information to support both work and daily life, conversational recommendation system will be one of the key techniques towards an intelligent web. The tutorial focuses on the foundations and algorithms for conversational recommendation, as well as their applications in real-world systems such as search engine, e-commerce and social networks. The tutorial aims at introducing and communicating conversational recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {1134–1136},
numpages = {3},
keywords = {conversational recommendation, dialog systems},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@article{10.1145/3417564.3417566,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3417564.3417566},
doi = {10.1145/3417564.3417566},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {11–16},
numpages = {6},
keywords = {cyber-physical systems, software engineering}
}

@inproceedings{10.4108/icst.urb-iot.2014.257310,
author = {Rothenpieler, Peter and Altakrouri, Bashar and Kleine, Oliver and Ruge, Lukas},
title = {Distributed crowd-sensing infrastructure for personalized dynamic IoT spaces},
year = {2014},
isbn = {9781631900372},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.urb-iot.2014.257310},
doi = {10.4108/icst.urb-iot.2014.257310},
abstract = {In this paper, we describe a distributed crowd-sensing infrastructure that integrates and bridges small scale personalized ad-hoc Internet of Things (IoT) spaces (consisting of personal interconnected smart devices, sensors and actuators dynamically deployed at runtime) to large scale IoT spaces. While a lot of innovation takes place on large scale IoT infrastructures, we focus on a personalized IoT infrastructure that allows user level control and management of personally owned IoT resources. Our approach uses a peer to peer (P2P) network together with distributed discovery- and directory-services, without the need for centralized infrastructure. The contribution of this paper is twofold: Firstly, we present an Android-based solution called Ambient Bridge that exposes a user-selected subset of the build-in sensors and actuators of a smart device as CoAP (Constrained Application Protocol) web services. Moreover, it is used to dynamically integrate external sensors and actuators at runtime that are normally only accessible via proprietary or non-networked interfaces. Secondly, we present a directory service and distributed semantic search engine called the Smart Service Proxy (SSP). The SSP allows application developers to search for sensors and actuators using SPARQL queries, which are automatically distributed between and processed by the cooperating SSPs.},
booktitle = {Proceedings of the First International Conference on IoT in Urban Space},
pages = {90–92},
numpages = {3},
keywords = {CoAP, ambient dynamix, distributed crowd-sensing, distributed fusion, internet of things, smart service proxy},
location = {Rome, Italy},
series = {URB-IOT '14}
}

@article{10.1145/3310013.3310021,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3310013.3310021},
doi = {10.1145/3310013.3310021},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {11–17},
numpages = {7}
}

@article{10.1145/3573074.3573077,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3573074.3573077},
doi = {10.1145/3573074.3573077},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {7–12},
numpages = {6}
}

@article{10.1145/3468744.3468746,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3468744.3468746},
doi = {10.1145/3468744.3468746},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {8–13},
numpages = {6}
}

@article{10.1145/3325642.3325643,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/3325642.3325643},
doi = {10.1145/3325642.3325643},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {aug},
pages = {6–12},
numpages = {7}
}

@article{10.1145/3502771.3502773,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3502771.3502773},
doi = {10.1145/3502771.3502773},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {5–7},
numpages = {3}
}

@article{10.1145/3635439.3635440,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3635439.3635440},
doi = {10.1145/3635439.3635440},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {dec},
pages = {4–9},
numpages = {6}
}

@article{10.1145/3485952.3485954,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3485952.3485954},
doi = {10.1145/3485952.3485954},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {8–13},
numpages = {6}
}

@article{10.1145/3650142.3650143,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/3650142.3650143},
doi = {10.1145/3650142.3650143},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {apr},
pages = {3–8},
numpages = {6}
}

@article{10.1145/3561846.3561850,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3561846.3561850},
doi = {10.1145/3561846.3561850},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {9–15},
numpages = {7}
}

@inproceedings{10.1145/3361149.3361174,
author = {Dobaj, J\"{u}rgen and Schuss, Markus and Krisper, Michael and Boano, Carlo Alberto and Macher, Georg},
title = {Dependable mesh networking patterns},
year = {2019},
isbn = {9781450362061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361149.3361174},
doi = {10.1145/3361149.3361174},
abstract = {In our daily life, we are increasingly relying on connected systems ranging from smart health care devices to industrial and intelligent transportation systems, as well as smart homes and cities. The unavailability or malfunctioning of these systems could threaten human life, cause environmental damage, and significant financial loss. To prevent such large scale and mission-critical systems from malfunctioning, it is of utmost importance to establish and guaranty reliable connections to attain a dependable networked system. Generally, mesh networking technologies are used for building such systems since mesh networks provide the best performance characteristics regarding fault-tolerance, throughput, resource usage, and service level flexibility.In this paper, we summarize the major challenges in dependable network design, to subsequently present three patterns that approach redundancy on the hardware level, software-defined networking, and cross-cutting concerns like monitoring and service discovery within distributed networked systems. These three patterns should help designers and engineers in choosing the appropriate technologies for building dependable networked systems at all scales. Since dependable network engineering requires a holistic system-wide design and engineering approach, we also present a pattern map guiding to complementary and closely related patterns. System architects and system engineers responsible for building mixed-criticality systems, internet-of-things (IoT), and industrial Internet-of-Things (IIoT) systems are the target audience of the patterns presented in this paper.},
booktitle = {Proceedings of the 24th European Conference on Pattern Languages of Programs},
articleno = {25},
numpages = {14},
keywords = {IIoT, IaaS, IoT, SoA, cloud architecture, dependability, industry 4.0, infrastructure-as-a-service, logical mesh pattern, microservices, network architecture, network design, physical mesh pattern, service mesh pattern, service-oriented architecture},
location = {Irsee, Germany},
series = {EuroPLop '19}
}

@inproceedings{10.1145/3317549.3326297,
author = {Celosia, Guillaume and Cunche, Mathieu},
title = {Himiko: A human interface for monitoring and inferring knowledge on bluetooth-low-energy objects: demo},
year = {2019},
isbn = {9781450367264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317549.3326297},
doi = {10.1145/3317549.3326297},
abstract = {The Bluetooth Low Energy (BLE) protocol is being included in a growing number of connected objects such as smartphones, fitness trackers, headphones and smartwatches. As part of the service discovery mechanism of BLE, devices announce themselves by broadcasting radio signals called advertisement packets that can be collected with off-the-shelf hardware and software. To avoid the risk of tracking based on those messages, BLE features an address randomization mechanism that substitutes the device MAC address with random temporary pseudonyms. However, the payload of the advertisement packet still contains fields that can hamper the randomization mechanism by exposing counters and static identifiers. In addition to defeating the randomization mechanism, some of these fields can leak sensitive attributes of the owner such as his medical condition.As a consequence, we implemented Himiko to raise awareness about the privacy issues that the BLE advertising mechanism can involve. This tool aims to show the information that a passive eavesdropper can infer by leveraging the contents of BLE advertisement packets. The advertising raw data are collected and processed from devices that have their Bluetooth interface enabled. The user is then shown the information that are leaking from his device.},
booktitle = {Proceedings of the 12th Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {292–293},
numpages = {2},
keywords = {address randomization, bluetooth low energy, internet of things, privacy, tracking},
location = {Miami, Florida},
series = {WiSec '19}
}

@inproceedings{10.1145/3415958.3433038,
author = {Carniel, Anderson Chaves},
title = {Spatial Information Retrieval in Digital Ecosystems: A Comprehensive Survey},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433038},
doi = {10.1145/3415958.3433038},
abstract = {Spatial information retrieval is a common task of digital ecosystems due to the popularity of collecting and storing spatial information and phenomena in the world of the Internet of Things (IoT). Spatial relationships play an important role in this context by specifying how two or more spatial objects are related or connected. Examples of spatial relationships include topological relationships (e.g., intersect, overlap, contains), metric relationships (e.g., nearest neighbors), and direction relationships (e.g., cardinal directions like north and south). Many works in the literature have proposed definitions and implementations of spatial queries based on specific types of spatial relationships. Hence, a holistic view of these works is important to understand their applicability and relations. This paper advances in the literature by providing a comprehensive survey of the implementations and types of spatial queries that can be used by digital ecosystems. We present a novel characterization based on spatial relationships to define topological-based, metric-based, and direction-based spatial queries. For each type of spatial query, we present its intuitive and formal definitions together with possible strategies of implementation. Further, we identify hybrid spatial queries as combinations of two or more spatial relationships, and spatial joins as generalization cases. In addition, we present some equivalences between some types of queries. As a result, we point out future research topics in spatial information retrieval.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {10–17},
numpages = {8},
keywords = {Direction relationship, Hybrid spatial queries, IoT, Metric relationship, Spatial information retrieval, Spatial join, Spatial relationship, Topological relationship},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3538950.3538954,
author = {Xu, Baolin and Jiang, Jin and Ye, Junbin},
title = {Information Intelligence System Solution Based on Big Data Flink Technology},
year = {2022},
isbn = {9781450395632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538950.3538954},
doi = {10.1145/3538950.3538954},
abstract = {[Purpose/Significance] This paper aims to discuss how to use big data Flink technology to develop information intelligence system, and provides a reference case for developing information intelligence system using big data real-time streaming technology. [Design/Methodology] This paper briefly introduces the basic concepts of big data, batch processing, stream processing, Flink technology and its superiority in real-time stream processing. And then, according to the thinking mode of big data, complete a set of technical framework for processing information intelligence system [Findings/Conclusion] Compared with some other big data processing technologies such as Spark , Storm, Flink technology has obvious performance advantages in processing real-time stream information. [Originality/Value] Based on the big data thinking mode and the general process of big data processing, the research team creatively proposed an information intelligence processing system solution based on big data Flink technology. The solution takes Flink streaming technology as the processing center, Kafka as the message transmission queue, Elasticsearch as the real-time search engine, Kibana as the front-end display and Mysql as the database storage system. Experiments show that this solution can effectively process big data real-time streaming data and has good practical reference value in the field of information intelligence processing based on big data real-time streaming data.},
booktitle = {Proceedings of the 4th International Conference on Big Data Engineering},
pages = {21–26},
numpages = {6},
keywords = {Batch processing, Flink, Kafka, Stream processing},
location = {Beijing, China},
series = {BDE '22}
}

@article{10.1145/3276463,
author = {Abuzaid, Firas and Bailis, Peter and Ding, Jialin and Gan, Edward and Madden, Samuel and Narayanan, Deepak and Rong, Kexin and Suri, Sahaana},
title = {MacroBase: Prioritizing Attention in Fast Data},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/3276463},
doi = {10.1145/3276463},
abstract = {As data volumes continue to rise, manual inspection is becoming increasingly untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient, accurate, and modular analyses that highlight and aggregate important and unusual behavior, acting as a search engine for fast data. MacroBase is able to deliver order-of-magnitude speedups over alternatives by optimizing the combination of explanation (i.e., feature selection) and classification tasks and by leveraging a new reservoir sampler and heavy-hitters sketch specialized for fast data streams. As a result, MacroBase delivers accurate results at speeds of up to 2M events per second per query on a single core. The system has delivered meaningful results in production, including at a telematics company monitoring hundreds of thousands of vehicles.},
journal = {ACM Trans. Database Syst.},
month = {dec},
articleno = {15},
numpages = {45},
keywords = {Streaming, analytics, database}
}

@inproceedings{10.1145/3344341.3368800,
author = {Pallewatta, Samodha and Kostakos, Vassilis and Buyya, Rajkumar},
title = {Microservices-based IoT Application Placement within Heterogeneous and Resource Constrained Fog Computing Environments},
year = {2019},
isbn = {9781450368940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344341.3368800},
doi = {10.1145/3344341.3368800},
abstract = {Fog computing paradigm has created innovation opportunities within Internet of Things (IoT) domain by extending cloud services to the edge of the network. Due to the distributed, heterogeneous and resource constrained nature of the Fog computing nodes, Fog applications need to be developed as a collection of interdependent, lightweight modules. Since this concept aligns with the goals of microservices architecture, efficient placement of microservices-based IoT applications within Fog environments has the potential to fully leverage capabilities of Fog devices. In this paper, we propose a decentralized microservices-based IoT application placement policy for heterogeneous and resource constrained Fog environments. The proposed policy utilizes the independently deployable and scalable nature of microservices to place them as close as possible to the data source to minimize latency and network usage. Moreover, it aims to handle service discovery and load balancing related challenges of the microservices architecture. We implement and evaluate our policy using iFogSim simulated Fog environment. Results of the simulations show around 85% improvement in latency and network usage for the proposed microservice placement policy when compared with Cloud-only placement approach and around 40% improvement over an alternative Fog application placement method known as Edge-ward placement policy. Moreover, the decentralized placement approach proposed in this paper demonstrates significant reduction in microservice placement delay over centralized placement.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
pages = {71–81},
numpages = {11},
keywords = {application deployment, application placement, fog computing, internet of things (iot), microservices architecture},
location = {Auckland, New Zealand},
series = {UCC'19}
}

@inproceedings{10.1145/3278681.3278710,
author = {Horscroft, Gina and Nagar, Jivashi and Suleman, Hussein},
title = {Re-ranking the search results for users with time-periodic intents},
year = {2018},
isbn = {9781450366472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278681.3278710},
doi = {10.1145/3278681.3278710},
abstract = {This paper investigates the time of search as a feature to improve the personalization of information retrieval systems. In general, users issue small and ambiguous queries, which can refer to different topics of interest. Although personalized information retrieval systems take care of user's topics of interest, but they do not consider if the topics are time periodic. The same ranked list cannot satisfy user search intents every time. This paper proposes a solution to rerank the search results for time sensitive ambiguous queries. An algorithm "HighTime" is presented here to disambiguate the time sensitive ambiguous queries and re-rank the default Google results by using a time sensitive user profile. The algorithm is evaluated by using two comparative measures, MAP and NDCG.Results from user experiments showed that re-ranking of search results based on HighTime is effective in presenting relevant results to the users.},
booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
pages = {239–247},
numpages = {9},
keywords = {re-ranking search results, time-periodic queries},
location = {Port Elizabeth, South Africa},
series = {SAICSIT '18}
}

@inproceedings{10.1145/1967486.1967538,
author = {Wauer, Matthias and Schuster, Daniel and Meinecke, Johannes},
title = {Aletheia: an architecture for semantic federation of product information from structured and unstructured sources},
year = {2010},
isbn = {9781450304214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967486.1967538},
doi = {10.1145/1967486.1967538},
abstract = {Product-related information can be found in various data sources and formats across the product lifecycle. Effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base. Existing solutions for product information management (PIM) are usually restricted to structured information, but most of the business-critical information resides in unstructured documents. We present a generic architecture for federating heterogeneous information from various sources, and argue how this process benefits from using semantic representations. An reference implementation tailor-made to business users is explained and evaluated. We also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community.},
booktitle = {Proceedings of the 12th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {325–332},
numpages = {8},
keywords = {architecture, federated information system, information extraction, ontology, product information management, semantic web},
location = {Paris, France},
series = {iiWAS '10}
}

@inproceedings{10.1145/3423423.3423462,
author = {Flauzac, Olivier and Herard, Joffrey and Nolot, Florent and Desrumaux, Florian},
title = {Comparison between different platform using a Uniform Relay Protocol in Linear Sensor Network},
year = {2020},
isbn = {9781450388207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423423.3423462},
doi = {10.1145/3423423.3423462},
abstract = {Organizations want to collect data remotely to predict incidents or simply to store and analyze them. To do that, a sensor network is often deployed to collect these data. However, the dynamic nature of these networks and possible sensors out of range must be taken into account to ensure that information retrieval is carried out correctly. In this paper, we propose a study of a relay protocol including a local synchronization solution on different platforms to evaluate the power consumption of many types of sensor architecture and communication technology. The platforms considered using the 3 following microcontroller: STMicroelectronics STM32, LoPy, and Texas Instrument CC1350 with the 3 following modulation wireless communications: LoRa, WIFI, and raw 868MHz.},
booktitle = {Companion Proceedings of the 10th International Conference on the Internet of Things},
articleno = {14},
numpages = {4},
keywords = {STM32, cc1350, linear sensor network, lopy, relay, sensors network},
location = {Malm\"{o}, Sweden},
series = {IoT '20 Companion}
}

@inproceedings{10.1145/3479241.3486689,
author = {Almon, Lars and Krause, Arno Manfred and Fietze, Oliver and Hollick, Matthias},
title = {Desynchronization and MitM Attacks Against Neighbor Awareness Networking Using OpenNAN},
year = {2021},
isbn = {9781450390798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479241.3486689},
doi = {10.1145/3479241.3486689},
abstract = {The Neighbor Awareness Networking (NAN) specification, also known as Wi-Fi Aware, offers a quick and energy efficient way of forming wireless ad-hoc networks. It includes ranging capabilities based on Fine Timing Measurements (FTM) and can sustain connectivity to a Wi-Fi network while participating in a NAN cluster. Android is currently the only operating system to support it, offering a limited and proprietary interface. NAN shares many fundamental properties with Apple's Wireless Direct Link protocol, which is used by millions of devices today. To facilitate research in this area, we present OpenNAN: the first open source NAN stack. It supports Linux and requires just common and cheap off-the-shelf Wi-Fi hardware. Using OpenNAN we perform a security analysis of NAN and identify three attacks. First, any malicious node can manipulate the Anchor Master Selection to become the Anchor Master. Second, a malicious Anchor Master can desynchronize individual nodes by sending synchronization beacons as unicast. And finally, we show a Machine-in-the-Middle (MitM) on the NAN Service Discovery Protocol. We successfully perform all attacks against different Android versions and NAN stacks.},
booktitle = {Proceedings of the 19th ACM International Symposium on Mobility Management and Wireless Access},
pages = {97–105},
numpages = {9},
keywords = {NAN, ad-hoc networks, security, wi-fi, wi-fi aware},
location = {Alicante, Spain},
series = {MobiWac '21}
}

@inproceedings{10.1145/3377571.3377641,
author = {Almazan, Van Keith B. and Mahipus, Frank Ismael B. and Santos, Jose Rafael M. and Samonte, Mary Jane C.},
title = {CAHM: Companion Animal Health Monitoring System},
year = {2020},
isbn = {9781450372947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377571.3377641},
doi = {10.1145/3377571.3377641},
abstract = {Internet of Things or IoT changes each and everyone's lives. It generally changes the way human interact with their environment and objects. In this project, IoT was conceptualized with improve human and pet interaction by medical or health opportunity, through information retrieval. This study is about the development of a mobile application, paired with a customized hardware device strapped on to the dog's collar called CAHM. Companion Animal Health Monitoring Application or CAHM is an application intended for the use of both pet owners and veterinarians. The system features the smart collar that has location tracking, heart rate tracking, body temperature and monitoring dog pet's activity. Functionality and usability testing were done in order to attain the developed systems' full features and gather feedback from the intended beneficiaries of the system. Testing results shows that every component of the systems works accordingly and intended beneficiaries are satisfied with the application.},
booktitle = {Proceedings of the 2020 11th International Conference on E-Education, E-Business, E-Management, and E-Learning},
pages = {417–421},
numpages = {5},
keywords = {GPS, Health IoTPet Monitoring, Medical Internet of Things, Microcontroller Unit, Smart Collar},
location = {Osaka, Japan},
series = {IC4E '20}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00061,
author = {Yusuf, Imam Nur Bani},
title = {Towards Automated Embedded Systems Programming},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00061},
doi = {10.1109/ICSE-Companion58688.2023.00061},
abstract = {Writing code for embedded systems poses unique challenges due to hardware involvement. Developers often need to learn domain-specific knowledge to write embedded codes. Learning such knowledge is time-consuming and hinders developers' productivity. This paper presents a proposal for an automated code generation approach, specifically designed for embedded systems. The work is composed of three milestones, i.e., understanding the needs of embedded developers by analyzing posts from discussion forums, developing a tool to recommend driver libraries of I/O hardware and generate its interface configurations and usage patterns, and improving the generation accuracy of the prior tool using program analysis techniques. The tool will be evaluated using various metrics from machine translation, classification, and information retrieval fields.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {224–226},
numpages = {3},
keywords = {code generation, library recommendation, embedded system, hardware configuration},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.3115/991365.991476,
author = {Hori, Koichi and Toda, Seinosuke and Yasunaga, Hisashi},
title = {Learning the space of word meanings for information retrieval systems},
year = {1986},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/991365.991476},
doi = {10.3115/991365.991476},
abstract = {Several methods to represent meanings of words have been proposed. However. they are not useful for information retrieval systems because they cannot deal with the entities which cannot be universally represented by symbols.In this paper, we propose a notion of semantic space. Semantic space is an Euclidean space where words and entities are put. A word is one point in the space. The meanings of the word are represented as the space configuration around the word. The entities that cannot be represented by symbols can be identified in the space by the location the entity should be settled in. We also give a learning mechanism for the space. We prove the effectiveness of the proposed method by an experiment on information retrieval for the study of Japanese literature.},
booktitle = {Proceedings of the 11th Coference on Computational Linguistics},
pages = {374–379},
numpages = {6},
location = {Bonn, Germany},
series = {COLING '86}
}

@inproceedings{10.1145/3404835.3462818,
author = {Zhang, Weinan and Zhao, Xiangyu and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
title = {DRL4IR: 2nd Workshop on Deep Reinforcement Learning for Information Retrieval},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462818},
doi = {10.1145/3404835.3462818},
abstract = {Modern information retrieval (IR) consists of a series of processes, including query expansion, candidate item recall, item ranking, item re-ranking, etc. The final ranked item list will be exposed to the user, which will accordingly provide feedback through some expected actions such as browsing and click. Such a whole process can be formulated as a decision-making process where the agent is the IR system while the environment is the specific user. This decision-making process can be one-step or sequential, depending on the scenarios or the ways of problem formulation. Since 2013, Deep reinforcement learning (DRL) has been a fast-developing technique for decision-making tasks. The high capacity of deep learning models is incorporated in the reinforcement learning framework so that the agent may successfully handle complex decision-making. In recent years, there have been a bunch of publications attempting to leverage DRL techniques for different IR tasks such as ad hoc retrieval, learning to rank and interactive recommendation. Nonetheless, the fundamental theory, the principle of RL methods or the recognized experimental protocols of decision-making in IR, has not been well developed, making it challenging to evaluate the correctness of a proposed method or judge whether the reported experimental performance is valid. We propose the second DRL4IR workshop at SIGIR 2021, which provides a venue to gather the academia researchers and industry practitioners to present the recent progress of DRL techniques for IR. More importantly, people in this workshop are expected to discuss more about the fundamental principles of formulating a decision-making IR task, the underlying theory as well as the practical effectiveness of the experiment protocol design, which would foster further research on novel methodologies, innovative experimental findings and new applications of DRL for information retrieval. DRL4IR organized at SIGIR'20 was one of the most popular workshops and attracted over 200 conference attendees. In this year, we will pay more attention to fundamental research topics and recent applications, and expect about 300 participants.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2681–2684},
numpages = {4},
keywords = {deep reinforcement learning, information retrieval},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3395351.3401796,
author = {Heinrich, Alexander and Stute, Milan and Hollick, Matthias},
title = {BTLEmap: Nmap for bluetooth low energy},
year = {2020},
isbn = {9781450380065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395351.3401796},
doi = {10.1145/3395351.3401796},
abstract = {The market for Bluetooth Low Energy (BLE) devices is booming and, at the same time, has become an attractive target for adversaries. To improve BLE security at large, we present BTLEmap, an auditing application for BLE environments. BTLEmap is inspired by network discovery and security auditing tools such as Nmap for IP-based networks. It allows for device enumeration, Generic Attribute Profile (GATT) service discovery, and device fingerprinting. It also features a BLE advertisement dissector, data exporter, and a user-friendly UI including a proximity view. BTLEmap currently runs on iOS and macOS using Apple's CoreBluetooth API but also accepts alternative data inputs such as a Raspberry Pi to overcome the restricted vendor API. The open-source project is under active development and will provide more advanced capabilities such as long-term device tracking (in spite of MAC address randomization) in the future.},
booktitle = {Proceedings of the 13th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {331–333},
numpages = {3},
keywords = {bluetooth low energy, device enumeration, network monitoring, privacy analysis, reverse engineering},
location = {Linz, Austria},
series = {WiSec '20}
}

@inproceedings{10.1145/3374664.3375739,
author = {Aktypi, Angeliki and Kalkan, Kubra and Rasmussen, Kasper B.},
title = {SeCaS: Secure Capability Sharing Framework for IoT Devices in a Structured P2P Network},
year = {2020},
isbn = {9781450371070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374664.3375739},
doi = {10.1145/3374664.3375739},
abstract = {The emergence of the internet of Things (IoT) has resulted in the possession of a continuously increasing number of highly heterogeneous connected devices by the same owner. To make full use of the potential of a personal IoT network, there must be secure and effective cooperation between them. While application platforms (e.g., Samsung SmartThings) and interoperable protocols (e.g., MQTT) exist already, the reliance on a central hub to coordinate communication introduces a single-point of failure, provokes bottleneck problems and raises privacy concerns. In this paper we propose SeCaS, a Secure Capability Sharing framework, built on top of a peer-to-peer (P2P) architecture. SeCaS addresses the problems of fault tolerance, scalability and security in resource discovery and sharing for IoT infrastructures using a structured P2P network, in order to take advantage of the self-organised and decentralised communication it provides. SeCaS brings three main contributions: (i) a capability representation that allows each device to specify what services they offer, and can be used as a common language to search for, and exchange, capabilities, resulting in flexible service discovery that can leverage the properties on a distributed hash table (DHT); (ii) a set of four protocols that provides identification of the different devices that exist in the network and authenticity of the messages that are exchanged among them; and (iii) a thorough security and complexity analysis of the proposed scheme that shows SeCaS to be both secure and scalable.},
booktitle = {Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy},
pages = {271–282},
numpages = {12},
keywords = {dht, fault-tolerance, iot, privacy, resource sharing, scalability},
location = {New Orleans, LA, USA},
series = {CODASPY '20}
}

@article{10.1145/2701583.2701597,
author = {Albakour, M-Dyaa and Macdonald, Craig and Ounis, Iadh and Clarke, Charles L.A. and Bicer, Veli},
title = {Report on the 1st International Workshop on Information Access in Smart Cities (i-ASC 2014)},
year = {2014},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/2701583.2701597},
doi = {10.1145/2701583.2701597},
abstract = {Modern cities are increasingly becoming smart where a digital knowledge infrastructure is deployed by local authorities (e.g. City councils and municipalities) to better serve the information needs of their citizens, and to ensure the sustainability and efficient use of power and resources. This knowledge infrastructure consists of a wide range of systems from lowlevel physical sensors to advanced sensing devices through social sensors. The i-ASC 2014 workshop was the first international event, within the Information Retrieval (IR) community, that is dedicated to research on smart/future cities. In particular, the workshop was a venue for research on digesting the city's data streams and knowledge databases in order to serve the information needs of citizens and support decision making for local authorities. Possible use cases include helping tourists to find interesting places to go or activities to do while visiting a city, or assisting journalists in reporting local incidents. Indeed, the workshop was intended to foster the development of new information access and retrieval models that can harness effectively and efficiently the large number of heterogeneous big data streams in a city to provide a new generation of information services. The workshop was well attended, where more than 45 participants were officially registered. It featured two keynote talks from industry (IBM andWaag Society) and two invited talks from academia (Pisa and Edinburgh). In addition, seven refereed papers were presented before breakout groups considered questions and issues identified from a panel discussion.},
journal = {SIGIR Forum},
month = {dec},
pages = {96–104},
numpages = {9}
}

@inproceedings{10.1145/1464291.1464363,
author = {Dodd, George G.},
title = {APL: a language for associative data handling in PL/I},
year = {1966},
isbn = {9781450378932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1464291.1464363},
doi = {10.1145/1464291.1464363},
abstract = {Engineering design and computer-aided problem solving occur in an atmosphere in which the relationships between the data elements are utilized. For example, information retrieval, computer-aided drawing, electrical network design, and engineering design systems are among those whose operation depends on efficient data manipulation and association techniques.},
booktitle = {Proceedings of the November 7-10, 1966, Fall Joint Computer Conference},
pages = {677–684},
numpages = {8},
location = {San Francisco, California},
series = {AFIPS '66 (Fall)}
}

@article{10.1145/3592604,
author = {Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam and Yun, Unil},
title = {Semi-Supervised Lexicon-Aware Embedding for News Article Time Estimation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3592604},
doi = {10.1145/3592604},
abstract = {In the information retrieval community, Temporal Information Retrieval (TIR) has become increasingly popular. Documents focused on the time surrounding their publication are more likely to be accurate and contain information relevant to the reader. In this study, we explore the inverted pyramid paradigm by extracting temporal expressions from news documents, standardizing their values, and evaluating them based on their position within the text. We present a lexicon expansion method that employs WordNet as input. This approach enhances the lexicon by grouping words with similar meanings, potentially improving the accuracy of event detection algorithms. Additionally, this process can introduce new words and phrases to the lexicon, expanding the vocabulary. Using each tagged dataset, a classifier is trained with a pre-trained network. A pool of unlabeled data are processed, and high-confidence pseudo-labels are assigned. Pseudo-labels are generated by leveraging the partially trained model and the original labelled data. As the classifier predicts the correct label for a data sample, the pseudo-labels of other data samples are updated, and vice versa. At the end of this process, the predictions from different matching classifiers are combined. It takes several rounds to label the unlabeled inputs using this method. To evaluate the proposed solutions, we conducted experiments on 4,500 online news articles relevant to temporal retrieval. LSTM, BiLSTM, and BERT models with and without lexicon expansion were assessed based on log loss and relative divergence of entropy. A jointly trained semi-supervised learning model achieved a mean KL divergence of 0.89, an F1 score of 0.74 for temporal events, and 0.63 for non-temporal events. Besides alleviating data sparsity issues and enabling the training of more complex networks, this technique can also serve as an alternative to data augmentation methods.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
keywords = {News classification, Data sparsity, Data augmentation, Attention network}
}

@inproceedings{10.1145/3486001.3486247,
author = {Gupta, Manish},
title = {Compression of Deep Learning Models for NLP},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486247},
doi = {10.1145/3486001.3486247},
abstract = {In recent years, the fields of NLP and information retrieval have made tremendous progress thanks to deep learning models like RNNs and LSTMs, and Transformer&nbsp;[36] based models like BERT&nbsp;[9]. But these models are humongous in size. Real world applications however demand small model size, low response times and low computational power wattage. We will discuss six different types of methods (pruning, quantization, knowledge distillation, parameter sharing, matrix decomposition, and other Transformer based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this tutorial is very timely. We will organize related work done by the ‘deep learning for NLP’ community in the past few years and present it as a coherent story.},
booktitle = {Proceedings of the First International Conference on AI-ML Systems},
articleno = {27},
numpages = {2},
location = {Bangalore, India},
series = {AIMLSystems '21}
}

@inproceedings{10.1145/3340531.3412171,
author = {Gupta, Manish and Varma, Vasudeva and Damani, Sonam and Narahari, Kedhar Nath},
title = {Compression of Deep Learning Models for NLP},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412171},
doi = {10.1145/3340531.3412171},
abstract = {In recent years, the fields of NLP and information retrieval have made tremendous progress thanks to deep learning models like RNNs and LSTMs, and Transformer[35] based models like BERT[9]. But these models are humongous in size. Real world applications however demand small model size, low response times and low computational power wattage. We will discuss six different types of methods (pruning, quantization, knowledge distillation, parameter sharing, matrix decomposition, and other Transformer based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this tutorial is very timely. We will organize related work done by the 'deep learning for NLP' community in the past few years and present it as a coherent story.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3507–3508},
numpages = {2},
keywords = {compression methods for transformers, knowledge distillation, matrix decomposition, model compression, parameter sharing, pruning, quantization},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3054977.3057296,
author = {Lu, Zongqing and Felemban, Noor and Chan, Kevin and La Porta, Thomas},
title = {On-demand Information Retrieval from Videos Using Deep Learning in Wireless Networks: Demo Abstract},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3057296},
doi = {10.1145/3054977.3057296},
abstract = {Mobile devices with cameras have greatly assisted in the prevalence of online videos. Valuable information may be retrieved from videos for various purposes. While deep learning enables automatic information retrieval from videos, it is a demanding task for mobile devices despite recent advances in their computational capability. Given a network consisting of mobile devices and a video-cloud, mobile devices may be able to upload videos to the video-cloud, a platform more computationally capable to process videos. However, due to network constraints, once a query initiates a video processing task of a specific interest, most videos will not likely have been uploaded to the video-cloud, especially when the query is about a recent event. We designed and implemented a distributed system for video processing using deep learning across a wireless network, where network devices answer queries by retrieving information from videos stored across the network and reduce query response time by computation offload from mobile devices to the video-cloud.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {279–280},
numpages = {2},
keywords = {Video processing, deep learning, wireless networks},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.1145/3397271.3401397,
author = {Czyzewski, Adam and Dalton, Jeffrey and Leuski, Anton},
title = {Agent Dialogue: A Platform for Conversational Information Seeking Experimentation},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401397},
doi = {10.1145/3397271.3401397},
abstract = {Conversational Information Seeking (CIS) is an emerging area of Information Retrieval focused on interactive search systems. As a result there is a need for new benchmark datasets and tools to enable their creation. In this demo we present the Agent Dialogue (AD) platform, an open-source system developed for researchers to perform Wizard-of-Oz CIS experiments. AD is a scalable cloud-native platform developed with Docker and Kubernetes with a flexible and modular micro-service architecture built on production-grade state-of-the-art open-source tools (Kubernetes, gRPC streaming, React, and Firebase). It supports varied front-ends and has the ability to interface with multiple existing agent systems, including Google Assistant and open-source search libraries. It includes support for centralized structure logging as well as offline relevance annotation.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2121–2124},
numpages = {4},
keywords = {Wizard-Of-Oz, data collection, interactive search},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.3115/991250.991314,
author = {Johansson, Christer},
title = {Catching the Cheshire Cat},
year = {1994},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/991250.991314},
doi = {10.3115/991250.991314},
abstract = {Finding useful phrases is important in applications like information retrieval, and text-to-speech systems. One of the currently most used statistics is the mutual information ratio. This paper compares the mutual information ratio and a measure that takes temporal ordering into account. Using this modified measure, some local syntactic constraints as well as phrases are captured.},
booktitle = {Proceedings of the 15th Conference on Computational Linguistics - Volume 2},
pages = {1021–1025},
numpages = {5},
location = {Kyoto, Japan},
series = {COLING '94}
}

@inproceedings{10.1145/3478905.3478955,
author = {Thakur, Nirmalya and Y. Han, Chia},
title = {Pervasive Activity Logging for Indoor Localization in Smart Homes},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478955},
doi = {10.1145/3478905.3478955},
abstract = {The scientific contribution of this work to address multiple global societal and economic challenges associated with the increasing aging population is primarily two-fold. First, it presents and discusses a new computing methodology - Pervasive Activity Logging that involves the creation of an adaptive and semantic collection or `log' of human activities in Internet of Things (IoT)-based pervasive environments, along with the characteristics of user interactions associated with these activities in terms of the contextual, behavioral, spatial, and temporal information. Second, by using this concept of Pervasive Activity Logging, this paper presents a novel machine learning and pattern recognition-based approach for Indoor Localization during different activities performed within the premises of an IoT-based pervasive environment, such as a Smart Home. This learning approach used a Random Forest-based classification model and detected the user's indoor location with an accuracy of 83.02%. These methodologies were developed by integrating the latest advancements from Pervasive Computing, Big Data, Information Retrieval, Internet of Things, Human-Computer Interaction, Machine Learning, and Pattern Recognition. The results presented and discussed uphold the relevance, potential, and importance of these methodologies for contributing towards independent living, healthy aging, and improved quality of life of elderly people in the future of IoT-based pervasive environments, such as Smart Homes.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {246–255},
numpages = {10},
keywords = {activities of daily living, activity recognition, big data, elderly population, indoor localization, internet of things, machine learning, pervasive computing, smart homes},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/133160.133181,
author = {Grefenstette, Gregory},
title = {Use of syntactic context to produce term association lists for text retrieval},
year = {1992},
isbn = {0897915232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/133160.133181},
doi = {10.1145/133160.133181},
abstract = {One aspect of world knowledge essential to information retrieval is knowing when two words are related. Knowing word relatedness allows a system given a user's query terms to retrieve relevant documents not containing those exact terms. Two words can be said to be related if they appear in the same contexts Document co-occurrence gives a measure of word relatedness that has proved to be too rough to be useful. The relatively recent apparition of on-line dictionaries and robust and rapid parsers permits the extraction of finer word contexts from large corpora. In this paper, we will describe such an extraction technique that uses only coarse syntactic analysis and no domain knowledge. This technique produces lists of words related to any work appearing in a corpus. When the closest related terms were used in query expansion of a standard information retrieval testbed, the results were much better than that given by document co-occurence techniques, and slightly better than using unexpanded queries, supporting the contention that semantically similar words were indeed extracted by this technique.},
booktitle = {Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {89–97},
numpages = {9},
location = {Copenhagen, Denmark},
series = {SIGIR '92}
}

@article{10.1145/3672615,
author = {Wang, Zhu and Han, Fengxia and Zhao, Shengjie},
title = {A Survey on Knowledge Graph Related Research in Smart City Domain},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4681},
url = {https://doi.org/10.1145/3672615},
doi = {10.1145/3672615},
abstract = {Knowledge graph employs the specific graph structure to store knowledge in the form of entities, relations, attributes, etc., which can effectively represent correlations among data and has been applied in many fields, including search engine optimization, intelligent question answering, and recommendation systems. In this paper, we mainly focus on the research and application of the domain-specific knowledge graph in the field of the smart city, which has not been fully paid attention to. By constructing the corresponding knowledge graph, the data of urban traffic, services, and public resources are integrated to provide help for city builders and managers to make important decisions. Currently, the major problem faced by the smart city lies in data mining and proper application. On the one hand, data is usually stored by government management departments, which creates challenges such as high data storing overhead and inefficient data usage. On the other hand, data cannot be coordinated and collaborated between different city management systems, and data silos exist. Therefore, we will review the related literature on the knowledge graph existing in the smart city domain. Specifically, we will analyze and summarize knowledge graph construction research in the field of smart cities from four perspectives, i.e., smart city ontology, urban data processing, urban knowledge graph construction, and their application. Finally, the research limitations and prospects of the urban knowledge graph are provided.},
note = {Just Accepted},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
keywords = {domain-specific knowledge graph, smart city, urban data, urban knowledge graph, construction and application}
}

@inproceedings{10.3115/991250.991255,
author = {Sibun, Penelope and Farrar, David S.},
title = {Content characterization using word shape tokens},
year = {1994},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/991250.991255},
doi = {10.3115/991250.991255},
abstract = {By quickly classifying character images into character shape categories, it is possible to automatically extract syntactic information from the text of document images without optical character recognition. Using word shape tokens composed of these character shape codes, a properly trained text tagger can extract part-of-speech information from scanned document images. Later components of a document processing system can then use this information to locate topics, characterize document style, and assist in information retrieval.},
booktitle = {Proceedings of the 15th Conference on Computational Linguistics - Volume 2},
pages = {686–690},
numpages = {5},
location = {Kyoto, Japan},
series = {COLING '94}
}

@article{10.1145/3485187,
author = {Huang, Shi Ming and Yen, David C. and Yan, Ting Jyun and Yang, Yi Ting},
title = {An Intelligent Mechanism to Automatically Discover Emerging Technology Trends: Exploring Regulatory Technology},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3485187},
doi = {10.1145/3485187},
abstract = {Technology trend analysis uses data relevant to historical performance and extrapolates it to estimate and assess the future potential of technology. Such analysis is used to analyze emerging technologies or predict the growing markets that influence the resulting social or economic development to assist in effective decision-making. Traditional trend analysis methods are time-consuming and require considerable labor. Moreover, the implemented processes may largely rely on the specific knowledge of the domain experts. With the advancement in the areas of science and technology, emerging cross-domain trends have received growing attention for its considerable influence on society and the economy. Consequently, emerging cross-domain predictions that combine or complement various technologies or integrate with diverse disciplines may be more critical than other tools and applications in the same domain. This study uses a design science research methodology, a text mining technique, and social network analysis (SNA) to analyze the development trends concerning the presentation of the product or service information on a company's website. This study applies regulatory technology (RegTech) as a case to analyze and justify the emerging cross-disciplinary trend. Furthermore, an experimental study is conducted using the Google search engine to verify and validate the proposed research mechanism at the end of this study. The study results reveal that, compared with Google Trends and Google Correlate, the research mechanism proposed in this study is more illustrative, feasible, and promising because it reduces noise and avoids the additional time and effort required to perform a further in-depth exploration to obtain the information.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {dec},
articleno = {17},
numpages = {29},
keywords = {Text mining, social network analysis, technology trend analysis, computer auditing, regtech, fintech}
}

@inproceedings{10.1145/3577530.3577559,
author = {Reyes Jr., Felizardo Calimag and Cayabyab, Gerald Tomelden and Mendoza, Paula Jean Castro},
title = {Analyzing Truck Driver's Behavior on the Road Using YOLO v4 Tiny Algorithm},
year = {2023},
isbn = {9781450397773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577530.3577559},
doi = {10.1145/3577530.3577559},
abstract = {The Philippine logistics industry is always in need of competent and qualified truck drivers to meet the increasing demands of the country's economic operations. Thus, this research paper investigates the driving competency of truck drivers driving behaviour. The industry plays a critical part in the country's economy, and any trucking accident creates a significant problem for the parties involved that causes a decline in productivity and loss of revenue. This research paper discusses the creation of a system which utilizes machine learning, data analytics, and internet-of-things for easy monitoring and assessment of the truck driver's driving behaviour. In collaboration with the Department of Science and Technology and the Technological Institute of the Philippines, the project aims to integrate various technologies and use it to assess the roadworthiness of truck drivers and use the YOLO v4 algorithm to train the model. It provides all the necessary details covering the statistics needed to assess the truck driver that aligns with the standards set required by the Land Transportation Office (LTO) administration. The Agile methodology was used throughout the development as there are constant changes being made which adds flexibility and puts fewer risks for the system development. Based on the model compiled and the results of the iteration on Yolov4-tiny the classes average precision has a minimum value of 75.45% and a maximum value of 99.89%. The results showed a high accuracy performance of the model created in information retrieval and object detection.},
booktitle = {Proceedings of the 2022 6th International Conference on Computer Science and Artificial Intelligence},
pages = {177–183},
numpages = {7},
keywords = {AWS, Ant Design, IoT, MERN Stack, Rest APIs, Truck Drivers, Web Application, Web Development},
location = {Beijing, China},
series = {CSAI '22}
}

@article{10.1145/3456722,
author = {Hassanpourghadi, Mohsen and Rasul, Rezwan A. and Chen, Mike Shuo-Wei},
title = {A Module-Linking Graph Assisted Hybrid Optimization Framework for Custom Analog and Mixed-Signal Circuit Parameter Synthesis},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3456722},
doi = {10.1145/3456722},
abstract = {Analog and mixed-signal (AMS) computer-aided design tools are of increasing interest owing to demand for the wide range of AMS circuit specifications in the modern system on a chip and faster time to market requirement. Traditionally, to accelerate the design process, the AMS system is decomposed into smaller components (called modules) such that the complexity and evaluation of each module are more manageable. However, this decomposition poses an interface problem, where the module’s input-output states deviate from when combined to construct the AMS system, and thus degrades the system expected performance. In this article, we develop a tool module-linking-graph assisted hybrid parameter search engine with neural networks (MOHSENN) to overcome these obstacles. We propose a module-linking-graph that enforces equality of the modules’ interfaces during the parameter search process and apply surrogate modeling of the AMS circuit via neural networks. Further, we propose a hybrid search consisting of a global optimization with fast neural network models and a local optimization with accurate SPICE models to expedite the parameter search process while maintaining the accuracy. To validate the effectiveness of the proposed approach, we apply MOHSENN to design a successive approximation register analog-to-digital converter in 65-nm CMOS technology. This demonstrated that the search time improves by a factor of 5 and 700 compared to conventional hierarchical and flat design approaches, respectively, with improved performance.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {jun},
articleno = {38},
numpages = {22},
keywords = {CAD tool, deep neural network}
}

@inproceedings{10.5555/3021385.3021405,
author = {Zhang, Lifang and Zheng, Yan and Kantoa, Raimo},
title = {A Review of Homomorphic Encryption and its Applications},
year = {2016},
isbn = {9781631901041},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Homomorphic Encryption (HE) enables meaningful computations on encrypted data without decrypting it. Thus, privacy concerns can be addressed in a satisfactory manner by encrypting data using the homomorphic cryptosystem before uploading the data to a cloud service provider. Recent years have witnessed rapid progress of (HE) development in theory and practice. However, current literature still lacks a thorough review on these new developments and their applications. This paper guides the reader through a journey of these developments by reviewing the state of the art of homomorphic cryptosystems and discussing their advantages, disadvantages as well as applicability. In addition, this work presents the privacy preserving applications of homomorphic cryptosystems in the field of private information retrieval, genomic data, cloud computing and participatory sensing. Moreover, this review further discusses open issues of such applications and future research trends.},
booktitle = {Proceedings of the 9th EAI International Conference on Mobile Multimedia Communications},
pages = {97–106},
numpages = {10},
keywords = {Cryptography, homomorphic encryption, privacy},
location = {Xi'an, China},
series = {MobiMedia '16}
}

@inproceedings{10.1145/2502081.2502279,
author = {Gupta, Amarnath and Jain, Ramesh},
title = {Social life networks: a multimedia problem?},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502279},
doi = {10.1145/2502081.2502279},
abstract = {Connecting people to the resources they need is a fundamental task for any society. We present the idea of a technology that can be used by the middle tier of a society so that it uses people's mobile devices and social networks to connect the needy with providers. We conceive of a world observatory called the Social Life Network (SLN) that connects together people and things and monitors for people's needs as their life situations evolve. To enable such a system we need SLN to register and recognize situations by combining people's activities and data streaming from personal devices and environment sensors, and based on the situations make the connections when possible. But is this a multimedia problem? We show that many pattern recognition, machine learning, sensor fusion and information retrieval techniques used in multimedia-related research are deeply connected to the SLN problem. We sketch the functional architecture of such a system and show the place for these techniques.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {203–212},
numpages = {10},
keywords = {event-based system, situation recognition, social computing, social networks},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1109/UCC.2014.29,
author = {Selimi, Mennan and Freitag, Felix and Centelles, Roger Pueyo and Moll, Agust\'{\i}},
title = {Distributed Storage and Service Discovery for Heterogeneous Community Network Clouds},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.29},
doi = {10.1109/UCC.2014.29},
abstract = {Community networks are decentralized communication networks built and operated by citizens, for citizens. Most users see in community networks only the possibility to gain Internet access, while we propose clouds in community networks as the real opportunity: different to the general purpose cloud-based applications offered in the Internet, community clouds would allow providing cloud-based services that are relevant for the community and that are shaped and owned by the community. It is in favour of our vision that today's cloud management systems and applications have consolidated and can run on commodity hardware, making them now ready for potential deployment and usage in community networks. The experience that we report in this paper is on a real distributed cloud that we have permanently running within a community network, where for our experiments two distributed file systems were deployed over very heterogeneous distributed cloud resources that are part of the system. Tahoe-LAFS and Xtreem FS were evaluated where the distributed storage nodes are provided by KVM-based VMs from Proxmox and Open Stack cloud management platforms, by Linux containers (LXC) from a community resource management platform and even from storage space on IoT embedded boards. Furthermore, we implement a service discovery and publishing mechanism that automatically publishes and discovers available services (e.g. Distributed storage service) of a cloud node to all the other nodes. We compared the performance of Tahoe-LAFS and Xtreem FS in this highly diverse settings and under the dynamic conditions of the community network. While both file system performed functionally correct, since Tahoe-LAFS offers end-to-end encryption by default and fault-tolerance to churn of nodes, it seems to be able to be a solution for important use cases for storage in community networks where privacy of data is important.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {204–212},
numpages = {9},
keywords = {cloud storage, community cloud, community networks, service discovery},
series = {UCC '14}
}

@inproceedings{10.1145/42005.42035,
author = {Krawczak, D. and Smith, P. and Shute, S.},
title = {EP-X: a demonstration of semantically based search of bibliographic databases},
year = {1987},
isbn = {0897912322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/42005.42035},
doi = {10.1145/42005.42035},
abstract = {EP-X (Environmental Pollution eXpert) is a prototype knowledge-based system that assists users in conducting bibliographic searches of the environmental pollution literature. This system combines artificial intelligence and human factors engineering techniques, allowing us to redesign traditional bibliographic information retrieval interfaces. The result supports semantically-based search as opposed to the typical character-string matching approach. This paper discusses
a sample interaction with EP-X,the knowledge representations necessary to support this semantically-based interaction,preliminary results of empirical studies to evaluate the interface, andrecommendations for future directions},
booktitle = {Proceedings of the 10th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {263–271},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '87}
}

@inproceedings{10.1145/3360774.3360777,
author = {Celosia, Guillaume and Cunche, Mathieu},
title = {Saving private addresses: an analysis of privacy issues in the bluetooth-low-energy advertising mechanism},
year = {2020},
isbn = {9781450372831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360774.3360777},
doi = {10.1145/3360774.3360777},
abstract = {The Bluetooth Low Energy (BLE) protocol is being included in a growing number of connected objects such as fitness trackers and headphones. As part of the service discovery mechanism of BLE, devices announce themselves by broadcasting radio signals called advertisement packets that can be collected with off-the-shelf hardware and software. To avoid the risk of tracking based on those messages, BLE features an address randomization mechanism that substitutes the device address with random temporary pseudonyms, called Private addresses.In this paper, we analyze the privacy issues associated with the advertising mechanism of BLE, leveraging a large dataset of advertisement packets collected in the wild. First, we identified that some implementations fail at following the BLE specifications on the maximum lifetime and the uniform distribution of random identifiers. Furthermore, we found that the payload of the advertisement packet can hamper the randomization mechanism by exposing counters and static identifiers. In particular, we discovered that advertising data of Apple and Microsoft proximity protocols can be used to defeat the address randomization scheme. Finally, we discuss how some elements of advertising data can be leveraged to identify the type of device, exposing the owner to inventory attacks.},
booktitle = {Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {444–453},
numpages = {10},
keywords = {address randomization, bluetooth low energy, privacy, tracking},
location = {Houston, Texas, USA},
series = {MobiQuitous '19}
}

@inproceedings{10.1145/3184558.3186979,
author = {Schneider, Rudolf and Arnold, Sebastian and Oberhauser, Tom and Klatt, Tobias and Steffek, Thomas and L\"{o}ser, Alexander},
title = {Smart-MD: Neural Paragraph Retrieval of Medical Topics},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186979},
doi = {10.1145/3184558.3186979},
abstract = {We demonstrate Smart-MD, an information retrieval system for medical professionals. The system supports topical queries in the form [disease topic], such as ["lyme disease", treatments]. In contrast to document-oriented retrieval systems, Smart-MD retrieves relevant paragraphs and reduces the reading load of a medical doctor drastically. We recognize diseases and topical aspects with a novel paragraph retrieval method based on bidirectional LSTM neural networks. We demonstrate Smart-MD on a dataset that contains 3,469 diseases from the English language part of Wikipedia and 6,876 distinct medical aspects extracted from Wikipedia headlines.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {203–206},
numpages = {4},
keywords = {neural information classification, paragraph retrieval},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3581754.3584165,
author = {Gadiraju, Ujwal and Abbas, Tahir and Allen, Garrett},
title = {DECI: A Tutorial on Designing Effective Conversational Interfaces},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584165},
doi = {10.1145/3581754.3584165},
abstract = {Conversational User Interfaces (CUIs) have been argued to have advantages over traditional GUIs due to having a more human-like interaction. The growing popularity of conversational agents has enabled humans to interact with machines more naturally. There is an increasing familiarity among people with conversational interactions mediated by technology due to the widespread use of mobile devices and messaging services and a hungry market for conversational agents. Based on the recent advances in conversational AI, as a result of the proliferation of large language models, the signs are that the future of human-computer interaction will have a significant conversational component. Today, over two-thirds of the population on our planet has access to the Internet, with ever-lowering barriers to accessibility. This tutorial will showcase the benefits of employing novel conversational interfaces for crowd computing, human-AI decision making, health and well-being, and information retrieval. Given the widespread adoption of AI systems across several domains, we will discuss the potential of conversational interfaces in facilitating and mediating people’s interactions with AI systems. The tutorial will include interactive elements and discussions and provide participants with insights to inform the design of effective conversational interfaces.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {187–189},
numpages = {3},
keywords = {conversational AI, conversational crowdsourcing, conversational user interfaces, human-AI decision making, human-AI interaction},
location = {Sydney, NSW, Australia},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/2307849.2307860,
author = {Costa, Pedro Maur\'{\i}cio and Pitt, Jeremy and Falc\~{a}o e Cunha, Jo\~{a}o and Galv\~{a}o, Teresa},
title = {Cloud2Bubble: enhancing quality of experience in mobile cloud computing settings},
year = {2012},
isbn = {9781450313193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307849.2307860},
doi = {10.1145/2307849.2307860},
abstract = {In recent years the mass adoption of mobile devices and increasingly ubiquitous connectivity have contributed to a radical change in the way people interact with computer systems. Moreover cloud computing infrastructures have paved the way for the development of smart systems in such settings, whose goal is to provide a service to enhance user experience based on environment and user sensed data. In this context, there is a clear disconnection between the two streams that flow continuously between user and cloud-based systems. On the one hand, user- and environment-generated data is being, for the most part, disregarded by service providers. On the other hand, services offered do not address users' specific needs and preferences. In addition, service discovery is a cognitive demanding process and it may have detrimental consequences in user experience. In this paper we propose a user-centric framework that addresses the disconnection between these two streams: Cloud2Bubble. The framework facilitates the design and development of smart systems. It aims at leveraging existing technology, such as environment sensors and personal devices, to aggregate localised user-related data - defined as a bubble - into the cloud. This aggregation later supports the delivery of personalised services, contextually relevant to users. The delivery of services with such characteristics has the potential to enhance quality of experience and influence user behaviour. A first iteration of the platform was developed and an evaluation in a simulated environment was performed with encouraging results. Thus, the platform will be further expanded for instantiation and evaluation in the context of urban public transports. We intend to investigate the effects of relevant service delivery in terms of enhancement of quality of experience and influencing user behaviour. The delivery of a service with these characteristics presents benefits for both users and service providers.},
booktitle = {Proceedings of the Third ACM Workshop on Mobile Cloud Computing and Services},
pages = {45–52},
numpages = {8},
keywords = {mobile cloud computing, policy computing, quality of experience, smart systems, user-centric development},
location = {Low Wood Bay, Lake District, UK},
series = {MCS '12}
}

@inproceedings{10.1145/3456887.3457105,
author = {Yue, Jin and Liu, Zeguo},
title = {Research on the Efficiency of Visual Communication in New Media Interface Design},
year = {2021},
isbn = {9781450389969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456887.3457105},
doi = {10.1145/3456887.3457105},
abstract = {In the new media, the design of visual communication interface can not only bring good visual experience for new media customers, but also effectively improve the efficiency of information retrieval and information transmission of media customers. However, there are some problems in most of the existing designs, such as insufficient attention and aesthetic ability. Therefore, this paper puts forward the research on the efficiency of visual communication in new media interface design. In this paper, the current situation of new media visual design is deeply studied, and the application of visual communication design in new media is analyzed. In view of the existing shortcomings, in order to improve the visual communication efficiency of new media, this paper focuses on the analysis of the basic characteristics of visual communication of new media, and puts forward the principle of readability. To upgrade the design scheme, we should pay attention to emotional tendency, ensure the unity of pictures and texts, and strengthen the interesting and interactive experience. In order to further understand the actual situation, this paper conducted a questionnaire survey. Through the analysis of the survey results, this paper believes that the new media interface visual design should pay more attention to the emotional experience of users, enrich the forms of expression, simplify the operation, and emphasize the practical performance, so as to effectively improve the efficiency of visual communication.},
booktitle = {2021 2nd International Conference on Computers, Information Processing and Advanced Education},
pages = {919–922},
numpages = {4},
keywords = {efficiency improvement, interface design, new media, visual communication},
location = {Ottawa, ON, Canada},
series = {CIPAE 2021}
}

@inproceedings{10.1145/2536146.2536150,
author = {Tekli, Joe and Rjeily, Antoine Abou and Chbeir, Richard and Tekli, Gilbert and Houngue, P\'{e}lagie and Yetongnon, Kokou and Abebe, Minale Ashagrie},
title = {Semantic to intelligent web era: building blocks, applications, and current trends},
year = {2013},
isbn = {9781450320047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536146.2536150},
doi = {10.1145/2536146.2536150},
abstract = {The Web has known a very fast evolution: going from the Web 1.0, known as Web of Documents where users are merely consumers of static information, to the more dynamic Web 2.0, known as social or collaborative Web where users produce and consume information simultaneously, and entering the more sophisticated Web 3.0, known as the Semantic Web by giving information a well-defined meaning so that it becomes more easily accessible by human users and automated processes. Fostering service intelligence and atomicity (the ability of autonomous services to interact automatically), remains one of the most upcoming challenges of the Semantic Web. This promotes the dawn of a new era: the Intelligent Web (Web 4.0), known as the Internet of Things (IoT), an extension of the Semantic Web where (physical/software) objects and services autonomously interact in a multimedia virtual environment, provided with embedded communication capabilities, common semantics and addressing schemes, promoting the concept of Digital Web Ecosystems where every where (human and software) agents collaborate, interact, compete, and evolve autonomously in order to automatically solve complex and dynamic problems. This paper briefly describes the recent evolution of the Web providing an overview of the technological breakthroughs contributing to this evolution, covering: knowledge bases and semantic data description, XML-based data representation and manipulation technologies (i.e., RDF, RDFS, OWL, and SPARQL) as well as the main challenges toward achieving the Intelligent Web: connectivity, semantic heterogeneity, collective knowledge management, collective intelligence, as well as data sustainability and evolution. We also present some of the main application domains characterizing the Intelligent (Semantic) Web, from information retrieval and content analysis, to systems status monitoring and improving business life-cycle through ubiquitous computing.},
booktitle = {Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems},
pages = {159–168},
numpages = {10},
keywords = {OWL, RDF, SPARQL, XML, data semantics, intelligent services, internet of things, knowledge base, semantic web, web},
location = {Luxembourg, Luxembourg},
series = {MEDES '13}
}

@inproceedings{10.1145/3477495.3531703,
author = {Zhao, Xiangyu and Xin, Xin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
title = {DRL4IR: 3rd Workshop on Deep Reinforcement Learning for Information Retrieval},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531703},
doi = {10.1145/3477495.3531703},
abstract = {Information retrieval (IR) systems have become an essential component in modern society to help users find useful information, which consists of a series of processes including query expansion, item recall, item ranking and re-ranking, etc. Based on the ranked information list, users can provide their feedbacks. Such an interaction process between users and IR systems can be naturally formulated as a decision-making problem, which can be either one-step or sequential. In the last ten years, deep reinforcement learning (DRL) has become a promising direction for decision-making, since DRL utilizes the high model capacity of deep learning for complex decision-making tasks. Recently, there have been emerging research works focusing on leveraging DRL for IR tasks. However, the fundamental information theory under DRL settings, the principle of RL methods for IR tasks, or the experimental evaluation protocols of DRL-based IR systems, has not been deeply investigated.To this end, we propose the third DRL4IR workshop (https://drl4ir.github.io) at SIGIR 2022, which provides a venue for both academia researchers and industry practitioners to present the recent advances of DRL-based IR system, to foster novel research, interesting findings, and new applications of DRL for IR. In the last two years, DRL4IR organized at SIGIR'20/21 was one of the most successful workshops and attracted over 200 workshop attendees each year. In this year, we will pay more attention to fundamental research topics and recent application advances, with an expectation of over 300 workshop participants.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3488–3491},
numpages = {4},
keywords = {deep reinforcement learning, information retrieval},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3378443,
author = {Carpineto, Claudio and Romano, Giovanni},
title = {An Experimental Study of Automatic Detection and Measurement of Counterfeit in Brand Search Results},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3378443},
doi = {10.1145/3378443},
abstract = {Brand search results are poisoned by fake ecommerce websites that infringe on the trademark rights of legitimate holders. In this article, we study how to tackle and measure this problem automatically. We present a pipeline with two machine learning stages that can detect the ecommerce websites present in the list of brand search results and distinguish between legitimate and fake ecommerce websites. For each classification task, we identify and extract suitable learning features and study their relative importance. Through a prototype system termed RI.SI.CO., we show that this approach is feasible, fast, and more accurate than both existing systems for trustworthiness assessment and non-expert humans. We next introduce two complementary metrics for evaluating the counterfeit incidence in brand search results: namely, a chart-based and a single-value measure. They allow us to analyze and compare counterfeit at various levels, including single brands within a specific sector as well as whole sectors. Experimenting with two luxury goods sectors, we report a number of interesting findings about how the main search parameters (e.g., search engine, query type, number of search results seen) affect counterfeiting and how this activity changes with time. On the whole, our research offers new insights and some very practical and useful means of analyzing and measuring counterfeit in brand search results, thus increasing awareness of and knowledge about this phenomenon and enabling targeted anti-counterfeiting actions.},
journal = {ACM Trans. Web},
month = {feb},
articleno = {6},
numpages = {35},
keywords = {Online counterfeit goods, cybercrime measurement, spam detection in web search results, trustworthiness assessment of eshops, website classification}
}

@inproceedings{10.1145/3538969.3543802,
author = {Ueda, Takahiro and Sasaki, Takayuki and Yoshioka, Katsunari and Matsumoto, Tsutomu},
title = {An Internet-Wide View of Connected Cars: Discovery of Exposed Automotive Devices},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3543802},
doi = {10.1145/3538969.3543802},
abstract = {As the number of connected cars increases, cyber-attacks targeting them become significant risks. Especially, On-Board Equipment (OBE) that is directly accessible from the Internet can be an immediate target. However, it is not known what kind of and how many connected automotive devices can be remotely accessed from the Internet and, if compromised, become an entry point for further attacks on in-vehicle networks. In this study, we investigate the prevalence of such exposed vehicular devices. We propose a discovery method that utilizes an Internet-wide scan engine and a regular web search engine to find Internet-facing OBE. Using the proposed method, we discovered 2,532 devices of 12 different OBE products across 27 countries. We also investigated the potential cyber-attack risks against the discovered devices. 11 out of the 12 products have security concerns for remote compromises, such as running Telnet or outdated server programs. Moreover, we found that nine products have the capability to connect to the in-vehicle network. We could confirm from the information displayed in their user interface that at least two of them indeed connected to the in-vehicle network. Additionally, we noticed three products expose privacy-sensitive information such as GPS location. We believe this result provides a lower bound of the security risk of Internet-facing vehicular devices.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {146},
numpages = {8},
keywords = {Censys, Connected cars, Discovery, Exposed cars, Internet-wide scan},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.5555/3026877.3026920,
author = {Angel, Sebastian and Setty, Srinath},
title = {Unobservable communication over fully untrusted infrastructure},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {Keeping communication private has become increasingly important in an era of mass surveillance and state-sponsored attacks. While hiding the contents of a conversation has well-known solutions, hiding the associated metadata (participants, duration, etc.) remains a challenge, especially if one cannot trust ISPs or proxy servers. This paper describes a communication system called Pung that provably hides all content and metadata while withstanding global adversaries. Pung is a key-value store where clients deposit and retrieve messages without anyone-- including Pung's servers--learning of the existence of a conversation. Pung is based on private information retrieval, which we make more practical for our setting with new techniques. These include a private multiretrieval scheme, an application of the power of two choices, and batch codes. These extensions allow Pung to handle 103\texttimes{} more users than prior systems with a similar threat model.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {551–569},
numpages = {19},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@article{10.1145/2809787,
author = {Costa, Alberto and Buccio, Emanuele Di and Melucci, Massimo},
title = {A Document Retrieval Model Based on Digital Signal Filtering},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2809787},
doi = {10.1145/2809787},
abstract = {Information retrieval (IR) systems are designed, in general, to satisfy the information need of a user who expresses it by means of a query, by providing him with a subset of documents selected from a collection and ordered by decreasing relevance to the query. Such systems are based on IR models, which define how to represent the documents and the query, as well as how to determine the relevance of a document for a query. In this article, we present a new IR model based on concepts taken from both IR and digital signal processing (like Fourier analysis of signals and filtering). This allows the whole IR process to be seen as a physical phenomenon, where the query corresponds to a signal, the documents correspond to filters, and the determination of the relevant documents to the query is done by filtering that signal. Tests showed that the quality of the results provided by this IR model is comparable with the state-of-the-art.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {6},
numpages = {37},
keywords = {DFT, Retrieval models, digital filtering, discrete Fourier transform}
}

@inproceedings{10.1145/2976749.2978370,
author = {Feng, Qian and Zhou, Rundong and Xu, Chengcheng and Cheng, Yao and Testa, Brian and Yin, Heng},
title = {Scalable Graph-based Bug Search for Firmware Images},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978370},
doi = {10.1145/2976749.2978370},
abstract = {Because of rampant security breaches in IoT devices, searching vulnerabilities in massive IoT ecosystems is more crucial than ever. Recent studies have demonstrated that control-flow graph (CFG) based bug search techniques can be effective and accurate in IoT devices across different architectures. However, these CFG-based bug search approaches are far from being scalable to handle an enormous amount of IoT devices in the wild, due to their expensive graph matching overhead. Inspired by rich experience in image and video search, we propose a new bug search scheme which addresses the scalability challenge in existing cross-platform bug search techniques and further improves search accuracy. Unlike existing techniques that directly conduct searches based upon raw features (CFGs) from the binary code, we convert the CFGs into high-level numeric feature vectors. Compared with the CFG feature, high-level numeric feature vectors are more robust to code variation across different architectures, and can easily achieve realtime search by using state-of-the-art hashing techniques. We have implemented a bug search engine, Genius, and compared it with state-of-art bug search approaches. Experimental results show that Genius outperforms baseline approaches for various query loads in terms of speed and accuracy. We also evaluated Genius on a real-world dataset of 33,045 devices which was collected from public sources and our system. The experiment showed that Genius can finish a search within 1 second on average when performed over 8,126 firmware images of 420,558,702 functions. By only looking at the top 50 candidates in the search result, we found 38 potentially vulnerable firmware images across 5 vendors, and confirmed 23 of them by our manual analysis. We also found that it took only 0.1 seconds on average to finish searching for all 154 vulnerabilities in two latest commercial firmware images from D-LINK. 103 of them are potentially vulnerable in these images, and 16 of them were confirmed.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {480–491},
numpages = {12},
keywords = {firmware security, graph encoding, machine learning},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/3132847.3133172,
author = {Vo, Hoang Tam and Mehedy, Lenin and Mohania, Mukesh and Abebe, Ermyas},
title = {Blockchain-based Data Management and Analytics for Micro-insurance Applications},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133172},
doi = {10.1145/3132847.3133172},
abstract = {In this paper, we demonstrate a blockchain-based solution for transparently managing and analyzing data in a pay-as-you-go car insurance application. This application allows drivers who rarely use cars to only pay insurance premium for particular trips they would like to travel. One of the key challenges from database perspective is how to ensure all the data pertaining to the actual trip and premium payment made by the users are transparently recorded so that every party in the insurance contract including the driver, the insurance company, and the financial institution is confident that the data are tamper-proof and traceable.  Another challenge from information retrieval perspective is how to perform entity matching and pattern matching on customer data as well as their trip and claim history recorded on the blockchain for intelligent fraud detection. Last but not least, the drivers' trip history, once have been collected sufficiently, can be much valuable for the insurance company to do offline analysis and build statistics on past driving behaviour and past vehicle runtime. These statistics enable the insurance company to offer the users with transparent and individualized insurance quotes. Towards this end, we develop a blockchain-based solution for micro-insurance applications that transparently keeps records and executes smart contracts depending on runtime conditions while also connecting with off-chain analytic databases.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2539–2542},
numpages = {4},
keywords = {blockchain, data analytics, data management, information management, information retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3482632.3487487,
author = {Shen, Wenbin and Wu, Qi and Chen, Su},
title = {Enterprise intelligent reimbursement system based on OCR technology and SVM algorithm},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487487},
doi = {10.1145/3482632.3487487},
abstract = {In order to realize the intelligent reimbursement and financial information management of large and medium-sized state-owned enterprises and improve the financial information network management and information level of large and medium-sized state-owned enterprises, an enterprise intelligent reimbursement system based on OCR technology and support vector machine (SVM) algorithm is proposed. The system design is divided into general structure design, algorithm design of intelligent reimbursement and financial information management of large and medium-sized state-owned enterprises, and software development design. Information retrieval of intelligent reimbursement and financial information management of large and medium-sized state-owned enterprises is carried out by using financial accounting operation and maintenance information extraction method, and financial accounting operation and maintenance correlation feature quantity of financial information of large and medium-sized state-owned enterprises is extracted, and similarity feature analysis model of intelligent reimbursement and financial information management of large and medium-sized state-owned enterprises is constructed. The OCR technology and SVM algorithm are used to realize the network modular design of accounting management and intelligent reimbursement of large and medium-sized state-owned enterprises and the networking control design of Internet of Things. RFID radio frequency tags are used to identify the two-dimensional code tags of financial information of large and medium-sized state-owned enterprises. The software development and network modular design under OCR technology and SVM algorithm are used to realize accounting management and intelligent reimbursement of large and medium-sized state-owned enterprises. Tests show that the designed intelligent reimbursement system for large and medium-sized state-owned enterprises has good output stability, and the enterprise accounting management and intelligent reimbursement informationization level is high.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2639–2646},
numpages = {8},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@proceedings{10.1145/3616855,
title = {WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 17th ACM International Conference on Web Search and Data Mining - WSDM 2024. WSDM is one of the premier conferences in the fields of web search and data mining, with a dynamic and growing community from academia and industry. After two years of virtual conferences and in-person conferences in Singapore, the 2024 edition is an in-person conference with virtual elements. We hope you enjoy the conference at the "Centro Internacional de Congresos de Yucatan (CIC)" in Merida from March 4 to March 8, 2024.We are excited to kick off the program with a dynamic mix of Tutorials and Industry Day. Our seven tutorials will cover a broad range of search and data mining topics. Industry Day will provide valuable insights from leaders at major technology companies. The core technical program continues WSDM's tradition of a single-track format, featuring 109 thought-provoking papers from both academic and industry experts. We're honored to have inspiring keynote speakers each day: Nicolas Christin (CMU), Elizabeth Reid (Google), and Saiph Savage (Civic A.I. Lab). Additionally, 17 interactive demonstrations will showcase the latest prototypes and systems. The final day offers a stimulating Doctoral Consortium and six engaging workshops on topics including integrity in social networks, large language model for society, psychology-informed information access system, interactive and scalable information retrieval system and machine learning on graphs. WSDM 2024 proudly presents WSDM day on information retrieval and Web in the region. WSDM Cup Day highlights finalists' presentations addressing challenges in Conversational Multi-Doc QA. This diverse and stimulating program promises to be an enriching experience for all!.},
location = {Merida, Mexico}
}

@article{10.1145/3437479.3437482,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3437479.3437482},
doi = {10.1145/3437479.3437482},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.htmlMini-editorial (PGN)This is now my 45th year with involvement in SEN. When can I quit? Sadly, the risks continue to be pervasive, and avoiding them is highly relevant to what principled software/system engineering should entail. We need ap- proaches to systems that consider trustworthiness as a col- lection of -ilities all of which can be critical, and which in many cases are intricately related; they cannot be solved in isolation. Trustworthiness requires reliablity, security, and much more especially when human safety is in- volved. And every system is likely to have critical aws that can be exploited or triggered accidentally. My Octo- ber 2020 CACM Inside Risks column, A Holistic View of Fu- ture Risks (http://www.csl.sri.com/neumann/cacm250.pdf) attempts to illustrate some of that. If you have not seen it, I would welcome some feedback.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {13–18},
numpages = {6}
}

@inproceedings{10.1145/3349266.3351405,
author = {Zheng, Yong},
title = {A Course on Applied AI and Data Science: Recommender Systems},
year = {2019},
isbn = {9781450369213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349266.3351405},
doi = {10.1145/3349266.3351405},
abstract = {Artificial intelligence (AI) and data science have become one of the most popular curricula in the computing educations. Plenty of theories, optimizations and math are involved in these courses, which results in a higher degree of difficulty for students to learn, not to mention the students without specializations in computer science or information technology. Beyond the complicated knowledge and theories, students may prefer to learn and focus on applied AI or data science which refer to the knowledge or skills for practical problem-solving and real-world applications. For example, information retrieval has been listed as one of the curricula in several undergraduate and/or graduate programs in the AI or data science programs. This paper describes a course that focuses on the topic of recommender systems which is in high demand in both academia and industries. This course has been extremely successful at the authors' institutions. In this paper, we introduce the course's objectives, structure and methodologies, discuss possible ways to deliver hands-on practice, summarize the outcomes, and finally present the lessons learned, as well as the feedbacks from the students. These experience could be useful and may give advice to other educators looking to create a similar course in their program.},
booktitle = {Proceedings of the 20th Annual SIG Conference on Information Technology Education},
pages = {43–48},
numpages = {6},
keywords = {curriculum, data science, it education, recommender system},
location = {Tacoma, WA, USA},
series = {SIGITE '19}
}

@article{10.1109/TNET.2023.3262651,
author = {Li, Ruixuan and Jia, Xiaofeng and Zhang, Zhenyong and Shao, Jun and Lu, Rongxing and Lin, Jingqiang and Jia, Xiaoqi and Wei, Guiyi},
title = {A Longitudinal and Comprehensive Measurement of DNS Strict Privacy},
year = {2023},
issue_date = {Dec. 2023},
publisher = {IEEE Press},
volume = {31},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3262651},
doi = {10.1109/TNET.2023.3262651},
abstract = {The DNS privacy protection mechanisms, DNS over TLS (DoT) and DNS over HTTPS (DoH), only work correctly if both the server and client support the Strict Privacy profile and no vulnerability exists in the implemented TLS/HTTPS. A natural question then arises: what is the landscape of DNS Strict Privacy? To this end, we provide the first longitudinal and comprehensive measurement of DoT/DoH deployments in recursive resolvers, authoritative servers, and browsers. With the collected data, we find the number of DoT/DoH servers increased substantially during our ten-month-long scan. However, around 60% of DoT and 44% of DoH recursive resolver certificates are invalid. Worryingly, our measurements confirm the centralization problem of DoT/DoH. Furthermore, we classify DNS Strict Privacy servers into four levels according to daily scanning results on TLS/HTTPS-related security features. Unfortunately, around 25% of DoH Strict Privacy recursive resolvers fail to meet the minimum level requirements. To help the Internet community better perceive the landscape of DNS Strict Privacy, we implement a DoT/DoH server search engine and recommender system. Additionally, we investigate five popular browsers across four operating systems and find some inconsistent behavior with their DNS privacy implementations. For example, Firefox in Windows, Linux, and Android allows DoH communication with the server without the SAN certificate. At last, we advocate that all participants head together for a bright DNS Strict Privacy landscape by discussing current hindrances and controversies in DNS privacy.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {2793–2808},
numpages = {16}
}

@inproceedings{10.1145/3495018.3501163,
author = {Guo, Yuling},
title = {Collaborative control method of library literature resources information based on digital information system},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3501163},
doi = {10.1145/3495018.3501163},
abstract = {In order to improve the information sharing and collaborative control ability of library literature resources, this paper puts forward a collaborative control method of library literature resources information based on digital information system. The paper-electricity synchronous control technology is adopted to establish the parameter analysis model of library literature resource information collaborative control, build the distributed storage structure model of library literature resource information collaborative paper-electricity synchronous, combine the feature reconstruction and reorganization method to restructure library literature resource information collaborative paper-electricity synchronous data, and extract the association regular feature quantity of library literature resource information collaborative paper-electricity synchronous data in high-dimensional feature space. The fuzzy correlation fusion method is used for information fusion and feature scheduling of library document resources information collaborative paper-electricity synchronization data, and the spectrum analysis method is used for paper-electricity synchronization and information retrieval. The role hierarchy tree is used for classification control in the process of library document resources information collaborative paper-electricity synchronization, which improves the access and scheduling ability of library document resources information collaborative paper-electricity synchronization data. The simulation results show that this method is adaptive to the paper-electricity synchronous control of library literature resources information collaboration, and the recall rate of library literature resources data access is high and the accuracy is good.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {2683–2689},
numpages = {7},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@book{10.1145/3336323,
author = {Freeman, Peter A. and Adrion, W. Richards and Aspray, William},
title = {Computing and the National Science Foundation, 1950--2016: Building a Foundation for Modern Computing},
year = {2019},
isbn = {9781450372763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This organizational history relates the role of the National Science Foundation (NSF) in the development of modern computing. Drawing upon new and existing oral histories, extensive use of NSF documents, and the experience of two of the authors as senior managers, this book describes how NSF's programmatic activities originated and evolved to become the primary source of funding for fundamental research in computing and information technologies.The book traces how NSF's support has provided facilities and education for computing usage by all scientific disciplines, aided in institution and professional community building, supported fundamental research in computer science and allied disciplines, and led the efforts to broaden participation in computing by all segments of society.Today, the research and infrastructure facilitated by NSF computing programs are significant economic drivers of American society and industry. For example, NSF supported work that led to the first widelyused web browser, Netscape; sponsored the creation of algorithms at the core of the Google search engine; facilitated the growth of the public Internet; and funded research on the scientific basis for countless other applications and technologies. NSF has advanced the development of human capital and ideas for future advances in computing and its applications.This account is the first comprehensive coverage of NSF's role in the extraordinary growth and expansion of modern computing and its use. It will appeal to historians of computing, policy makers and leaders in government and academia, and individuals interested in the history and development of computing and the NSF.}
}

@inproceedings{10.1145/3307772.3328288,
author = {Hviid, Jakob and Johansen, Aslak and Sangogboye, Fisayo Caleb and Kj\ae{}rgaard, Mikkel Baun},
title = {Enabling Auto-Configuring Building Services: The Road to Affordable Portable Applications for Smart Grid Integration},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328288},
doi = {10.1145/3307772.3328288},
abstract = {The transition to green energy is imposing new requirements on the energy sector, such as managing an energy flow through the system that is significantly less predictable than traditional energy sources. Technologies like demand response (DR) can help influence the consumer's energy use, but is costly to implement, often to a degree that vastly overshadows the benefits for the consumer and grid operator. Reducing initial investment cost for deployments is, therefore, a considerable factor for the success of DR. One approach to DR is using building operating systems (BOS) to facilitate the functionality. This paper seeks to reduce the cost of deployments of BOSes, for demand response purposes, by creating the tooling needed for automatic configuration of BOS services. Building on top of an existing BOS, tooling is created to support services from the first introduction into the BOS environment, to the exposure of its functionality to the rest of the system, effectively supporting service discovery and publication at runtime. The tooling is evaluated based on how it impacts the BOS ecosystem, but also the business ecosystem around a BOS. Results show that the tooling requires only minor changes to existing services, and that adding new services to the BOS, using the new tooling, can be achieved with little effort compared to traditional installation methods in the ecosystem. Also, shifting implementation complexity and cost from entrepreneurs to application developers, allows for reducing costs significantly, as application developers target multiple buildings for each implementation. The addition of the new tooling allows the them to potentially create new markets and products.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {68–77},
numpages = {10},
keywords = {Dependency Resolution, Ontology, Service Abstraction, Service Discovery},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@article{10.1145/3285029,
author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
title = {Deep Learning Based Recommender System: A Survey and New Perspectives},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3285029},
doi = {10.1145/3285029},
abstract = {With the growing volume of online information, recommender systems have been an effective strategy to overcome information overload. The utility of recommender systems cannot be overstated, given their widespread adoption in many web applications, along with their potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also to the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. The field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning-based recommender systems. More concretely, we provide and devise a taxonomy of deep learning-based recommendation models, along with a comprehensive summary of the state of the art. Finally, we expand on current trends and provide new perspectives pertaining to this new and exciting development of the field.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {5},
numpages = {38},
keywords = {Recommender system, deep learning, survey}
}

@inproceedings{10.1145/3079368.3079383,
author = {Avila, Humberto Rodriguez and Boix, Elisa Gonzalez and De Meuter, Wolfgang},
title = {An Elixir library for programming concurrent and distributed embedded systems},
year = {2017},
isbn = {9781450348362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079368.3079383},
doi = {10.1145/3079368.3079383},
abstract = {Development of embedded systems has been receiving in last decade a considerable attention for both academy and industry. Advances in hardware miniaturization, in particular, single-boards computers like Raspberry Pi, Beaglebone Black, allow for the use of high-level programming languages. Still, the greatest challenges when developing IoT applications are distribution and coordination. To date, mainstream languages (e.g. Java, JavaScript, Python) lack high- level abstractions to deal with distribution and coordination. For example, implementing standard leader election algorithms (e.g., Ring, Bully) with these languages, force developers to write code on top of low-level networking APIs. As a result, programmers need to write over and over, aspects like the discovery of peers, coordination, failure handling necessary for implementing distributed algorithms. Furthermore, craft and deploy such systems on embedded devices is also complex, due to hardware restrictions and software dependencies. In this demo, we show how a concurrent and distributed language with a rich macro system could in an easy and declarative way solve these problems. Our demo recreates a leader election algorithm between the nodes in an embedded environment by employing a Raspberry Pi cluster. The code for the leader election algorithm is built on top of a macro-based Elixir library which includes abstractions for distribution and coordination. Developers can use this library to translate core operations described by the Bully algorithm to declarative macro-constructs. This library also abstracts developers of communication and network concerns (e.g. service discovery and network monitoring). Furthermore, it includes an implementation of an Elixir behaviour (design pattern) to provide a base skeleton for developing leader-election applications.},
booktitle = {Companion Proceedings of the 1st International Conference on the Art, Science, and Engineering of Programming},
articleno = {6},
numpages = {1},
keywords = {Concurrent Programming, Distributed Programming, Embedded systems, Leader Election},
location = {Brussels, Belgium},
series = {Programming '17}
}

@article{10.1109/TNET.2022.3157654,
author = {Li, Meng and Zhu, Liehuang and Zhang, Zijian and Lal, Chhagan and Conti, Mauro and Alazab, Mamoun},
title = {User-Defined Privacy-Preserving Traffic Monitoring Against n-by-1 Jamming Attack},
year = {2022},
issue_date = {Oct. 2022},
publisher = {IEEE Press},
volume = {30},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3157654},
doi = {10.1109/TNET.2022.3157654},
abstract = {Traffic monitoring services collect traffic reports and respond to users’ traffic queries. However, the reports and queries may reveal the user’s identity and location. Although different anonymization techniques have been applied to protect user privacy, a new security threat arises, namely, n-by-1 jamming attack, in which an anonymous contributing driver impersonates &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$n$ &lt;/tex-math&gt;&lt;/inline-formula&gt; drivers and uploads &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$n$ &lt;/tex-math&gt;&lt;/inline-formula&gt; normal reports by using &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$n$ &lt;/tex-math&gt;&lt;/inline-formula&gt; reporting devices. Such an attack will mislead the traffic monitoring service provider and further degrade the service quality. Existing traffic monitoring services do not support customized queries, and private information retrieval techniques cannot be applied directly in traffic monitoring. We formally define the new attack and propose a traffic monitoring scheme TraJ to defend the attack and achieve user-defined location privacy. Specifically, we bridge anonymous contributing drivers without disclosing their speed set by using private set intersection. Each RSU collects time traffic reports and structures a weighted proximity graph to filter out malicious colluding drivers. We design a user-defined privacy-preserving query method by encoding complex road network. We leverage the uploading phase from private aggregation to collect traffic conditions and allow requesting drivers to dynamically and privately query traffic conditions. We provide a formal analysis of TraJ to prove its privacy and security properties. We also construct a prototype based on a real-world dataset and Android smartphones to demonstrate its feasibility and efficiency. A formal analysis demonstrates the privacy and security properties. Extensive experiments illustrate the performance and defense efficacy.},
journal = {IEEE/ACM Trans. Netw.},
month = {mar},
pages = {2060–2073},
numpages = {14}
}

@inproceedings{10.1145/3582197.3582201,
author = {Fan, Xing and Pan, Wei and Huang, Qingqing},
title = {Research on Chinese short text semantic matching based on lightweight ERNIE},
year = {2023},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582197.3582201},
doi = {10.1145/3582197.3582201},
abstract = {Semantic matching of short texts is a hot research problem in the field of natural language processing, which has a wide range of application needs in information retrieval, dialogue system, text paraphrase questions, and question answering system etc. Short texts have the characteristics of less information and lack of contextual background. The existing semantic matching methods for short texts also generally have the problem of low matching accuracy. With the speedy development of deep learning technology, various models of deep learning have been extensively used in natural language processing and achieved good results. In this paper, we design a FERNIE model based on a lightweight pre-training model ERNIE3.0-medium of BERT architecture, which integrates low-level features and high-level features. Experiments on several datasets show that the FERNIE model has good results in short text semantic matching, and the accuracy is further improved compared to ERNIE3.0-medium.},
booktitle = {Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
pages = {22–28},
numpages = {7},
keywords = {BERT, ERNIE, feature fusion, lightweight, short text matching},
location = {Shanghai, China},
series = {ICIT '22}
}

@inproceedings{10.5555/2555523.2555590,
author = {Ng, Joanna W.},
title = {From representational state transfer (REST) to representational action state transfer (REAST): enabling smarter web interactions for web tasking},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Today, the web is designed for information retrieval. Its current interaction model is web browsing. However, browsing is not an intuitive interaction model for web tasking. This paper provides a solution by introducing an architectural style called REpresentational Action State Transfer (REAST), extended from the classic architectural style of REpresentational State Transfer (REST). REAST represents the action state of the resource instead of the resource data state as in REST. By transferring the representation of action state of the resource between the requesting processer and the resource server in the architectural style of REAST, instead of transferring the data state of the resource as in the architectural style of REST, machine operated web interfaces with the dispensability of human web users is, for the first time, made possible. This opens up opportunities for new interaction model design that are better fit for web tasking where web browsing falls short. With its RESTful inheritance, REASTful resources share universal resource representation and the unified interface, thus have the architecturally built-in interoperability between enterprise resource items and items from the Internet of Things, in both machine automation and mixed initiatives between machine and human web users. The RESTful inheritance also means that REASTful resources relate in execution under the constraint of hypermedia controls. This enables the building of resource oriented intelligent web agents that can work generically across the web without domain specificity and can execute on actions that are non-preprogrammed. This paper coined such Resource Oriented Intelligent Agents 'the BOT'.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {422–429},
numpages = {8},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.1145/2968219.2971352,
author = {Haus, Michael},
title = {System approach towards private proximity services},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2971352},
doi = {10.1145/2968219.2971352},
abstract = {The problem domain of my PhD thesis is divided into two parts. First, we introduce position aware systems including the well-known Location-based Services (LBS) and Proximity-based Services (PBS) as subclass of LBS. The study [33] reports that 19% of the world's mobile users already using LBS and the most popular application is the navigation via maps and GPS signal. Furthermore, one in five (22%) of LBS users enrich their social lives by finding friends in the nearby environment. For example, MobiClique [26] alerts users when other users are in physical proximity and share a relationship with them based on profile and friend list. The LBS are based upon the absolute position of an user to answer the question "where we are?". In contrast, PBS are based upon context information to find co-location with other points of interest to answer the question "who are we with?". The goal of LBS and PBS is to improve the users' daily lives by providing a personalized service to enable sharing of location information and location-aware information retrieval. Therefore, the LBS focus on a centralized architecture, the location server acts as Trusted Party (TP) which receives coordinates from the users to provide location-specific information, e.g. nearby friends. Figure 1(a) shows the common LBS architecture consisting of four major entities: mobile devices, global positioning systems, communication networks and service providers [29]. However, the assumption of a TP is questionable [36]. Besides that, most of the LBS use global positioning systems, which limits their functionality to outdoor environments, although people spend the majority of their time indoors [2].},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {417–422},
numpages = {6},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3099023.3099085,
author = {Ardissono, Liliana and Gena, Cristina and Kuflik, Tsvi},
title = {UMAP 2017 PATCH 2017: Personalized Access to Cultural Heritage Organizers' Welcome},
year = {2017},
isbn = {9781450350679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3099023.3099085},
doi = {10.1145/3099023.3099085},
abstract = {Since 2007, the PATCH workshop series have been a gathering place for researchers and professionals from various countries and institutions to discuss the topics of digital access to Cultural Heritage and specifically the personalization aspects of this process. Due to this rich history, the reach of the PATCH workshop in various research communities is extensive. PATCH 2017 is another link in the long chain of PATCH events and we hope that it will point out future research challenges and directions and its success will pave the way to future events. Following the successful series of PATCH workshops, PATCH 2017 is organized as the meeting point between state of the art cultural heritage research and personalization -- using any kind of technology, while focusing on ubiquitous and adaptive scenarios, to enhance the personal experience in cultural heritage sites. The workshop is aimed at bringing together researchers and practitioners who are working on various aspects of cultural heritage and are interested in exploring the potential of state of the art of personalized approaches that may enhance the CH visit experience. In this edition we received 7 submissions, 2 full papers (28%), 3 short papers (42%), 1 demo paper (14%), and 1 position paper (14%). To select the workshop papers a peer-review process was carried out. At least three members of the Program Committee (which is listed below) were assigned to each paper. As result, we have accepted all the papers, and downgraded 1 short paper to position paper. The 2017 workshop includes contributions covering diverse research aspects, such as: advanced brain informatics and IoT approaches to understand museum visitors' behavior or to personalize their visits; novel models for information retrieval, information visualization and automated personalized content generation, social recommendation of CH information based on Linked Open Data; and a study of the interplay among human cognitive processing differences and cultural heritage activities towards gaming experience and performance. We believe that this is a nice spectrum of topics and we wish you to enjoy reading the workshop proceedings. The contributions collected in this workshop reflect these topics.},
booktitle = {Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {317–319},
numpages = {3},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@article{10.1145/3154525,
author = {Fathy, Yasmin and Barnaghi, Payam and Tafazolli, Rahim},
title = {Large-Scale Indexing, Discovery, and Ranking for the Internet of Things (IoT)},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3154525},
doi = {10.1145/3154525},
abstract = {Network-enabled sensing and actuation devices are key enablers to connect real-world objects to the cyber world. The Internet of Things (IoT) consists of the network-enabled devices and communication technologies that allow connectivity and integration of physical objects (Things) into the digital world (Internet). Enormous amounts of dynamic IoT data are collected from Internet-connected devices. IoT data are usually multi-variant streams that are heterogeneous, sporadic, multi-modal, and spatio-temporal. IoT data can be disseminated with different granularities and have diverse structures, types, and qualities. Dealing with the data deluge from heterogeneous IoT resources and services imposes new challenges on indexing, discovery, and ranking mechanisms that will allow building applications that require on-line access and retrieval of ad-hoc IoT data. However, the existing IoT data indexing and discovery approaches are complex or centralised, which hinders their scalability. The primary objective of this article is to provide a holistic overview of the state-of-the-art on indexing, discovery, and ranking of IoT data. The article aims to pave the way for researchers to design, develop, implement, and evaluate techniques and approaches for on-line large-scale distributed IoT applications and services.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {29},
numpages = {53},
keywords = {Internet of things (IoT), discovery, indexing, large-scale data, ranking, wireless sensor network (WSN)}
}

@inproceedings{10.1145/2964284.2973833,
author = {Takahashi, Toru and Kudo, Yuta and Ishiyama, Rui},
title = {Intelli-Wrench: Smart Navigation Tool for Mechanical Assembly and Maintenance},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973833},
doi = {10.1145/2964284.2973833},
abstract = {We present Intelli-Wrench, a smart tool for providing just-in-time information for mechanical assembly and maintenance workers, which releases the workers from miss-operation and annoying manual documents. The Intelli-Wrench captures the image of a bolt head as its "fingerprint" by using FIBAR (Fingerprint Imaging by Binary Angular Reflection) imaging method. The fingerprint image is then uploaded to a cloud server and matched with parts databases in order to provide the specific information associated with the bolt. Receiving the information, the Intelli-Wrench informs the workers about the designated location and torque requirements of the bolt. This direct integration of information retrieval into a tangible tool provides immediate access to relevant information otherwise found in manual documents. Furthermore, the Intelli-Wrench automatically logs the interaction and eliminates the annoying pointing-and-calling procedure which is the traditional way of secure servicing. We demonstrate a working prototype and interaction scenario.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {752–753},
numpages = {2},
keywords = {FIBAR, IoT, fingerprint of things, image recognition, individual identification, traceability},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.1145/2795403.2795413,
author = {Ahlers, Dirk and Wilde, Erik and Martins, Bruno},
title = {Report on the Fourth Workshop on Location and the Web (LocWeb 2014)},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/2795403.2795413},
doi = {10.1145/2795403.2795413},
abstract = {Location is a major aspect behind many approaches in Information Retrieval, naturally appearing in diverse forms and as a cross-cutting topic. To give the location topic an adequate venue, LocWeb 2014 restarted a workshop series at the intersection of geospatial search, information management, and Web architecture, with a main focus on location-aware information access. LocWeb has in the past been a 'travelling' workshop, gathering views from different communities, but always centered on the topic of location. As previously, contributions to the workshop reflected a multitude of fields that demand and use location features and geospatial information, and LocWeb 2014 featured presentations that look at the topic of location on the Web from an interdisciplinary perspective. Reflecting this interdisciplinarity, LocWeb 2014 consisted of an interesting variety of contributions from a range of topics: one keynote, four long and two short papers, and a discussion session. This report gives an overview of the workshop and summarizes its major contributions.},
journal = {SIGIR Forum},
month = {jun},
pages = {35–40},
numpages = {6}
}

@inproceedings{10.1145/3292500.3330825,
author = {Wang, Pinghui and Qi, Yiyan and Zhang, Yuanming and Zhai, Qiaozhu and Wang, Chenxu and Lui, John C.S. and Guan, Xiaohong},
title = {A Memory-Efficient Sketch Method for Estimating High Similarities in Streaming Sets},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330825},
doi = {10.1145/3292500.3330825},
abstract = {Estimating set similarity and detecting highly similar sets are fundamental problems in areas such as databases, machine learning, and information retrieval. MinHash is a well-known technique for approximating Jaccard similarity of sets and has been successfully used for many applications such as similarity search and large scale learning. Its two compressed versions, b-bit MinHash and Odd Sketch, can significantly reduce the memory usage of the original MinHash method, especially for estimating high similarities (i.e., similarities around 1). Although MinHash can be applied to static sets as well as streaming sets, of which elements are given in a streaming fashion and cardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd Sketch fail to deal with streaming data. To solve this problem, we design a memory efficient sketch method, MaxLogHash, to accurately estimate Jaccard similarities in streaming sets. Compared to MinHash, our method uses smaller sized registers (each register consists of less than 7 bits) to build a compact sketch for each set. We also provide a simple yet accurate estimator for inferring Jaccard similarity from MaxLogHash sketches. In addition, we derive formulas for bounding the estimation error and determine the smallest necessary memory usage (i.e., the number of registers used for a MaxLogHash sketch) for the desired accuracy. We conduct experiments on a variety of datasets, and experimental results show that our method MaxLogHash is about 5 times more memory efficient than MinHash with the same accuracy and computational cost for estimating high similarities.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {25–33},
numpages = {9},
keywords = {Jaccard coefficient similarity, sketch, streaming algorithms},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3582197.3582250,
author = {Xu, Sa},
title = {A Knowledge Reasoning Model Based on Non-Factoid Information Enhancement},
year = {2023},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582197.3582250},
doi = {10.1145/3582197.3582250},
abstract = {Q&amp;A system plays an increasingly important role in the modern society with information explosion, and knowledge reasoning model (KRM) is the main research content of Q&amp;A system. Existing knowledge reasoning models are mainly divided into text-based information retrieval (IR) and knowledge graph embedding (KGE). KGE is superior to IR in terms of storage and reasoning capabilities for massive factoid information, but lacks the ability to reason non-factoid information, merely focus on the mining of structural information without the semantic information. We proposed a knowledge reasoning model based on non-factoid information enhancement (NFE-KRM) in scenic Q&amp;A. It realizes the KGE integrates semantic information (SIKGE) and the unified semantic embedding space (USES), so that NFE-KRM has the ability to answer both factoid and non-factoid questions. We have used a large number of experiments to prove that SIKGE gets a better performance on Mean Rank and Hits@10. NFE-KRM's F1 score and accuracy on the mixed dataset are both competitive.},
booktitle = {Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
pages = {318–323},
numpages = {6},
keywords = {Q&amp;A system, Information Retrieval, Knowledge Graph Embedding, NLP.},
location = {Shanghai, China},
series = {ICIT '22}
}

@inproceedings{10.1145/3613424.3614273,
author = {Lee, Kyungmi and Yan, Mengjia and Emer, Joel and Chandrakasan, Anantha},
title = {SecureLoop: Design Space Exploration of Secure DNN Accelerators},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614273},
doi = {10.1145/3613424.3614273},
abstract = {Deep neural networks (DNNs) are gaining popularity in a wide range of domains, ranging from speech and video recognition to healthcare. With this increased adoption comes the pressing need for securing DNN execution environments on CPUs, GPUs, and ASICs. While there are active research efforts in supporting a trusted execution environment (TEE) on CPUs, the exploration in supporting TEEs on accelerators is limited, with only a few solutions available [18, 19, 27]. A key limitation along this line of work is that these secure DNN accelerators narrowly consider a few specific architectures. The design choices and the associated cost for securing these architectures do not transfer to other diverse architectures. This paper strives to address this limitation by developing a design space exploration tool for supporting TEEs on diverse DNN accelerators. We target secure DNN accelerators equipped with cryptographic engines where the cryptographic operations are closely coupled with the data movement in the accelerators. These operations significantly complicate the scheduling for DNN accelerators, as the scheduling needs to account for the extra on-chip computation and off-chip memory accesses introduced by these cryptographic operations, and even needs to account for potential interactions across DNN layers. We tackle these challenges in our tool, called SecureLoop, by introducing a scheduling search engine with the following attributes: 1) considers the cryptographic overhead associated with every off-chip data access, 2) uses an efficient modular arithmetic technique to compute the optimal authentication block assignment for each individual layer, and 3) uses a simulated annealing algorithm to perform cross-layer optimizations. Compared to the conventional schedulers, our tool finds the schedule for secure DNN designs with up to 33.2% speedup and 50.2% improvement of energy-delay-product.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {194–208},
numpages = {15},
keywords = {Trusted execution environment, accelerator scheduling, neural networks},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/2258056.2258059,
author = {Guo, Tian and Yan, Zhixian and Aberer, Karl},
title = {An adaptive approach for online segmentation of multi-dimensional mobile data},
year = {2012},
isbn = {9781450314428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2258056.2258059},
doi = {10.1145/2258056.2258059},
abstract = {With increasing availability of mobile sensing devices including smartphones, online mobile data segmentation becomes an important topic in reconstructing and understanding mobile data. Traditional approaches like online time series segmentation either use a fixed model or only apply an adaptive model on one dimensional data; it turns out that such methods are not very applicable to build online segmentation for multiple dimensional mobile sensor data (e.g., 3D accelerometer or 11 dimension features like 'mean', 'variance', 'covariance', 'magnitude', etc).In this paper, we design an adaptive model for segmenting real-time accelerometer data from smartphones, which is able to (a) dynamically select suitable dimensions to build a model, and (b) adaptively pick up a proper model. In addition to using the traditional residual-style regression errors to evaluate time series segmentation, we design a rich metric to evaluate mobile data segmentation results, including (1) traditional regression error, (2) information retrieval style measurements (i.e., precision, recall, F-measure), and (3) segmentation time delay.},
booktitle = {Proceedings of the Eleventh ACM International Workshop on Data Engineering for Wireless and Mobile Access},
pages = {7–14},
numpages = {8},
keywords = {adaptive model creation, feature selection, mobile data mining, online segmentation},
location = {Scottsdale, Arizona},
series = {MobiDE '12}
}

@article{10.14778/3551793.3551830,
author = {Paparrizos, John and Boniol, Paul and Palpanas, Themis and Tsay, Ruey S. and Elmore, Aaron and Franklin, Michael J.},
title = {Volume under the surface: a new accuracy evaluation measure for time-series anomaly detection},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551830},
doi = {10.14778/3551793.3551830},
abstract = {Anomaly detection (AD) is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. In contrast to other domains where AD mainly focuses on point-based anomalies (i.e., outliers in standalone observations), AD for time series is also concerned with range-based anomalies (i.e., outliers spanning multiple observations). Nevertheless, it is common to use traditional point-based information retrieval measures, such as Precision, Recall, and F-score, to assess the quality of methods by thresholding the anomaly score to mark each point as an anomaly or not. However, mapping discrete labels into continuous data introduces unavoidable shortcomings, complicating the evaluation of range-based anomalies. Notably, the choice of evaluation measure may significantly bias the experimental outcome. Despite over six decades of attention, there has never been a large-scale systematic quantitative and qualitative analysis of time-series AD evaluation measures. This paper extensively evaluates quality measures for time-series AD to assess their robustness under noise, misalignments, and different anomaly cardinality ratios. Our results indicate that measures producing quality values independently of a threshold (i.e., AUC-ROC and AUC-PR) are more suitable for time-series AD. Motivated by this observation, we first extend the AUC-based measures to account for range-based anomalies. Then, we introduce a new family of parameter-free and threshold-independent measures, VUS (Volume Under the Surface), to evaluate methods while varying parameters. Our findings demonstrate that our four measures are significantly more robust in assessing the quality of time-series AD methods.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2774–2787},
numpages = {14}
}

@inproceedings{10.1145/3372278.3388041,
author = {Dao, Minh-Son and Fjeld, Morten and Biljecki, Filip and Yavanoglu, Uraz and Dong, Mianxiong},
title = {ICDAR'20: Intelligent Cross-Data Analysis and Retrieval},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3388041},
doi = {10.1145/3372278.3388041},
abstract = {The First International Workshop on "Intelligence Cross-Data Analytics and Retrieval" (ICDAR'20) welcomes any theoretical and practical works on intelligence cross-data analytics and retrieval to bring the smart-sustainable society to human beings. We have witnessed the era of big data where almost any event that happens is recorded and stored either distributedly or centrally. The utmost requirement here is that data came from different sources, and various domains must be harmonically analyzed to get their insights immediately towards giving the ability to be retrieved thoroughly. These emerging requirements lead to the need for interdisciplinary and multidisciplinary contributions that address different aspects of the problem, such as data collection, storage, protection, processing, and transmission, as well as knowledge discovery, retrieval, and security and privacy. Hence, the goal of the workshop is to attract researchers and experts in the areas of multimedia information retrieval, machine learning, AI, data science, event-based processing and analysis, multimodal multimedia content analysis, lifelog data analysis, urban computing, environmental science, atmospheric science, and security and privacy to tackle the issues as mentioned earlier.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {580–581},
numpages = {2},
keywords = {artificial intelligence, cross-data, data analytic, information retrieval, knowledge discovery, security and privacy},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@article{10.1145/3615952.3615956,
author = {Khan, Arijit},
title = {Knowledge Graphs Querying},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3615952.3615956},
doi = {10.1145/3615952.3615956},
abstract = {Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL were constructed to store large-scale, real-world facts as (subject, predicate, object) triples - that can also be modeled as a graph, where a node (a subject or an object) represents an entity with attributes, and a directed edge (a predicate) is a relationship between two entities. Querying KGs is critical in web search, question answering (QA), semantic search, personal assistants, fact checking, and recommendation. While significant progress has been made on KG construction and curation, thanks to deep learning recently we have seen a surge of research on KG querying and QA. The objectives of our survey are two-fold. First, research on KG querying has been conducted by several communities, such as databases, data mining, semantic web, machine learning, information retrieval, and natural language processing (NLP), with different focus and terminologies; and also in diverse topics ranging from graph databases, query languages, join algorithms, graph patterns matching, to more sophisticated KG embedding and natural language questions (NLQs). We aim at uniting different interdisciplinary topics and concepts that have been developed for KG querying. Second, many recent advances on KG and query embedding, multimodal KG, and KG-QA come from deep learning, IR, NLP, and computer vision domains. We identify important challenges of KG querying that received less attention by graph databases, and by the DB community in general, e.g., incomplete KG, semantic matching, multimodal data, and NLQs. We conclude by discussing interesting opportunities for the data management community, for instance, KG as a unified data model and vector-based query processing.},
journal = {SIGMOD Rec.},
month = {aug},
pages = {18–29},
numpages = {12}
}

@inproceedings{10.1145/3626772.3657875,
author = {Li, Xinhang and Zhao, Xiangyu and Wang, Yejing and Liu, Yu and Chen, Chong and Long, Cheng and Zhang, Yong and Xing, Chunxiao},
title = {OpenSiteRec: An Open Dataset for Site Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657875},
doi = {10.1145/3626772.3657875},
abstract = {As a representative information retrieval task, site recommendation, which aims at predicting the optimal sites for a brand or an institution to open new branches in an automatic data-driven way, is beneficial and crucial for brand development in modern business. However, there is no publicly available dataset so far and most existing approaches are limited to an extremely small scope of brands, which seriously hinders the research on site recommendation. Therefore, we collect, construct and release an open comprehensive dataset, namely OpenSiteRec, to facilitate and promote the research on site recommendation. Specifically, OpenSiteRec leverages a heterogeneous graph schema to represent various types of real-world entities and relations in four international metropolises. To evaluate the performance of the existing general methods on the site recommendation task, we conduct benchmarking experiments of several representative recommendation models on OpenSiteRec. Furthermore, we also highlight the potential application directions to demonstrate the wide applicability of OpenSiteRec. We believe that our OpenSiteRec dataset is significant and anticipated to encourage the development of advanced methods for site recommendation. OpenSiteRec is available online at https://OpenSiteRec.github.io/.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1483–1493},
numpages = {11},
keywords = {benchmark, dataset, heterogeneous graph, site recommendation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3339252.3339282,
author = {Mahaini, Mohamad Imad and Li, Shujun and Sa\u{g}lam, Rahime Belen},
title = {Building Taxonomies based on Human-Machine Teaming: Cyber Security as an Example},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3339282},
doi = {10.1145/3339252.3339282},
abstract = {Taxonomies and ontologies are handy tools in many application domains such as knowledge systematization and automatic reasoning. In the cyber security field, many researchers have proposed such taxonomies and ontologies, most of which were built based on manual work. Some researchers proposed the use of computing tools to automate the building process, but mainly on very narrow sub-areas of cyber security. Thus, there is a lack of general cyber security taxonomies and ontologies, possibly due to the difficulties of manually curating keywords and concepts for such a diverse, inter-disciplinary and dynamically evolving field.This paper presents a new human-machine teaming based process to build taxonomies, which allows human experts to work with automated natural language processing (NLP) and information retrieval (IR) tools to co-develop a taxonomy from a set of relevant textual documents. The proposed process could be generalized to support non-textual documents and to build (more complicated) ontologies as well. Using the cyber security as an example, we demonstrate how the proposed taxonomy building process has allowed us to build a general cyber security taxonomy covering a wide range of data-driven keywords (topics) with a reasonable amount of human effort.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {30},
numpages = {9},
keywords = {Twitter, cyber security, information retrieval (IR), knowledge representation, natural language processing (NLP), online social network (OSN), ontology, taxonomy, visualization},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3366423.3380179,
author = {Shezan, Faysal Hossain and Hu, Hang and Wang, Jiamin and Wang, Gang and Tian, Yuan},
title = {Read Between the Lines: An Empirical Measurement of Sensitive Applications of Voice Personal Assistant Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380179},
doi = {10.1145/3366423.3380179},
abstract = {Voice Personal Assistant (VPA) systems such as Amazon Alexa and Google Home have been used by tens of millions of households. Recent work demonstrated proof-of-concept attacks against their voice interface to invoke unintended applications or operations. However, there is still a lack of empirical understanding of what type of third-party applications that VPA systems support, and what consequences these attacks may cause. In this paper, we perform an empirical analysis of the third-party applications of Amazon Alexa and Google Home to systematically assess the attack surfaces. A key methodology is to characterize a given application by classifying the sensitive voice commands it accepts. We develop a natural language processing tool that classifies a given voice command from two dimensions: (1) whether the voice command is designed to insert action or retrieve information; (2) whether the command is sensitive or nonsensitive. The tool combines a deep neural network and a keyword-based model, and uses Active Learning to reduce the manual labeling effort. The sensitivity classification is based on a user study (N=404) where we measure the perceived sensitivity of voice commands. A ground-truth evaluation shows that our tool achieves over 95% of accuracy for both types of classifications. We apply this tool to analyze 77,957 Amazon Alexa applications and 4,813 Google Home applications (198,199 voice commands from Amazon Alexa, 13,644 voice commands from Google Home) over two years (2018-2019). In total, we identify 19,263 sensitive “action injection” commands and 5,352 sensitive “information retrieval” commands. These commands are from 4,596 applications (5.55% out of all applications), most of which belong to the “smart home” category. While the percentage of sensitive applications is small, we show the percentage is increasing over time from 2018 to 2019.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1006–1017},
numpages = {12},
keywords = {Active-learning., Alexa, Google-Home, Malicious-command, Sensitive-commands, Sensitive-keyword, Skill, Voice-applications},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3578938,
author = {Menghani, Gaurav},
title = {Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3578938},
doi = {10.1145/3578938},
abstract = {Deep learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval, and more. However, with the progressive improvements in deep learning models, their number of parameters, latency, and resources required to train, among others, have all increased significantly. Consequently, it has become important to pay attention to these footprint metrics of a model as well, not just its quality. We present and motivate the problem of efficiency in deep learning, followed by a thorough survey of the five core areas of model efficiency (spanning modeling techniques, infrastructure, and hardware) and the seminal work there. We also present an experiment-based guide along with code for practitioners to optimize their model training and deployment. We believe this is the first comprehensive survey in the efficient deep learning space that covers the landscape of model efficiency from modeling techniques to hardware support. It is our hope that this survey would provide readers with the mental model and the necessary understanding of the field to apply generic efficiency techniques to immediately get significant improvements, and also equip them with ideas for further research and experimentation to achieve additional gains.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {259},
numpages = {37},
keywords = {Efficient deep learning, efficient machine learning, efficient artificial intelligence, quantization, pruning, sparsity, distillation, model compression, model optimization}
}

@article{10.1145/1538788.1538816,
author = {Fabian, Benjamin and G\"{u}nther, Oliver},
title = {Security challenges of the EPCglobal network},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/1538788.1538816},
doi = {10.1145/1538788.1538816},
abstract = {Introduction The "Internet of Things," once reality, will have to rely on a global IT infrastructure that provides information about all those "things" in a secure and reliable manner. The EPCglobal Network is a proposal for a widely distributed information system to offer such services. But it may introduce more challenges concerning security, privacy, and political control than was initially anticipated.If the vision of many RFID proponents becomes true, more and more common objects will soon acquire a cyber presence. Objects will be equipped with RFID tags containing identification data and possibly some additional information about the object in question (data on tag). To keep tag costs low, one may often just store an identifier and use it as a key to access databases containing the actual object information (data on network). This second approach is typical for "EPC tags"—RFID tags that aim to replace the conventional barcode system. They use an Electronic Product Code (EPC, see Figure 1), which is globally unique, as a key to retrieve information from the EPCglobal Network, envisioned as a large distributed system of databases. The EPC standard represents a numbering framework that is independent of specific hardware features, such as tag generation or specific radio frequency.The databases compromising the EPCglobal Network are to be run by manufacturers, logistic providers, retailers, or third parties, and can be accessed via special web services called EPC Information Services (EPCIS). The network architecture is designed and administered by the standardization consortium EPCglobal, which is a joint venture of GS1 U.S. (formerly Uniform Code Council) and GS1 (formerly EAN International).By improving the information flow, as objects pass from suppliers to manufacturers, distributors, retail stores, and customers, the EPCglobal Network aims to facilitate cooperation within supply chains and thus to make them more efficient. Once established, it could also be used to support a wide range of applications in the area of ubiquitous computing. An often-cited example is the "smart home," in which "intelligent" cupboards and fridges could be realized using RFID technology. By scanning the RFID tags on objects and using the EPCglobal Network for information retrieval, such devices can identify their current content and offer new services like food counseling or automated replenishing of goods.As a result of this broadened use of the EPCglobal Network, its security context would change from closed supply chains to the rather open environments of ubiquitous computing–just like the security context of the Internet was changed by moving from relatively closed groups of fellow researchers to the global environment it represents today.In this article, we first describe the EPCglobal Network architecture, as currently specified. We then discuss its security and privacy risks, as well as possible countermeasures. We conclude with suggestions on how to improve existing design proposals, once appropriate security and privacy requirements have been established.},
journal = {Commun. ACM},
month = {jul},
pages = {121–125},
numpages = {5}
}

@article{10.1145/3575658,
author = {Zhang, Jiwei and Yu, Yi and Tang, Suhua and Wu, Jianming and Li, Wei},
title = {Variational Autoencoder with CCA for Audio–Visual Cross-modal Retrieval},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3575658},
doi = {10.1145/3575658},
abstract = {Cross-modal retrieval is to utilize one modality as a query to retrieve data from another modality, which has become a popular topic in information retrieval, machine learning, and databases. Finding a method to effectively measure the similarity between different modality data is the major challenge of cross-modal retrieval. Although several research works have calculated the correlation between different modality data via learning a common subspace representation, the encoder’s ability to extract features from multi-modal information is not satisfactory. In this article, we present a novel variational autoencoder architecture for audio–visual cross-modal retrieval by learning paired audio–visual correlation embedding and category correlation embedding as constraints to reinforce the mutuality of audio–visual information. On the one hand, audio encoder and visual encoder separately encode audio data and visual data into two different latent spaces. Further, two mutual latent spaces are respectively constructed by canonical correlation analysis. On the other hand, probabilistic modeling methods are used to deal with possible noise and missing information in the data. Additionally, in this way, the cross-modal discrepancies from intra-modal and inter-modal information are simultaneously eliminated in the joint embedding subspace. We conduct extensive experiments over two benchmark datasets. The experimental results confirm that the proposed architecture is effective in learning audio–visual correlation and is appreciably better than the existing cross-modal retrieval methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {130},
numpages = {21},
keywords = {Cross-modal retrieval, audio–visual correlation learning}
}

@article{10.1145/65943.65945,
author = {Raghavan, Vijay and Bollmann, Peter and Jung, Gwang S.},
title = {A critical investigation of recall and precision as measures of retrieval system performance},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/65943.65945},
doi = {10.1145/65943.65945},
abstract = {Recall and precision are often used to evaluate the effectiveness of information retrieval systems. They are easy to define if there is a single query and if the retrieval result generated for the query is a linear ordering. However, when the retrieval results are weakly ordered, in the sense that several documents have an identical retrieval status value with respect to a query, some probabilistic notion of precision has to be introduced. Relevance probability, expected precision, and so forth, are some alternatives mentioned in the literature for this purpose. Furthermore, when many queries are to be evaluated and the retrieval results averaged over these queries, some method of interpolation of precision values at certain preselected recall levels is needed. The currently popular approaches for handling both a weak ordering and interpolation are found to be inconsistent, and the results obtained are not easy to interpret. Moreover, in cases where some alternatives are available, no comparative analysis that would facilitate the selection of a particular strategy has been provided. In this paper, we systematically investigate the various problems and issues associated with the use of recall and precision as measures of retrieval system performance. Our motivation is to provide a comparative analysis of methods available for defining precision in a probabilistic sense and to promote a better understanding of the various issues involved in retrieval performance evaluation.},
journal = {ACM Trans. Inf. Syst.},
month = {jul},
pages = {205–229},
numpages = {25}
}

@inproceedings{10.1145/3397271.3401173,
author = {Zheng, Jianming and Cai, Fei and Chen, Honghui},
title = {Incorporating Scenario Knowledge into A Unified Fine-tuning Architecture for Event Representation},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401173},
doi = {10.1145/3397271.3401173},
abstract = {Given an occurred event, human can easily predict the next event or reason the preceding event, yet which is difficult for machine to perform such event reasoning. Event representation bridges the connection and targets to model the process of event reasoning as a machine-readable format, which then can support a wide range of applications in information retrieval, e.g., question answering and information extraction. Existing work mainly resorts to a joint training to integrate all levels of training loss in event chains by a simple loss summation, which is easily trapped into a local optimum. In addition, the scenario knowledge in event chains is not well investigated for event representation. In this paper, we propose a unified fine-tuning architecture, incorporated with scenario knowledge for event representation, i.e., UniFA-S, which mainly consists of a unified fine-tuning architecture (UniFA) and a scenario-level variational auto-encoder (S-VAE). In detail, UniFA employs a multi-step fine-tuning to integrate all levels of training and S-VAE applies a stochastic variable to implicitly represent the scenario-level knowledge. We evaluate our proposal from two aspects, i.e., the representation and inference abilities. For the representation ability, our ensemble model UniFA-S can beat state-of-the-art baselines for two similarity tasks. For the inference ability, UniFA-S can outperform the best baseline, achieving 4.1%-8.2% improvements in terms of accuracy for various inference tasks.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {249–258},
numpages = {10},
keywords = {event representation, fine-tuning, pre-training, scenario knowledge},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3563456,
author = {Shu, Jiwu and Fang, Kedong and Chen, Youmin and Wang, Shuo},
title = {TH-iSSD: Design and Implementation of a Generic and Reconfigurable Near-Data Processing Framework},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3563456},
doi = {10.1145/3563456},
abstract = {We present the design and implementation of TH-iSSD, a near-data processing framework to address the data movement problem. TH-iSSD does not pose any restriction to the hardware selection and is highly reconfigurable—its core components, such as the on-device compute unit (e.g., FPGA, embedded CPUs) and data collectors (e.g., camera, sensors), can be easily replaced to adapt to different use cases. TH-iSSD achieves this goal by incorporating highly flexible computation and data paths. In the data path, TH-iSSD adopts an efficient device-level data switch that exchanges data with both host CPUs and peripheral sensors; it also enables direct accesses between the sensing, computation, and storage hardware components, which completely eliminates the redundant data movement overhead, and thus delivers both high performance and energy efficiency. In the computation path, TH-iSSD provides an abstraction of filestream for developers, which abstracts a collection of data along with the related computation task as a file. Since existing applications are familiar with POSIX-like interfaces, they can be ported on top of our platform with minimal code modification. Moreover, TH-iSSD also introduces mechanisms including pipelined near-data processing and priority-aware I/O scheduling to make TH-iSSD perform more effectively. We deploy TH-iSSD to accelerate two types of applications: the content-based information retrieval system and the edge zero-streaming system. Our experimental results show that TH-iSSD achieves up to 1.6\texttimes{} higher throughput and 36% lower latency than compute-centric designs.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {nov},
articleno = {96},
numpages = {23},
keywords = {Near data processing, information retrieval, deep learning, storage architecture}
}

@inbook{10.1145/3122865.3122871,
author = {Jeundefinedou, Herv\'{e}},
title = {Efficient similarity search},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122871},
abstract = {This chapter addresses one of the fundamental problems involved in multimedia systems, namely efficient similarity search for large collections of multimedia content. This problem has received a lot of attention from various research communities. In particular, it is a historical line of research in computational geometry and databases. The computer vision and multimedia communities have adopted pragmatic approaches guided by practical requirements: the large sets of features required to describe image collections make visual search a highly demanding task. As a result, early works [Flickner et al. 1995, Fagin 1998, Beis and Lowe 1997] in image indexing have foreseen the interest in approximate algorithms, especially after the dissemination of methods based on local description in the 90s, as any improvement obtained on this indexing part improves the whole visual search system.Among the existing approximate nearest neighbors (ANN) strategies, the popular framework of Locality-Sensitive Hashing (LSH) [Indyk and Motwani 1998, Gionis et al. 1999] provides theoretical guarantees on the search quality with limited assumptions on the underlying data distribution. It was first proposed [Indyk and Motwani 1998] for the Hamming and l1 spaces, and was later extended to the Euclidean/ cosine cases [Charikar 2002, Datar et al. 2004] or the earth mover's distance [Charikar 2002, Andoni and Indyk 2006]. LSH has been successfully used for local descriptors [Ke et al. 2004], 3D object indexing [Matei et al. 2006, Shakhnarovich et al. 2006], and other fields such as audio retrieval [Casey and Slaney 2007, Ryynanen and Klapuri 2008]. It has also received some attention in a context of private information retrieval [Pathak and Raj 2012, Aghasaryan et al. 2013, Furon et al. 2013].A few years ago, approaches inspired by compression and more specifically quantization-based approaches [Jundefinedou et al. 2011] were shown to be a viable alternative to hashing methods, and shown successful for efficiently searching in a billion-sized dataset.This chapter discusses these different trends. It is organized as follows. Section 5.1 gives some background references and concepts, including evaluation issues. Most of the methods and variants are exposed within the LSH framework. It is worth mentioning that LSH is more of a concept than a particular algorithm. The search algorithms associated with LSH follow two distinct search mechanisms, the probe-cell model and sketches, which are discussed in Sections 5.2 and 5.3, respectively. Section 5.4 describes methods inspired by compression algorithms, while Section 5.5 discusses hybrid approaches combining the non-exhaustiveness of the cell-probe model with the advantages of sketches or compression-based algorithms. Other metrics than Euclidean and cosine are briefly discussed in Section 5.6.},
booktitle = {Frontiers of Multimedia Research},
pages = {105–134},
numpages = {30}
}

@inproceedings{10.1145/3126858.3126880,
author = {Dilli, Renato and Filho, Huberto Kaiser and Pernas, Ana Marilza and Yamin, Adenauer},
title = {EXEHDA-RR: Machine Learning and MCDA with Semantic Web in IoT Resources Classification},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126880},
doi = {10.1145/3126858.3126880},
abstract = {Currently, a lot of resources are connected to the Internet, many simultaneously requesting and providing services. The adequate selection of resources that best meet the demands of users with a broad range of options has been a relevant and current research challenge. Based on the non-functional parameters of QoS play a significant role in the ranking of these resources according to the services they offer. This paper aims to aggregate machine learning in the pre-classification of EXEHDA middleware resources, to reduce the computational cost generated by MCDA algorithms. We presented the proposed software architecture (EXEHDA-RR), and the obtained results with the integration of machine learning in the classification process are promissing, and indicate to the research continuation.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {293–300},
numpages = {8},
keywords = {internet of things, machine learning, mcda, resource ranking},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1145/3184558.3186981,
author = {Ruta, Michele and Scioscia, Floriano and Loseto, Giuseppe and Gramegna, Filippo and Ieva, Saverio and Pinto, Agnese and Di Sciascio, Eugenio},
title = {A journey from the Physical Web to the Physical Semantic Web},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186981},
doi = {10.1145/3184558.3186981},
abstract = {ThePhysical Semantic Web (PSW) is a novel paradigm built upon the Google Physical Web (PW) approach and devoted to improve the quality of interactions in the Web of Things. Beacons expose semantic annotations instead of basic identifiers, \i{}e machine-understandable descriptions of physical resources. This enables novel ontology-based object advertisement and discovery and --in turn-- advanced user-to-thing and autonomous thing-to-thing interactions. The demo shows the evolution from the PW to the PSW in a discovery scenario set in a winery, where bottles are equipped with Bluetooth Low Energy beacons and a customer can discover them using her smartphone. The final goal is to prove benefits of PSW over basic PW, including: rich semantic-based object annotation; dynamic annotations exploiting on-board sensors; enhanced discovery and ranking of nearby objects through semantic matchmaking; availability of interactions even without working Internet infrastructure, by means of point-to-point data exchanges.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {211–214},
numpages = {4},
keywords = {non-standard reasoning, physical web, resource discovery, semantic web of things},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/1807167.1807281,
author = {Liu, Zhen Hua and Baby, Thomas and Chakraborty, Sukhendu and Ding, Junyan and Novoselsky, Anguel and Arora, Vikas},
title = {Pay-as-you-go: an adaptive approach to provide full context-aware text search over document content},
year = {2010},
isbn = {9781450300322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807167.1807281},
doi = {10.1145/1807167.1807281},
abstract = {RDBMS provides best performance for querying structured data that starts out with a well-defined schema. However, such a 'schema first, data later' approach does not work for unstructured data or data without much structure. Therefore, RDBMS typically stores such data without any schema in LOB columns (for example, Character Large Object (CLOB) or Binary Large Object (BLOB) columns) and provides Information-Retrieval (IR) style, keyword-based search capability over these LOB columns. Lately, XML as a native datatype (XMLType) in RDBMS has been introduced via the SQL/XML standard. Semi-structured data with or without any schema can be stored into such XMLType columns, and XQuery provides query capability over them. In particular, XQuery full text specification provides the capability of searching keywords within document context. Such full context-aware text search capability is more powerful than pure keyword search, since the user can now provide fine-grained context in which the keywords should occur. However, XML with XQuery full text searching requires that the user first convert her text data into XML and store them into XMLType column. Such massive physical data migration with possible loss of document fidelity and its potential impact on existing production environments are often expensive enough that users are reluctant to adopt the XML/XQuery approach.In this paper, we propose a pay-as-you-go architecture to provide XML text view over LOB columns, so that user can take advantage of context-aware full-text search capability adaptively. This adaptive architecture includes a novel XML text index that can be created over the LOB column where the content is stored. The XML text index supports an XML text view over LOB data on top of which XQuery full-text search capability is feasible. Such an adaptive index/view approach provides least intrusion over existing data, as it requires no physical data migration. We describe the design and challenge of building such an adaptive XML text index. Furthermore, we advocate that the pay-as-you-go approach provides the integration bridge between the structured relational world and text oriented document world and fulfills the primary motivation of XML in the database.},
booktitle = {Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data},
pages = {1025–1036},
numpages = {12},
keywords = {information retrieval, keyword search, text index, tree index, xml, xml index, xquery, xquery full text},
location = {Indianapolis, Indiana, USA},
series = {SIGMOD '10}
}

@inproceedings{10.1145/2661829.2661890,
author = {Qin, Yongrui and Sheng, Quan Z. and Falkner, Nickolas J.G. and Zhang, Wei Emma and Wang, Hua},
title = {Indexing Linked Data in a Wireless Broadcast System with 3D Hilbert Space-Filling Curves},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661890},
doi = {10.1145/2661829.2661890},
abstract = {Semantic technologies aim to facilitate machine-to-machine communication and are attracting more and more interest from both academia and industry, especially in the emerging Internet of Things (IoT). In this paper, we consider large-scale information sharing scenarios among mobile objects in IoT by leveraging semantic techniques. We propose to broadcast Linked Data on-air using RDF format to allow simultaneous access to the information and to achieve better scalability. We introduce a novel air indexing method to reduce the information access latency and energy consumption. To build air indexes, we firstly map RDF triples in the Linked Data into points in a 3D space and build B+-trees based on 3D Hilbert curve mappings for all of the 3D points. We then convert these trees into linear sequences so that they can be broadcast over a wireless channel. A novel search algorithm is also designed to efficiently evaluate queries against the air indexes. Experiments show that our indexing method outperforms the air indexing method based on traditional 3D R-trees.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {1775–1778},
numpages = {4},
keywords = {air indexing, linked data, wireless broadcast},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/3512576.3512644,
author = {Xia, Wei and Zhang, Qiyu and He, Xin and Wang, Wei and Li, Zhen and Xiong, Gang},
title = {After Everything is Connected: A Client Certificate-Oriented Perspective of IoT Device Security Analysis},
year = {2022},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512644},
doi = {10.1145/3512576.3512644},
abstract = {In the IoT era, more and more devices of different types and functions are connected to the network. However, smart devices are bringing about increasingly serious security problems. Although some giants or well-known equipment manufacturers have introduced the transport layer security protocol as a secure transmission mechanism, they are still missing tens of thousands in specific practices. In this paper, we provide a client certificates-oriented perspective on the security analysis of IoT devices, which proves that although the TLS protocol is used, it is still not enough to ensure security. We utilized our self-developed passive traffic-based client certificate collection tool to conduct extensive TLS certificate collection on the ISP-level network CSTNET. We use the keywords already collected to filter out certificates related to IoT smart devices from these certificates, and analyze the security issues that exist. We designed an active crawling subsystem, put the keywords that identify the manufacturer in the certificate into the Internet to crawl its homepage, and use the characteristics of page elements to dig out unknown IoT smart devices, and conduct research on the issue of its certificate. It turns out that more needs to be done to meet the advanced security requirements in practice and deployment.},
booktitle = {Proceedings of the 2021 9th International Conference on Information Technology: IoT and Smart City},
pages = {321–325},
numpages = {5},
keywords = {Active discovery, Client certificate, Security issues, Validity period},
location = {Guangzhou, China},
series = {ICIT '21}
}

@inproceedings{10.1145/3241539.3267744,
author = {Narang, Nishit and Kar, Subrat},
title = {Utilizing Social Networks Data for Trust Management in a Social Internet of Things Network},
year = {2018},
isbn = {9781450359030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241539.3267744},
doi = {10.1145/3241539.3267744},
abstract = {Social Internet of Things (SIoT), an amalgamation of Social Networking concepts to the Internet of Things (IoT), is a strong architectural alternative for IoT solutions. A lot of research work in SIoT has proposed the use of social networking data for community and trust management in SIoT networks. While it seems like an interesting choice, it is important to analyze the effectiveness of social networking data for application to SIoT. In this paper, we analyze the accuracy of using tie information from the Facebook Friend Graph to mimic real-world SIoT network ties. We also discuss a method for ranking the strength of ties in a SIoT network by analyzing the structure of the Facebook Friend Graph. A similar analysis can be performed on data available from other Social Networking platforms, like Twitter, LinkedIn etc.},
booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
pages = {768–770},
numpages = {3},
keywords = {internet of things (IoT), social iot (SIoT), social networks, trust management},
location = {New Delhi, India},
series = {MobiCom '18}
}

@inproceedings{10.1145/3351108.3351118,
author = {Sithole, Vusi and Marshall, Linda},
title = {Attributes Extraction for Fine-grained Differentiation of the Internet of Things Patterns},
year = {2019},
isbn = {9781450372657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351108.3351118},
doi = {10.1145/3351108.3351118},
abstract = {The Internet of Things (IoT) is a paradigm with multitudes of design patterns. However, in order to use these patterns quickly and effectively, one must be able to make a differentiation between the existing patterns. At the moment, there is no known catalogue for the IoT patterns in which each pattern is described at a fine-grained level, i.e. in terms of its attributes. The need to discuss these patterns in terms of their attributes is important as it enables ease of understanding and allows us to group related patterns together for speedy retrieval. In this paper, we present an attributes extraction system which generates a list of attributes for a given IoT pattern. The attributes extraction system is based on identification and extraction of important sentences which describe the core properties of the given IoT pattern. The system uses multiple linguistics features to identify the most important sentences in a document with regard to describing the core essence of a given pattern. The system calculates an independent score for each sentence per feature. Through aggregation, the independent scores for each feature can then be combined to give a weighted mean score for each sentence. The evaluation results show that the attributes selected by the system are consistent with human ranking in the bulk of the examined documents.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists 2019},
articleno = {9},
numpages = {10},
keywords = {Attributes Extraction, Internet of Things, Patterns},
location = {Skukuza, South Africa},
series = {SAICSIT '19}
}

@inproceedings{10.1145/3549206.3549289,
author = {Deshmukh, Atharva and Patil, Disha and Tyagi, Amit Kumar and S S, Arumugam and Arumugam},
title = {Recent Trends on Blockchain for Internet of Things based Applications: Open Issues and Future Trends},
year = {2022},
isbn = {9781450396752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549206.3549289},
doi = {10.1145/3549206.3549289},
abstract = {Today Blockchain, a decentralized, and distributed ledger technology have started taking over almost all the applications from banking, agriculture, finance, transportation, etc. Such enhancements are possible when this technology is enabled with some other technology like the Internet of Things, cloud computing, machine learning, etc. This paper does a thorough evaluation of the literature concerning blockchain enabling IoT-based applications in diverse sectors. The goal is to look at the current state of blockchain based IoT systems and applications and to demonstrate how special qualities of this technology can transform the working methods in the industry. We analysed high-quality papers published in high-ranking scientific journals over the last years to write this paper. This work presents a comprehensive classification of IoT and Blockchain enabled applications across many sectors such as Agriculture, Business, Supply Chain, Data Management, Finance, Healthcare, IoT, and&nbsp;Privacy, and in the end, we have added recent trends, emerging areas, and open challenges, for future research across various sectors and industries, based on a structured, systematic review and thematic content analysis of the discovered literature. We feel that our work will be of tremendous use to both academics and practitioners because we have discovered such critical components for this technology.},
booktitle = {Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing},
pages = {484–492},
numpages = {9},
keywords = {Blockchain Technology, Internet of Things, Recent Trends, Research Analysis, Research statements},
location = {Noida, India},
series = {IC3-2022}
}

@inproceedings{10.5555/1602165.1602186,
author = {Elahi, B. Maryam and Romer, Kay and Ostermaier, Benedikt and Fahrmair, Michael and Kellerer, Wolfgang},
title = {Sensor ranking: A primitive for efficient content-based sensor search},
year = {2009},
isbn = {9781424451081},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The increasing penetration of the real world with embedded and globally networked sensors enables the formation of a Web of Things (WoT), where high-level state information derived from sensors is embedded into Web representations of real-world entities (e.g. places, objects). A key service for the WoT is searching for entities which exhibit a certain dynamic state at the time of the query, which is a challenging problem due to the dynamic nature of the sought state information and due to the potentially huge scale of the WoT. In this paper we introduce a primitive called sensor ranking to enable efficient search for sensors that have a certain output state at the time of the query. The key idea is to efficiently compute for each sensor an estimate of the probability that it matches the query and process sensors in the order of decreasing probability, such that effort is first spent on sensors that are very likely to actually match the query. Using real data sets, we show that sensor ranking can significantly improve the efficiency of content-based sensor search.},
booktitle = {Proceedings of the 2009 International Conference on Information Processing in Sensor Networks},
pages = {217–228},
numpages = {12},
series = {IPSN '09}
}

@article{10.1109/TNET.2023.3312162,
author = {Qu, Jian and Ma, Xiaobo and Liu, Wenmao and Sang, Hongqing and Li, Jianfeng and Xue, Lei and Luo, Xiapu and Li, Zhenhua and Feng, Li and Guan, Xiaohong},
title = {On Smartly Scanning of the Internet of Things},
year = {2023},
issue_date = {April 2024},
publisher = {IEEE Press},
volume = {32},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3312162},
doi = {10.1109/TNET.2023.3312162},
abstract = {Cyber search engines, such as Shodan and Censys, have gained popularity due to their strong capability of indexing the Internet of Things (IoT). They actively scan and fingerprint IoT devices for unearthing IP-device mapping. Because of the large address space of the Internet and the mapping’s mutative nature, efficiently tracking the evolution of IP-device mapping with a limited budget of scans is essential for building timely cyber search engines. An intuitive solution is to use reinforcement learning to schedule more scans to networks with high churn rates of IP-device mapping. However, such an intuitive solution has never been systematically studied. In this paper, we take the first step toward demystifying this problem based on our experiences in maintaining a global IoT scanning platform. Inspired by the measurement study of large-scale real-world IoT scan records, we land reinforcement learning onto a system capable of smartly scanning IoT devices in a principled way. We disclose key parameters affecting the effectiveness of different scanning strategies, and real-world experiments demonstrate that our system can scan up to around 40 times as many IP-device mapping mutations as random/sequential scanning.},
journal = {IEEE/ACM Trans. Netw.},
month = {sep},
pages = {1019–1034},
numpages = {16}
}

@inproceedings{10.1145/3127479.3132254,
author = {Iyer, Anand Padmanabha and Stoica, Ion},
title = {A scalable distributed spatial index for the internet-of-things},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3132254},
doi = {10.1145/3127479.3132254},
abstract = {The increasing interest in the Internet-of-Things (IoT) suggests that a new source of big data is imminent---the machines and sensors in the IoT ecosystem. The fundamental characteristic of the data produced by these sources is that they are inherently geospatial in nature. In addition, they exhibit unprecedented and unpredictable skews. Thus, big data systems designed for IoT applications must be able to efficiently ingest, index and query spatial data having heavy and unpredictable skews. Spatial indexing is well explored area of research in literature, but little attention has been given to the topic of efficient distributed spatial indexing.In this paper, we propose Sift, a distributed spatial index and its implementation. Unlike systems that depend on load balancing mechanisms that kick-in post ingestion, Sift tries to distribute the incoming data along the distributed structure at indexing time and thus incurs minimal rebalancing overhead. Sift depends only on an underlying key-value store, hence is implementable in many existing big data stores. Our evaluations of Sift on a popular open source data store show promising results---Sift achieves up to 8\texttimes{} reduction in indexing overhead while simultaneously reducing the query latency and index size by over 2\texttimes{} and 3\texttimes{} respectively, in a distributed environment compared to the state-of-the-art.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {548–560},
numpages = {13},
keywords = {big data, distributed data store, spatial indexing},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/3229710.3229744,
author = {Hern\'{a}ndez, Daniel and Arcas-T\'{u}nez, Francisco and Mu\~{n}oz, Andr\'{e}s and Cecilia, Jos\'{e} M.},
title = {BAUSPACE: A Scalable Infrastructure for Soft Sensors Development},
year = {2018},
isbn = {9781450365239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229710.3229744},
doi = {10.1145/3229710.3229744},
abstract = {The Internet of Things (IoT) is driving the next economic revolution where the main actors are both the data volume and the immediacy. However, the IoT world is increasingly generating vast amounts of data classified as a "dark data", since most of them are generated but never analysed. Therefore, efficient big data analysis in IoT infrastructure is becoming mandatory to transform this data deluge into meaningful information. Even after enabling this analysis, the quantitative information provided by traditional "hard" sensors is not enough to deal with some scenarios where human observations are required. These observations could be targeted through "soft sensors", where people's opinion in social networks, posts, news or comments may be analyzed to create dynamic observation resources. Combining both sources-of-information (devices and humans) automatically would provide a very powerful tool that could represent a step forward in the data understanding science. However, the development of soft sensors implies the use of many services for crawling text sources and mashup Web-based content, storage it, understanding the language or inferring information, just to mention a few. Therefore, a novel cloud-based distributed system is mandatory to be able to develop such frameworks. In this paper we introduce a work-in-progress for a distributed and modular framework to develop soft sensors in a scalable manner and transparently to the cloud provider.},
booktitle = {Workshop Proceedings of the 47th International Conference on Parallel Processing},
articleno = {22},
numpages = {4},
keywords = {Distributed Computing, Docker, Kafka, Soft Sensors, Storm, Zookeeper},
location = {Eugene, OR, USA},
series = {ICPP Workshops '18}
}

@inproceedings{10.1145/3341105.3373930,
author = {Sachidananda, Vinay and Bhairav, Suhas and Elovici, Yuval},
title = {OVER: overhauling vulnerability detection for iot through an adaptable and automated static analysis framework},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373930},
doi = {10.1145/3341105.3373930},
abstract = {Internet of Things (IoT) exposes various vulnerabilities at the software level. In this paper, we propose a static analysis framework for IoT. The proposed framework is designed for detecting security vulnerabilities such as Buffer Overflow, Memory Leaks, Code Injection, TOCTOU, Banned functions, and other code-related vulnerabilities. We consider end-to-end IoT software suite that includes kernels, protocol stacks, APKs, firmware, and others. In particular, we unpacked and analyzed over 21,000 IoT firmware, 628 IoT APKs and 50 IoT Open Source Software (OSS).Our framework is an adaptable and automated static analysis technique that begins with crawling the web for fetching the IoT related files and ends with report generation consisting of IoT Risk Rating. In total, we were able to raise 7 new CVEs and detected 342 existing CVEs and 894 vulnerable code clones in IoT OSS. We found over 70% of APKs vulnerable to SQL Injection and 56% APKs using weak cryptographic algorithms. Also, our framework found 3783 hard-coded passwords and archaic BusyBox versions in IoT firmware.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {729–738},
numpages = {10},
keywords = {internet of things (IoT), security analysis, security and privacy, static analysis, vulnerabilities, vulnerability detection},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/2079353.2079359,
author = {Kalegele, Khamisi and Sveholm, Johan and Takahashi, Hideyuki and Sasai, Kazuto and Kitagata, Gen and Kinoshita, Tetsuo},
title = {On-demand numerosity reduction for object learning},
year = {2011},
isbn = {9781450310437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2079353.2079359},
doi = {10.1145/2079353.2079359},
abstract = {In Internet of Things, softwares shall enable their host objects (everyday-objects) to monitor other objects, take actions, and notify humans while using some form of reasoning. The ever changing nature of real life environment necessitates the need for these objects to be able to generalize various inputs inductively in order to play their roles more effectively. These objects shall learn from stored training examples using some generalization algorithm. In this paper, we investigate training sets requirements for object learning and propose a Stratified Ordered Selection (SOS) method as a means to scale down training sets. SOS uses a new instance ranking scheme called LO ranking. Everyday-objects use SOS to select training subsets based on their capacity (e.g. memory, CPU). LO ranking has been designed to broaden class representation, achieve significant reduction while offering same or near same analytical results and to facilitate faster on-demand subset selection and retrieval for resource constrained objects. We show how SOS outperforms other methods using well known machine learning datasets.},
booktitle = {Proceedings of the Workshop on Internet of Things and Service Platforms},
articleno = {6},
numpages = {8},
keywords = {data reduction, learning, ranking, sampling},
location = {Tokyo, Japan},
series = {IoTSP '11}
}

@inproceedings{10.1145/3267357.3267362,
author = {Arias-Cabarcos, Patricia and Almen\'{a}rez, Florina and D\'{\i}az-S\'{a}nchez, Daniel and Mar\'{\i}n, Andr\'{e}s},
title = {FRiCS: A Framework for Risk-driven Cloud Selection},
year = {2018},
isbn = {9781450359887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267357.3267362},
doi = {10.1145/3267357.3267362},
abstract = {Our devices and interactions in a world where physical and digital realities are more and more blended, generate a continuum of multimedia data that needs to be stored, shared and processed to provide services that enrich our daily lives. Cloud computing plays a key role in these tasks, dissolving resource allocation and computational boundaries, but it also requires advanced security mechanisms to protect the data and provide privacy guarantees. Therefore, security assurance must be evaluated before offloading tasks to a cloud provider, a process which is currently manual, complex and inadequate for dynamic scenarios. However, though there are many tools for evaluating cloud providers according to quality of service criteria, automated categorization and selection based on risk metrics is still challenging. To address this gap, we present FRiCS, a Framework for Risk-driven Cloud Selection, which contributes with: 1) a set of cloud security metrics and risk-based weighting policies, 2) distributed components for metric extraction and aggregation, and 3) decision-making plugins for ranking and selection. We have implemented the whole system and conducted a case-study validation based on public cloud providers' security data, showing the benefits of the proposed approach.},
booktitle = {Proceedings of the 2nd International Workshop on Multimedia Privacy and Security},
pages = {18–26},
numpages = {9},
keywords = {cloud computing, cloud-based multimedia systems, decision making, risk-driven security, security metrics},
location = {Toronto, Canada},
series = {MPS '18}
}

@article{10.1145/3639708,
author = {Golendukhina, Valentina and Foidl, Harald and H\"{o}rl, Daniel and Felderer, Michael},
title = {A Catalog of Consumer IoT Device Characteristics for Data Quality Estimation},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3639708},
doi = {10.1145/3639708},
abstract = {The Internet of Things (IoT) is rapidly growing and spreading across different markets, including the customer market and consumer IoT (CIoT). The large variety of gadgets and their availability makes CIoT more and more influential, especially in the wearable and smart home domains. However, the large variety of devices and their inconsistent quality due to varying hardware costs have an influence on the data produced by such devices. In this article, a catalog of CIoT properties is introduced, which enables the prediction of data quality. The data quality catalog contains six categories and 21 properties with descriptions and trust score calculation methods. A diagramming tool is implemented to support and facilitate the process of evaluation. The tool was assessed in an experimental setting with 14 users and received positive feedback. Additionally, we provide an exemplary application for smartwatch devices and compare the results obtained with the approach with the users’ evaluation based on the feedback from 158 smartwatch owners. As a result, the method-based ranking does not provide similar results to the regular users. However, it yields comparable outcomes to the assessment conducted by experienced users.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = {jan},
keywords = {IoT, CIoT, internet of things, consumer IoT, data quality, wearable devices}
}

@article{10.1145/3372407,
author = {Dong, Jialin and Yang, Kai and Shi, Yuanming},
title = {Ranking from Crowdsourced Pairwise Comparisons via Smoothed Riemannian Optimization},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3372407},
doi = {10.1145/3372407},
abstract = {Social Internet of Things has recently become a promising paradigm for augmenting the capability of humans and devices connected in the networks to provide services. In social Internet of Things network, crowdsourcing that collects the intelligence of the human crowd has served as a powerful tool for data acquisition and distributed computing. To support critical applications (e.g., a recommendation system and assessing the inequality of urban perception), in this article, we shall focus on the collaborative ranking problems for user preference prediction from crowdsourced pairwise comparisons. Based on the Bradley--Terry--Luce (BTL) model, a maximum likelihood estimation (MLE) is proposed via low-rank approach in order to estimate the underlying weight/score matrix, thereby predicting the ranking list for each user. A novel regularized formulation with the smoothed surrogate of elementwise infinity norm is proposed in order to address the unique challenge of the coupled the non-smooth elementwise infinity norm constraint and non-convex low-rank constraint in the MLE problem. We solve the resulting smoothed rank-constrained optimization problem via developing the Riemannian trust-region algorithm on quotient manifolds of fixed-rank matrices, which enjoys the superlinear convergence rate. The admirable performance and algorithmic advantages of the proposed method over the state-of-the-art algorithms are demonstrated via numerical results. Moreover, the proposed method outperforms state-of-the-art algorithms on large collaborative filtering datasets in both success rate of inferring preference and normalized discounted cumulative gain.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {19},
numpages = {26},
keywords = {Ranking, crowdsourced data, low-rank optimization, pairwise comparison, smoothed matrix manifold optimization, social Internet of Things}
}

@inproceedings{10.1145/3481127.3481215,
author = {Jiang, Shudong and Xie, Jiagui},
title = {A Shared E-Learning Resources Database Using Big Data and Cloud Environment},
year = {2021},
isbn = {9781450390064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3481127.3481215},
doi = {10.1145/3481127.3481215},
abstract = {The world has entered into the era of Big Data unwittingly. Although no universally accepted definition can be given as of now, the term “Big Data” is commonly referred to as extensive datasets, primarily in the characteristics of 5 Vs: value, variability, variety, velocity, and volume, which require scalable architectures for efficient storage, manipulation, and analysis [1]. Higher education institutions are encouraged to construct a shared E-learning resources database based on Big Data and Cloud because of its convenience and cost-savings to E-learning teachers in terms of data crawling, storage, analysis, as well as data processing, optimization and sharing [2]. This article introduces the conceptual underlying frameworks for developing a shared E-learning resources database based on Big Data and Cloud as well as its prospective benefits to teachers, students, and university-corporation connections. Its prospective application holds a vision to assist associated teachers for improved efficiency and effectiveness of data acquisition, teaching material preparation and processing, user (students) satisfaction and prominent university-corporation connections.},
booktitle = {Proceedings of the 2021 12th International Conference on E-Business, Management and Economics},
pages = {770–775},
numpages = {6},
keywords = {Keyword: E-learning,educational resources,Cloud,Big Data,Hadoop},
location = {Beijing, China},
series = {ICEME '21}
}

@inproceedings{10.1145/3661638.3661702,
author = {Wang, Xixuan and Zhao, Heyu and Deng, Huixian and Hairula, Aili and Wang, Xiao and Guo, Lijing},
title = {Real-time detection method of edge iot proxy network intrusion based on PSO-ELM},
year = {2024},
isbn = {9798400716966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661638.3661702},
doi = {10.1145/3661638.3661702},
abstract = {The edge IoT proxy network involves a large number of devices, which are large and widely distributed, making the monitoring and detection of abnormal states complex. Therefore, a real-time intrusion detection method based on Particle Swarm Optimization (PSO) for Extreme Learning Machine (ELM) edge IoT proxy network is proposed. This method first determines whether the intrusion interference items captured by the network host meet the optimal adaptive conditions through particle boundary coordinates, ensuring that each data sample corresponds only to one optimal fitness value. Then, according to the optimization expression of the particle swarm optimization algorithm, the intrusion interference information is recorded and extracted, and real-time detection is carried out using information parameters. The method also includes two key steps: promoting information aggregation and recording behavioral item indexing. Among them, the directional features of information aggregation are determined based on whether the transmission direction of communication data is the same as the transmission direction of intrusion risk items, while the recording of behavior item indexes requires a numerical matching relationship between the directional information aggregation conditions and the sampling results of intrusion samples. In summary, this method combines particle swarm optimization algorithm and edge IoT proxy network to achieve real-time detection and recording of intrusion objects. The experimental results show that this method can accurately detect the frequency of edge IoT proxy network intrusion, indicating that the application effect of this method is ideal.},
booktitle = {Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security},
pages = {337–342},
numpages = {6},
location = {Mianyang, China},
series = {AISNS '23}
}

@inproceedings{10.1145/3627050.3627052,
author = {Guo, Xuanchi and Le-Tuan, Anh and Le-Phuoc, Danh},
title = {Building a P2P RDF Store for Edge Devices},
year = {2024},
isbn = {9798400708541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627050.3627052},
doi = {10.1145/3627050.3627052},
abstract = {The Semantic Web technologies have been used in the Internet of Things (IoT) to facilitate data interoperability and address data heterogeneity issues. The Resource Description Framework (RDF) model is employed in the integration of IoT data, with RDF engines serving as gateways for semantic integration. However, storing and querying RDF data obtained from distributed sources across a dynamic network of edge devices presents a challenging task. The distributed nature of the edge shares similarities with Peer-to-Peer (P2P) systems. These similarities include attributes like node heterogeneity, limited availability, and resources. The nodes primarily undertake tasks related to data storage and processing. Therefore, the P2P models appear to present an attractive approach for constructing distributed RDF stores. Based on P-Grid, a data indexing mechanism for load balancing and range query processing in P2P systems, this paper proposes a design for storing and sharing RDF data on P2P networks of low-cost edge devices. Our design aims to integrate both P-Grid and an edge-based RDF storage solution, RDF4Led for building an P2P RDF engine. This integration can maintain RDF data access and query processing while scaling with increasing data and network size. We demonstrated the scaling behavior of our implementation on a P2P network, involving up to 16 nodes of Raspberry Pi 4 devices.},
booktitle = {Proceedings of the 13th International Conference on the Internet of Things},
pages = {33–41},
numpages = {9},
keywords = {Distributed RDF Store, Edge Devices, Peer-To-Peer system, The Semantic Web},
location = {Nagoya, Japan},
series = {IoT '23}
}

@inproceedings{10.1145/3460179.3460185,
author = {Omar, Michael and Tung Trung, Doan},
title = {Scalable Peer-to-Peer Fog Computing Integrated to a Fast-Searching Distributed Blockchain System},
year = {2021},
isbn = {9781450388948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460179.3460185},
doi = {10.1145/3460179.3460185},
abstract = {Internet of Things (IoT) is becoming a promising area to support communication of all type of devices. Blockchain aids to ensure higher security in this field, but the increase in ubiquitous connectivity leads to increase load and hence it requires a perfect search for the nodes. This paper addresses the issue by the design of a three tier P2p fog-IoT architecture using distributed blockchain. The tier-1 employs an authenticator responsible to authenticate IoT nodes with identity, IP address and physical unclonable function (PUF). To balance IoT nodes request, super peers are dynamically selected from multi-criteria ranking based optimal points (MC-RBOP). Further the requests are forwarded to blockchain present in tier-3. In tier-3 the Master node performs storage and searching. Due to the possibility of redundant data storage, Jaro-Winkler measures a similarity in the data before storing it. An Adaptive Chord with fuzzy neural (AC-FNN) is incorporated to search the lightweight U-QUARK algorithm-based hash key-values in the directory. The design of fog-IoT with new chord algorithm is implemented in network simulator-3 and the results are evaluated in terms of latency, response time, blockchain size and network usage.},
booktitle = {Proceedings of the 2021 6th International Conference on Intelligent Information Technology},
pages = {29–39},
numpages = {11},
location = {Ho Chi Minh, Viet Nam},
series = {ICIIT '21}
}

@inproceedings{10.1145/2998476.2998493,
author = {Jhummarwala, Abdul and Alkathiri, Mazin and Karamta, Miren and Potdar, M. B.},
title = {Comparative Evaluation of Various Indexing Techniques of Geospatial Vector Data for Processing in Distributed Computing Environment},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998493},
doi = {10.1145/2998476.2998493},
abstract = {The explosion of ever increasing geospatial data is today met with the challenge of maintaining it in spatial databases and utilization of traditional methods of spatial data processing. The sheer volume and complexity of spatial databases makes them an ideal candidate for use with parallel and distributed processing architectures. There is a lot of enthusiasm toward using MapReduce paradigm and distributed computing for processing of large volumes of vector data. As spatial data cannot be indexed using traditional B-tree structures used by R/DBMS, several libraries such as JSI (Java Spatial Index), libspatialindex and SpatiaLite depend upon advanced data structures such as R/R*-tree, Quad-tree and their variants for spatial indexing. These indexing mechanisms have also been natively incorporated in frameworks such as Spatial Hadoop, Hadoop GIS SATO and GeoSpark. Additionally, most widely used open source RDBMS such as MySQL, Postgres and SQLite incorporate spatial indexing using extensions/add-ons. In this paper, we benchmark and compare the performance of various spatial indexing mechanisms in addition to evaluating the performance of distributed frameworks for planet sized datasets. We conclude by highlighting the characteristics of spatial tools and frameworks for better selection and implementation of R-tree indexing in a big geo-spatial processing system.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {167–172},
numpages = {6},
keywords = {Apache Hadoop, Geospatial processing, MapReduce, R-tree, spatial indexing},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/3405962.3405989,
author = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis},
title = {Scalable Machine Learning on High-Dimensional Vectors: From Data Series to Deep Network Embeddings},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405989},
doi = {10.1145/3405962.3405989},
abstract = {There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to analyze very large collections of static and streaming sequences (a.k.a. data series), predominantly in real-time. Examples of such applications come from Internet of Things installations, neuroscience, astrophysics, and a multitude of other scientific and application domains that need to apply machine learning techniques for knowledge extraction. It is not unusual for these applications, for which similarity search is a core operation, to involve numbers of data series in the order of hundreds of millions to billions, which are seldom analyzed in their full detail due to their sheer size. Such application requirements have driven the development of novel similarity search methods that can facilitate scalable analytics in this context. At the same time, a host of other methods have been developed for similarity search of high-dimensional vectors in general. All these methods are now becoming increasingly important, because of the growing popularity and size of sequence collections, as well as the growing use of high-dimensional vector representations of a large variety of objects (such as text, multimedia, images, audio and video recordings, graphs, database tables, and others) thanks to deep network embeddings. In this work, we review recent efforts in designing techniques for indexing and analyzing massive collections of data series, and argue that they are the methods of choice even for general high-dimensional vectors. Finally, we discuss the challenges and open research problems in this area.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {1–6},
numpages = {6},
keywords = {data series, deep learning, embeddings, high-dimensional vectors, machine learning, similarity search, time series},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3407982.3408006,
author = {Nacheva, Radka and Sulova, Snezhana},
title = {Internationalization in Context of Education 4.0: AHP Ranking of Bulgarian Universities},
year = {2020},
isbn = {9781450377683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407982.3408006},
doi = {10.1145/3407982.3408006},
abstract = {The Fourth Industrial Revolution has brought about major economic, political and social changes. The world around us is moving at a rapid pace, and artificial intelligence, big data, the Internet of Things and robotics are imposing new rules on the labor market. Many job requirements have changed -- employees must have digital competencies to handle workplace responsibilities. Educational institutions also seek to meet the demands of business by changing the curriculum and teaching methods, i.e. gradually progressing towards the concept of Education 4.0. On the other hand, the pressure exerted by the globalization process on universities inevitably leads to the internationalization of education. In this regard, the purpose of this paper is to present the university ranking criteria about their internationalization. The criteria have been developed by the authoring team in the context of Education 4.0 and applied in accordance with the analytic hierarchy process (AHP).},
booktitle = {Proceedings of the 21st International Conference on Computer Systems and Technologies},
pages = {278–284},
numpages = {7},
keywords = {Education 4.0, Internationalization, analytic hierarchy process, digital education, university ranking},
location = {Ruse, Bulgaria},
series = {CompSysTech '20}
}

@inproceedings{10.1145/3371158.3371170,
author = {Tanted, Sapan and Agarwal, Anshul and Mitra, Shinjan and Bahuman, Chaitra and Ramamritham, Krithi},
title = {Database and Caching Support for Adaptive Visualization of Large Sensor Data},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371170},
doi = {10.1145/3371158.3371170},
abstract = {Rapid deployment of Internet of Things (IoT) has led to ubiquitous and pervasive sensing of objects in the physical world, such as artifacts in buildings, agriculture, cities, the electric grid, etc. Meaningful visualization of large amounts of sensor data demands user-friendly, convenient and flexible tools. In this paper, we discuss the design, implementation and performance of a novel distributed caching &amp; aggregation mechanism to handle the visualization of sensor data, which is time series data. Its features include a) bitmap indexing for capturing the dynamics of the cached data b) exploiting recency of data usage when making cache insertion and replacement decisions and c) integrating existing databases and open-source visualization platforms to provide quick and effective distributed caching solutions to handle time-series data. We evaluate our system on real-world data generated by sensors deployed in an academic building and demonstrate empirically that the system adapts to evolving workload patterns and makes it attractive for a variety of workloads.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {98–106},
numpages = {9},
keywords = {IoT, aggregation, caching, sensor data, visualization},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/3627106.3627133,
author = {P\"{u}tz, Philipp and Mitev, Richard and Miettinen, Markus and Sadeghi, Ahmad-Reza},
title = {Unleashing IoT Security: Assessing the Effectiveness of Best Practices in Protecting Against Threats},
year = {2023},
isbn = {9798400708862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627106.3627133},
doi = {10.1145/3627106.3627133},
abstract = {The Internet of Things (IoT) market is rapidly growing and is expected to double from 2020 to 2025. The increasing use of IoT devices, particularly in smart homes, raises crucial concerns as inadequate security designs and implementations by IoT vendors can lead to significant vulnerabilities endangering the privacy and security of sensitive user information handled by these devices. To address these IoT device vulnerabilities, institutions and organizations have published IoT security best practices (BPs) to guide manufacturers in ensuring the security of their products. However, there is currently no standardized approach for evaluating the effectiveness of individual BP recommendations. This leads to manufacturers investing effort in implementing less effective BPs while potentially neglecting measures with greater impact. In this paper, we propose a methodology for evaluating the security impact of IoT BPs and ranking them based on their effectiveness in protecting against security threats. Our approach involves translating identified BPs into concrete test cases that can be applied to real-world IoT devices to assess their effectiveness in mitigating vulnerabilities. We applied this methodology to evaluate the security impact of nine commodity IoT products, discovering 18 vulnerabilities. By empirically assessing the actual impact of BPs on device security, IoT designers and implementers can prioritize their security investments more effectively, improving security outcomes and optimizing limited security budgets.},
booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference},
pages = {190–204},
numpages = {15},
location = {Austin, TX, USA},
series = {ACSAC '23}
}

@inproceedings{10.1145/3372177.3373320,
author = {Verevka, Tatiana V. and Gutman, Svetlana S. and Shmatko, Alexey},
title = {Prospects for Innovative Development of World Automotive Market in Digital Economy},
year = {2020},
isbn = {9781450372442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372177.3373320},
doi = {10.1145/3372177.3373320},
abstract = {Digital transformation, creating new digital enterprises, products and business models, is changing the structure of industries. Automotive, as a high-tech industry, ranking the third in the world in terms of R&amp;D costs, is now becoming one of the leading drivers in the development of the digital economy. The aim of this work is to study the digital technology impact on the product and regional structure of the global automotive market, as well as to assess the prospects for its innovative development. The work analyzes main trends in the automotive market, explores the R&amp;D activity and R&amp;D efficiency of the world's leading automakers, presents a forecast for the innovative development of the industry under the influence of digital technologies. The research results conducted in the work show that the main trends that have the most significant consequences for the future development of the automotive market in the context of its digitalization are emergence of fundamentally new digital products (autonomous, connected cars) and digital services (mobility services), as well as a change in the quality of consumer demand caused by the development of innovative technologies. Adapting new digital products and services to the needs of individual customers through the creation of an appropriate digital strategy will allow existing automotive enterprises to enter the emerging markets and win the growing digital competition with innovative startups.},
booktitle = {Proceedings of the 2019 International SPBPU Scientific Conference on Innovations in Digital Economy},
articleno = {27},
numpages = {6},
keywords = {Automotive industry, Autonomous vehicles, Digital products, Digital transformation, Market forecast, Mobility services, Research &amp; Development (R&amp;D)},
location = {Saint Petersburg, Russian Federation},
series = {SPBPU IDE '19}
}

@inproceedings{10.1145/3230833.3232806,
author = {Michaels, Alan J.},
title = {Improved RNS-based PRNGs},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3232806},
doi = {10.1145/3230833.3232806},
abstract = {In developing pseudorandom number generation mechanisms for low-power systems like the Internet of Things (IoT), there exists a large tradeoff between computational complexity and the resulting security enabled by the generator. For most communications applications, the use of any PRNG stream must be performed in a synchronizable, and sometimes invertible, fashion. This paper focuses on improvements to prior residue number space (RNS)-based PRNGs, configured to support extremely low-power IoT applications via internal dynamics like switching between PRNG components, simpler permutation-based mappings as opposed to pre-defined polynomials, dynamic indexing processes for improved multiple access operation, and computationally efficient sequence combination techniques. While similarly scalable to applications of prior RNS-based PRNGs, simulation and hardware prototyping results on an MSP430 and Altera FPGAs (Cyclone V and Arria 10) equally validate the suitability of the proposed PRNG techniques for microprocessor-level low-power implementations like industrial IoT.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {20},
numpages = {5},
keywords = {Residue number system, pseudorandom number generation},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.1145/3297280.3297556,
author = {Peralta-Aranibar, Roger and Pahins, Cicero A. L. and Comba, Joao L. D. and Gomez-Nieto, Erick},
title = {Similarity-based visual exploration of very large georeferenced multidimensional datasets},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297556},
doi = {10.1145/3297280.3297556},
abstract = {Big data visualization is a main task for data analysis. Due to its complexity in terms of volume and variety, very large datasets are unable to be queried for similarities among entries in traditional Database Management Systems. In this paper, we propose an effective approach for indexing millions of elements with the purpose of performing single and multiple visual similarity queries on multidimensional data associated with geographical locations. Our approach makes use of Z-Curve algorithm to map into 1D space considering similarities between data. Additionally, we present a set of results using real data of different sources and we analyze the insights obtained from the interactive exploration.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {683–686},
numpages = {4},
keywords = {geographic information, interactive visualization, multidimensional data, similarity},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3277593.3277600,
author = {Le-Tuan, Anh and Hayes, Conor and Wylot, Marcin and Le-Phuoc, Danh},
title = {RDF4Led: an RDF engine for lightweight edge devices},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277600},
doi = {10.1145/3277593.3277600},
abstract = {Semantic interoperability for the Internet of Things(IoT) is being enabled by standards and technologies from the Semantic Web. As recent research suggests a move towards decentralised IoT architectures, our focus is on how to enable scalable and robust RDF engines that can be embedded throughout the architecture, in particular at edge nodes. RDF processing at edge enables the creation of semantic integration gateways for locally connected low-level devices. We introduce a lightweight RDF engine, which comprises of RDF storage and SPARQL processor, for the lightweight edge devices, called RDF4Led. RDF4Led follows the RISCstyle (Reduce Instruction Set Computer) design philosophy. The design comprises a flash-aware storage structure, an indexing scheme and a low-memory-footprint join algorithm which improves scalability as well as robustness over competing solutions. With a significantly smaller memory footprint, we show that RDF4Led can handle 2 to 5 times more data than RDF engines such as Jena TDB and Virtuoso. On three types of ARM boards, RDF4Led requires 10--30% memory of its competitors to operate up to 30 million triples dataset; it can perform faster updates and can scale better than Jena TDB and Virtuoso. Furthermore, we demonstrate considerably faster query operations than Jena TDB.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {2},
numpages = {8},
keywords = {RDF engine, edge device},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/3477314.3506964,
author = {Yousfi, Houssameddine},
title = {Spatial data processing meets RDF graph exploration: student research abstract},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3506964},
doi = {10.1145/3477314.3506964},
abstract = {Efficient processing of RDF data is a basic requirement for querying RDF knowledge graphs, which are today a centerpiece in the semantic processing of information on the Web. In this paper, we investigate the issue of efficient processing of RDF spatial data. Our aim is to extend the query evaluation strategy that relies on graph exploration to support spatial processing. We propose several methods to achieve this goal. We shown the drawbacks and advantages of each method via a few sample queries. Our first results show that the proposal that combines graph exploration and spatial indexing can give very good and promising results compared to existing approaches.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {389–392},
numpages = {4},
keywords = {GeoSPAQL, RDF, data management, query optimisation, spatial data},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3440749.3442617,
author = {Fomin, Vladimir and Aleksandrov, Igor and Gallyamov, Denis and Kirichek, Ruslan},
title = {Modified Indexing Algorithm based on Priority Queue in Metric Space for MVP Tree},
year = {2021},
isbn = {9781450388863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440749.3442617},
doi = {10.1145/3440749.3442617},
abstract = {The Internet of Things (IoT) algorithms process huge amounts of heterogeneous data in real-time. One of the most computationally intensive tasks using cloud technologies is the task of clustering and classifying data. The authors propose to develop an approach to data classification within the “Query by Similarity” paradigm, which uses the technology of data indexing based on Metric Access Methods (MAM). To improve the performance of data indexing, this paper proposes a similar nearest neighbor search method combining a multiple vantage point tree (MVP) and improved algorithms for processing the priority queue of nodes. The following two algorithms for processing the priority queue of nodes were developed: 1) algorithm for all kinds of points-queries, which makes it possible to take into account parent nodes of all higher levels; 2) algorithm for grouped based on clustering of points-queries by reusing previously obtained search results. Experimental results confirm the effectiveness of the proposed approaches and algorithms.},
booktitle = {Proceedings of the 4th International Conference on Future Networks and Distributed Systems},
articleno = {24},
numpages = {8},
keywords = {complex data indexing, metric access methods, similarity search problem research, tree data structures},
location = {St.Petersburg, Russian Federation},
series = {ICFNDS '20}
}

@article{10.1109/TASLP.2023.3343608,
author = {Wang, Xu and Zhang, Hainan and Zhao, Shuai and Chen, Hongshen and Ding, Zhuoye and Wan, Zhiguo and Cheng, Bo and Lan, Yanyan},
title = {Debiasing Counterfactual Context With Causal Inference for Multi-Turn Dialogue Reasoning},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3343608},
doi = {10.1109/TASLP.2023.3343608},
abstract = {In the multi-turn dialogue reasoning task, existing models conduct word-level interaction on the entire context to gather reasoning evidence, which aims to select the logically correct one from the candidate response options. Observing the fact that the salient reasoning evidence usually comes from certain snippets of the whole dialogue session, one promising study direction is to explicitly identify the candidate reasoning contexts correlated with the dialogue reasoning options, called option-related contexts, and then make logical inference among them. However, such option-related contexts are stained with noisy information. As a result, existing models may reason unfairly with biased context and select wrong options. To tackle the context bias problem, in this article, we propose a novel CounterFactual learning framework for Dialogue Reasoning, named CF-DialReas, which mitigates the bias information by subtracting the counterfactual representation from the total causal representation. Specifically, we consider two scenarios, i.e., factual dialogue reasoning where the whole context is available to estimate the total causal representation, and the counterfactual dialogue reasoning, which firstly utilizes three different types of utterance selectors to select option-&lt;italic&gt;unrelated&lt;/italic&gt; context, and then only the option-&lt;italic&gt;unrelated&lt;/italic&gt; context is available to guess the counterfactual representation. Experimental results on two public dialogue reasoning datasets show that the model with our mechanism can obtain higher ranking measures, validating the effectiveness of counterfactual learning of CF-DialReas. Further analysis on the generality of CF-DialReas shows that our counterfactual learning mechanism is generally effective to the widely-used models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {1125–1132},
numpages = {8}
}

@inproceedings{10.1145/3083187.3084020,
author = {Li, Qiang and Feng, Xuan and Wang, Haining and Sun, Limin},
title = {Automatically Discovering Surveillance Devices in the Cyberspace},
year = {2017},
isbn = {9781450350020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083187.3084020},
doi = {10.1145/3083187.3084020},
abstract = {Surveillance devices with IP addresses are accessible on the Internet and play a crucial role in monitoring physical worlds. Discovering surveillance devices is a prerequisite for ensuring high availability, reliability, and security of these devices. However, today's device search depends on keywords of packet head fields, and keyword collection is done manually, which requires enormous human efforts and induces inevitable human errors. The difficulty of keeping keywords complete and updated has severely impeded an accurate and large-scale device discovery. To address this problem, we propose to automatically generate device fingerprints based on webpages embedded in surveillance devices. We use natural language processing to extract the content of webpages and machine learning to build a classification model. We achieve real-time and non-intrusive web crawling by leveraging network scanning technology. We implement a prototype of our proposed discovery system and evaluate its effectiveness through real-world experiments. The experimental results show that those automatically generated fingerprints yield very high accuracy of 99% precision and 96% recall. We also deploy the prototype system on Amazon EC2 and search surveillance devices in the whole IPv4 space (nearly 4 billion). The number of devices we found is almost 1.6 million, about twice as many as those using commercial search engines.},
booktitle = {Proceedings of the 8th ACM on Multimedia Systems Conference},
pages = {331–342},
numpages = {12},
keywords = {Automatic device discovery, Network measurement, Surveillance device},
location = {Taipei, Taiwan},
series = {MMSys'17}
}

@article{10.1109/TNET.2021.3110782,
author = {Xie, Junjie and Qian, Chen and Guo, Deke and Wang, Minmei and Wang, Ge and Chen, Honghui},
title = {COIN: An Efficient Indexing Mechanism for Unstructured Data Sharing Systems},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3110782},
doi = {10.1109/TNET.2021.3110782},
abstract = {Edge computing promises a dramatic reduction in the network latency and the traffic volume, where many edge servers are placed at the edge of the Internet. Furthermore, those edge servers cache data to provide services for edge users. The data sharing among those edge servers can effectively shorten the latency to retrieve the data and further reduce the network bandwidth consumption. The key challenge is to construct an efficient data indexing mechanism no matter how the data is cached in the edge network. Although this is essential, it is still an open problem. Moreover, existing methods such as the centralized indexing and the DHT indexing in other fields fail to meet the performance demand of edge computing. This paper presents a COordinate-based INdexing (COIN) mechanism for the data sharing in edge computing. COIN maintains a virtual space where switches and data indexes are associated with their coordinates. Then, COIN distributes data indexes to indexing edge servers based on those coordinates. The COIN is effective because any query request from an edge server can be responded when the data has been stored in the edge network. More importantly, COIN is efficient in both routing path lengths and forwarding table sizes for publishing/querying data indexes. We implement COIN in a P4 prototype. Experimental results show that COIN uses 59% shorter path length and 30% less forwarding table entries to retrieve data indexes compared to using Chord, a well-known DHT solution.},
journal = {IEEE/ACM Trans. Netw.},
month = {sep},
pages = {313–326},
numpages = {14}
}

@inproceedings{10.1145/3559795.3559806,
author = {Zhang, Jiazheng and Bai, Fenhua and Shen, Tao and Gong, Bei and Luo, Jianzhao},
title = {Trusted Blockchain-based Data Fingerprinting Differential-Traceability and SkipList Indexing Methods in Privacy Protection},
year = {2022},
isbn = {9781450396622},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559795.3559806},
doi = {10.1145/3559795.3559806},
abstract = {The process of data interaction is often criticized by the public for data privacy leakage, attribution disputes, as well as being maliciously tampered with and difficult to trace the source of data. This paper proposes a data privacy protection and fingerprint tracing method based on the features of decentralization, traceability and non-tampering in blockchain technology. And we combined with the protection of data by secure network platform in trusted computing and differential-traceability algorithm (DTA). We propose a data privacy protection and fingerprint traceability method, and use the SkipList table index structure to improve interaction efficiency. We discuss the proposed data fingerprint-based traceability framework model in a trusted environment using the features of blockchain and trusted computing platforms. In this paper, the data can be processed safely and efficiently to solve the privacy problem, the transaction data is encrypted and recorded on the chain by DTA, and the interaction process uses the SkipList table index to enhance retrieval efficiency. Finally, through multiple experiments and comparison of the results obtained from security testing, it is verified that the blockchain-trusted computing data privacy protection and data fingerprint traceability SkipList table indexing method can achieve traceability for data privacy protection, transaction security management and peer-to-peer verification of encryption and decryption algorithms. We not only provide a secure, trustworthy and efficient data privacy protection model, but also bring a time efficiency optimization of 6ms per 10,000 queries.},
booktitle = {Proceedings of the 2022 4th Blockchain and Internet of Things Conference},
pages = {73–83},
numpages = {11},
keywords = {Blockchain, Data Fingerprinting, Secure Traceability, SkipList Table, Trusted Computing},
location = {Tokyo, Japan},
series = {BIOTC '22}
}

@inproceedings{10.1145/3415048.3416114,
author = {Xu, Yinong and Liao, Yunsen and Zhao, Ying},
title = {Filter Pruning Based on Connection Sensitivity},
year = {2020},
isbn = {9781450387699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415048.3416114},
doi = {10.1145/3415048.3416114},
abstract = {For the goal of reducing the remarkable redundancy in deep convolutional neural networks (CNNs), we propose an efficient framework to compress and accelerate CNN models. This work focus on pruning at filter level, mainly removing those less important filters. Firstly, we measure the importance of the filter by introducing a saliency criterion based on its corresponding connection sensitivity. In addition, we apply an algorithm, which transform a vanilla CNN module, to provide a quantitative ranking. Next, we prune the redundancy by discarding unimportant filters. Finally, we fine-tune the network to improve its accuracy. We verify the effectiveness of our method with VGGNet and ResNet on multiple datasets, such as CIFAR-10 and ImageNet ILSVRC-12. For instance, we achieve more than 50% FLOPs reduction on ResNet-56 with virtually the same accuracy as the reference network.},
booktitle = {Proceedings of the 2020 International Conference on Pattern Recognition and Intelligent Systems},
articleno = {21},
numpages = {5},
keywords = {Accelerate CNN Models, Connection Sensitivity, Convolutional Neural Networks},
location = {Athens, Greece},
series = {PRIS '20}
}

@inproceedings{10.1145/3604237.3626901,
author = {Bamford, Tom and Coletta, Andrea and Fons, Elizabeth and Gopalakrishnan, Sriram and Vyetrenko, Svitlana and Balch, Tucker and Veloso, Manuela},
title = {Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626901},
doi = {10.1145/3604237.3626901},
abstract = {Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections capture not only the time series trends but also other desirable information or properties of the financial time-series data (such as price volatility). Moreover, our approach allows user-friendly query interfaces, enabling natural language text or sketches of time-series, for which we have developed intuitive interfaces. We demonstrate the advantages of our method in terms of computational efficiency and accuracy on real historical data as well as synthetic data, and highlight the utility of latent-space projections in the storage and retrieval of financial time-series data with intuitive query modalities.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {498–506},
numpages = {9},
keywords = {Time-series, datasets, neural networks, text tagging},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3429523.3429533,
author = {Li, Wenfeng and Li, Ping},
title = {Balanced Dominating Top-k Queries over Uncertain Data},
year = {2020},
isbn = {9781450375276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429523.3429533},
doi = {10.1145/3429523.3429533},
abstract = {Uncertainty of data is inherent in many important applications. Effectively extracting valuable information to enable better decisions is important but not a trivial task over uncertain data. We have witnessed a great deal of significant researches for this purpose, such as top-k queries, skyline queries and dominated top-k queries. As for uncertainty, the common challenge that those researches face is to answer the ranking methods in consideration of user's function score and probability. In this paper, we propose a novel ranking method to select reliable and worthy results. In our method the coordinated and balanced degree of score and probability is also an evaluation target. After constructing of balance degree, we design the balanced dominating top-k query semantic and effective algorithms to identify the top-k answers. Comprehensive experiments with both real and synthetic data sets demonstrate the effectiveness and efficiency of our proposed approach.},
booktitle = {Proceedings of the 2020 5th International Conference on Cloud Computing and Internet of Things},
pages = {69–76},
numpages = {8},
keywords = {Uncertain big data, balanced dominating top-k queries, valuable information},
location = {Okinawa, Japan},
series = {CCIOT '20}
}

@article{10.1145/2489253.2489265,
author = {Bisdikian, Chatschik and Kaplan, Lance M. and Srivastava, Mani B.},
title = {On the quality and value of information in sensor networks},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/2489253.2489265},
doi = {10.1145/2489253.2489265},
abstract = {The increasing use of sensor-derived information from planned, ad-hoc, and/or opportunistically deployed sensor networks provides enhanced visibility to everyday activities and processes, enabling fast-paced data-to-decision in personal, social, civilian, military, and business contexts. The value that information brings to this visibility and ensuing decisions depends on the quality characteristics of the information gathered. In this article, we highlight, refine, and extend upon our past work in the areas of quality and value of information (QoI and VoI) for sensor networks. Specifically, we present and elaborate on our two-layer QoI/VoI definition, where the former relates to context-independent aspects and the latter to context-dependent aspects of an information product. Then, we refine our taxonomy of pertinent QoI and VoI attributes anchored around a simple ontological relationship between the two. Finally, we introduce a framework for scoring and ranking information products based on their VoI attributes using the analytic hierarchy multicriteria decision process, illustrated via a simple example.},
journal = {ACM Trans. Sen. Netw.},
month = {jul},
articleno = {48},
numpages = {26},
keywords = {AHP, Internet of things, IoT, QoI, Quality of information, VoI, analytic hierarchy process, information systems, metadata, provenance, sensor information fusion, value of information}
}

@inproceedings{10.1145/2733373.2806400,
author = {Wang, Zheng and Hu, Ruimin and Yu, Yi and Liang, Chao and Huang, Wenxin},
title = {Multi-Level Fusion for Person Re-identification with Incomplete Marks},
year = {2015},
isbn = {9781450334594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2733373.2806400},
doi = {10.1145/2733373.2806400},
abstract = {Most video surveillance suspect investigation systems rely on the videos taken in different camera views. Actually, besides the videos, in the investigation process, investigators also manually label some marks, which, albeit incomplete, can be quite accurate and helpful in identifying persons. This paper studies the problem of Person Re-identification with Incomplete Marks (PRIM), aiming at ranking the persons in the gallery according to both the videos and incomplete marks. This problem is solved by a multi-step fusion algorithm, which consists of three key steps: (i) The early fusing step exploits both visual features and marked attributes to predict a complete and precise attribute vector. (ii) Based on the statistical attribute d ominance and saliency phenomena, a dominance-saliency matching model is suggested for measuring the distance between attribute vectors. (iii) The gallery is ranked separately by using visual features and attribute vectors, and the overall ranking list is the result of a late fusion. Experiments conducted on VIPeR dataset have validated the effectiveness of the proposed method in all the three key steps. The results also show that through introducing marks, the retrieval accuracy is significantly improved.},
booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
pages = {1267–1270},
numpages = {4},
keywords = {attributes, fusion, person re-identification},
location = {Brisbane, Australia},
series = {MM '15}
}

@inproceedings{10.1145/3442555.3442582,
author = {Hossain Sani, Shaon and Shopon, Md and Hossain Khan, Mashrur and Hasan, Moenul and Mridha, M.F.},
title = {Short-term and Long-term Air Quality Forecasting Technique Using Stacked LSTM},
year = {2021},
isbn = {9781450388092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442555.3442582},
doi = {10.1145/3442555.3442582},
abstract = {For the entire globe, air pollution has been a worrying issue. Earth's Atmosphere contains numerous toxic gases and harmful solid particles are caused by Air pollution. Contaminated Air have been many mischievous effects on human health. Asthma, emphysema, chronic obstructive pulmonary disease and lung cancer can happen due to air contamination. Among other enlisted polluted cities, Dhaka lies in a hazardous problem for air pollution. This paper has approached two Long Short-Term Memory (Vanilla LSTM, Stacked LSTM) model and Gated Recurrent Unit (GRU) model to Predict air Quality Indexing with different hyper-parameter tuning. And analyze future the health effects based on Air Quality Index Level. We have worked on Dhaka Air Quality data which was collected by the United States Environmental Protection Agency (EPA). Among the two models, we have acquired the highest accuracy of 91.61% for Short-term prediction and 90.83% for Long-term prediction. And RMSE value of 4.65 and 16.19 for Air Quality Index value prediction on Stacked LSTM tuned with 200 hidden nodes on the first layer and 100 nodes on the second layer.},
booktitle = {Proceedings of the 6th International Conference on Communication and Information Processing},
pages = {165–171},
numpages = {7},
keywords = {AQI, Air Quality Index, Forecasting Technique LSTM, RNN},
location = {Tokyo, Japan},
series = {ICCIP '20}
}

@inproceedings{10.1145/3603781.3603853,
author = {Li, Haohong and Zhuang, Yi and Ge, Yujia and Lou, Tao},
title = {Towards Effective Crowd-Assisted Similarity Retrieval of Large Cursive Chinese Calligraphic Character Images},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603853},
doi = {10.1145/3603781.3603853},
abstract = {Chinese calligraphy is the art of handwriting which draws a lot of attention for its beauty and elegance. People can easily access and enjoy these priceless calligraphy works through the Internet as more and more ancient Chinese calligraphic scripts are digitalized. Despite some research on shape-based retrieval, it is still a great challenge to accurately retrieve the cursive Chinese calligraphy character image(CCI) due to the randomness and complexity of the shape. The paper proposes an effective and efficient crowd-assisted retrieval method of the CCIs which includes three supporting techniques: 1) a NRP- based similarity measure to represent calligraphic character shapes by their contour points extracted from the CCIs; 2) a Hybrid- Distance-Tree(HD-Tree)-based high-dimensional indexing scheme to boost the retrieval performance; and 3) a crowdsourcing-based human verification scheme to refine the result CCIs. Our extensive experiments have demonstrated the satisfactory performance of our proposed retrieval and indexing schemes, respectively.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {403–409},
numpages = {7},
keywords = {Chinese calligraphic character, centre distance, contour point, high- dimensional indexing, zero distance},
location = {Xiamen, China},
series = {CNIOT '23}
}

@inproceedings{10.1145/3446434.3446451,
author = {Kobzev, Vladimir and Izmaylov, Maxim and Skvortsov, Stanislav and Capo, Dorothea},
title = {Digital transformation in the Russian industry: key aspects, prospects and trends},
year = {2021},
isbn = {9781450388900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446434.3446451},
doi = {10.1145/3446434.3446451},
abstract = {The article presents a study on the features and key aspects of digital transformation in the Russian industry. Statistical data are given to reflect the share of high-tech and knowledge-intensive products in GDP, the structure of gross domestic costs aimed at digital economy development, export and import trends for goods and services related to information and communication technologies. Data obtained from international organizations on the Russia's ranking by the level of information and communication technology development were analyzed, on the basis of which it was concluded that successful industrial development forming on the "smart" principle will be challenging since international indices related to digital technology development are low.Promising areas and potential efficiency of using smart industry technologies have been identified.The analysis of barriers and risks to digital transformation of the Russian industry was carried out. Aggravation of employment and inequality problems; violations of security, confidentiality and intellectual property rights; environmental pollution risks, and geoeconomic risks to increase tension in state-to-state relations due to changes in the global production location and income distribution were determined to be the major ones.Ways to minimize risks going with the industrial digitalization processes are proposed. Digitalization trends regarding functional, sectoral and spatial aspects were proposed for the Russia's industry. A set of recommendations on fiscal, financial and credit mechanisms to digitize the industry in Russia has been developed.},
booktitle = {Proceedings of the International Scientific Conference - Digital Transformation on Manufacturing, Infrastructure and Service},
articleno = {3},
numpages = {8},
keywords = {Digital economy, Digital transformation, Digitalization, Industry, Industry 4.0, Information and communication technologies},
location = {Saint Petersburg, Russian Federation},
series = {DTMIS '20}
}

@inproceedings{10.1145/3603781.3603836,
author = {Peng, Yaowei and Qian, Xuezhong and Song, Wei},
title = {A Re-ranking Approach for Two-sided Fairness on Recommendation Systems},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603836},
doi = {10.1145/3603781.3603836},
abstract = {The filter bubble problem has long constrained users of recommender systems from using it freely. Two stakeholders of the recommendation system, which refer to the content consumer, and the content provider, are disturbed by the meaningless repeating of few high-frequency contents. While most previous work concerns the fairness issue of recommenders from one side, in this paper we provide a new lightweight approach through a re-ranking method increasing fairness for both sides. Experiments on 2 datasets and 4 existing models demonstrate that our proposed algorithm can reduce unfairness and increase overall accuracy. The time complexity for our approach is linear to the total user amount for each user. And it fits all existing recommendation systems that generate a rank score.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {312–316},
numpages = {5},
keywords = {Fair Recommendation, Information Retrieval, Re-ranking, Two-sided Fairness},
location = {Xiamen, China},
series = {CNIOT '23}
}

@inproceedings{10.1145/3366423.3380245,
author = {Kolbe, Niklas and Vandenbussche, Pierre-Yves and Kubler, Sylvain and Le Traon, Yves},
title = {LOVBench: Ontology Ranking Benchmark},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380245},
doi = {10.1145/3366423.3380245},
abstract = {Ontology search and ranking are key building blocks to establish and reuse shared conceptualizations of domain knowledge on the Web. However, the effectiveness of proposed ontology ranking models is difficult to compare since these are often evaluated on diverse datasets that are limited by their static nature and scale. In this paper, we first introduce the LOVBench dataset as a benchmark for ontology term ranking. With inferred relevance judgments for more than 7000 queries, LOVBench is large enough to perform a comparison study using learning to rank (LTR) with complex ontology ranking models. Instead of relying on relevance judgments from a few experts, we consider implicit feedback from many actual users collected from the Linked Open Vocabularies (LOV) platform. Our approach further enables continuous updates of the benchmark, capturing the evolution of ontologies’ relevance in an ever-changing data community. Second, we compare the performance of several feature configurations from the literature using LOVBench in LTR settings and discuss the results in the context of the observed real-world user behavior. Our experimental results show that feature configurations which are (i) well-suited to the user behavior, (ii) cover all features types, and (iii) consider decomposition of features can significantly improve the ranking performance.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1750–1760},
numpages = {11},
keywords = {ground truth mining, learning to rank, ontology reuse, ontology search, semantic interoperability},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/2932194.2932199,
author = {Mazumdar, Parijat and Wang, Li and Winslet, Marianne and Zhang, Zhenjie and Jung, Deokwoo},
title = {An index scheme for fast data stream to distributed append-only store},
year = {2016},
isbn = {9781450343107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2932194.2932199},
doi = {10.1145/2932194.2932199},
abstract = {Distributed systems are now commonly used to manage massive data flooding from physical world, such as user-generated contents from online social media and communication records from mobile phones. The new generation of distributed data management systems, such as HBase, are designed to accept tuple insertions only, such that other database operations (e.g., deletion and update) are simply simulated by appending operation logs with keys associated to the target tuples. Such append-only store architecture maximizes the processing throughput on incoming data, but potentially incur higher costs on query processing, in which additional computation is needed to generate consistent snapshots of the database. Indexing is known as the key to enable efficient query processing by fast data retrieval and aggregation under such system architecture. This paper presents a new indexing scheme for distributed append-only stores. Our new scheme utilizes traditional index structures based on B-trees and its variant without the overhead of expensive node split, by using template-based tree construction. Optimized domain partitioning and multi-thread insertion techniques are further proposed to exploit the advantages of our template B-tree structure. Empirical evaluations show that our proposal outperforms existing solutions on insertion throughput by a large margin on a variety of real and synthetic workloads.},
booktitle = {Proceedings of the 19th International Workshop on Web and Databases},
articleno = {5},
numpages = {6},
location = {San Francisco, California},
series = {WebDB '16}
}

@inbook{10.1145/3122865.3122866,
title = {Preface},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122866},
abstract = {The field of multimedia is dedicated to research and studies that leverage multiple modalities of signals and data in developing intelligent systems and technologies. Be it search engine, recommendation system, streaming service, interactive agent, or collaborative system, multimedia plays a critical role in ensuring full understanding of multimodal sensory signals, robust modeling of user-content interaction, natural and rich communication experience, and scalable system deployment. The goal is to utilize unique contributions from each modality, integrate complementary synergies, and achieve the best performance and novel functions beyond what's separately available in each individual medium. In this community, most contributors also maintain strong activities in other disciplines such as networking, computer vision, human-computer interaction, and machine learning. But the field of multimedia is unique in offering a rich and dynamic forum for researchers from "traditional" fields to collaborate and develop new solutions and knowledge that transcend the boundaries of individual disciplines.The field enjoys a long history of vibrant research. For example, the flagship ACM SIGMM Multimedia Conference was established in 1993, celebrating its 25th anniversary this year. The community also has several well-known conferences and journals organized by ACM, IEEE, and other groups, attracting a large number of researchers and practitioners from around the world. However, despite the prolific research activities and outcomes, there has been less effort toward developing books that serve as an introduction to the rich spectrum of topics in this broad field. Most of the few books available today either focus on specific subfields or basic background. There is a lack of tutorial-style materials covering the active topics being pursued by the leading researchers at frontiers of the field.SIGMM launched a new initiative to address this need in 2015, by selecting and inviting 12 rising-star speakers from different subfields of multimedia to deliver plenary tutorial style talks at ACM Multimedia 2015. Each speaker discussed challenges and the state of the art within their prospective research areas in a general manner to the broad community. Topics covered were comprehensive, including multimedia content understanding, multimodal human-human and human-computer interaction, multimedia social media, and multimedia system architecture and deployment. Following the very positive responses to the talks, these rising-star speakers were invited to expand the content covered in their talks to chapters that can be used as reference materials for researchers, students, and practitioners. Each resulting chapter discusses problems, technical challenges, state-of-the-art approaches and performances, open issues, and promising directions for future work. Collectively, the chapters provide an excellent sampling of major topics addressed by the community as a whole. This book, capturing outcomes of such efforts, is well positioned to fill the aforementioned needs by providing tutorial-style reference materials for frontier topics of multimedia.Section 1 of the book includes five chapters that are focused on analysis and understanding of multimedia content. Topics covered range from analysis of video content, audio content, multimodal content about interaction of freestanding conversational groups, and analysis of multimedia data in the encrypted format for preserving privacy on cloud servers, to efficient approximate similarity search techniques for searching over large-scale databases.First, Zuxuan Wu et al. review current research on understanding video content by detecting the classes of actions or events contained in a given video clip and generation of full-sentence captions describing the content in each such video. Unlike previous surveys, this review focuses on solutions based on deep learning, reflecting the recent trend of research in this area. The chapter also gives extensive reviews of the datasets used in state-of-the-art research and benchmarking efforts.Extending the modality from video to audio, in Chapter 2, Gerald Friedland et al. introduce the field of computer audition, aiming to develop the theory behind artificial systems that can extract information from sound. This chapter reviews the research datasets available, appropriate representations needed for audio, and a few challenging problems such as automatic extraction of hierarchical semantic structures from audio content and automatic discovery of high-level semantic concepts from massive audio data and associated metadata.The holy grail of research for the multimedia community is to be able to integrate and fuse information extracted from multiple modalities of data. In Chapter 3, Xavier Alameda-Pineda et al. present an excellent example and emergent research challenges in the application of detecting social interaction among freestanding conversational groups. The chapter includes overviews of research issues, approaches, evaluation of joint estimation of head and body poses using multiPreface modality data (such as wearable sensors and distributed camera networks), and results of detecting dynamic group formation of interacting people.Chapter 4 addresses a novel emerging topic prompted by the popular approach to multimedia analysis using cloud computing servers. When multimedia data is sent to the cloud for storage or processing, there is a risk of privacy breach via unauthorized access by third parties to the content in the cloud. Pradeep Atrey et al. review state-of-the-art methods and open issues for processing multimedia content in the encrypted domain without needing to convert data to the original format. This allows content to stay in its protected form while useful analysis is performed on it.In Chapter 5, Herv\'{e}e Jeundefinedou surveys efficient techniques for finding approximate solutions for similarity search, which is of particular interest when searching massive multimedia data like images, videos, and audio recordings. Jeundefinedou considers various performance factors like query speed, memory requirement, and search accuracy. Multiple frameworks based on locality sensitive hashing (LSH), quantization/ compression, and hybrid combinations are also reviewed in a coherent manner.In Section 2 of the book, the emphasis shifts from content analysis to humancentered aspects of multimedia computing. This new focus goes beyond extraction of semantic information from multimedia data. Instead, the broad research scope incorporates understanding of users and user-content interaction so as to improve effectiveness of multimedia systems in many applications, such as search and recommendation.Under the human-centric theme, Chapter 6, authored by Peng Cui, discusses the evolution of multimedia computing paradigms from the data-centric, to the content-centric, and recently to the human-centric. Cui presents a new framework, called social-sensed multimedia computing, to capture many key issues involved and advances achieved, including understanding of user-content interaction behavior, understanding of user intent, multimedia representation considering user intention, and integration of heterogeneous data sensed on multimedia social networks.Chapter 7 follows the human-centric theme and further moves the focus from processing individual multimedia data streams to processing a large number of heterogeneous streams in different modalities involving a large number of people. Analysis of such massive streams offers the possibility of detecting important situations of society, such as socio-economic affairs, as well as the living environment. Vivek Singh provides an overview of the problem definition, research framework, and the EventShop toolkit he developed for application development in this emerging area.The extension to the human-centric computing paradigm also calls for formal mathematical theories and tools for explaining the phenomena observed, such as the information propagation behaviors and the occurrences of information cascades on social networks. In Chapter 8, Marian-Andrei Rizoiu et al. review stochastic processes such as the Hawkes point process for modeling discrete, interdependent events over continuous time. These are strongly related to patterns corresponding to retweet cascade events on social media. Successful models like these can help researchers understand information dissemination patterns and predict popularity on social media.Interaction between users and content reveals not only the intent of the user (covered in Chapter 6), but also attributes of the content as well as of the user him/herself. Such interaction can be manifested in multiple forms including explicit cues such as visual and verbal expressions, and implicit cues such as eye movement and physiological signals like brain activity and heart rate. Chapter 9 includes a survey by Subramanian Ramanathan et al. on how such implicit user interaction cues can be explored to improve analysis of content (scene understanding) and user (user emotion recognition).To support research and development of emerging multimedia topics discussed above, there is a critical need for new generations of communication and computing systems that take into account the unique requirements of multimedia, such as real-time, high bandwidth, distributiveness, major power consumption, and resource uncertainty. The popular cloud-based computing systems, though prevalent formanyapplications, are not suitable for large-scale multimedia applications such as cloud-based gaming service and animation rendering service.The last section of the book focuses on the systems aspect, covering distinct topics of multimedia fog computing (Chapter 10) and cloud gaming (Chapter 11). Cheng-Hsin Hsu et al. survey the emerging paradigm focused on fog computing, in which computing services are crowdsourced to the edge nodes or even to the client devices on the user end. This offers major potential benefits in terms of low latency, location awareness, scalability, and heterogeneity. However, it also poses many significant challenges in areas such as resource discovery, resource allocation and management, quality of service, and security. Discussion of these challenges, along with recent advances in this area, are presented in Chapter 10.Finally, as a concrete example of large-scale distributed multimedia computing systems, Chapter 11 (by Kuan-Ta Chen et al.) presents a comprehensive survey of cloud gaming, with emphasis on the development of platform and testbed, test scenarios, and evaluation of performance, in order to enhance optimal design of various components of the complex cloud gaming systems. In particular, the chapter overviews extensive research in areas such as open-source platforms, cloud deployment, client design, and communication between gaming servers and clients.The scope of this book is by no means exhaustive or complete. For example, it can be expanded to include other important topics such as (but not limited to) multimedia content generation, multimodal knowledge discovery and representation, multimedia immersive networked environments, and applications in areas like healthcare, learning, and infrastructure. Nonetheless, the comprehensive survey materials already covered in the book provide an excellent foundation for exploring additional topics mentioned above, and many other relevant fields.We would like to give sincere acknowledgment to Dr. Svebor Karaman, who has provided tremendous assistance in communicating with contributors and organizing the content of this book. In addition, Diane Cerra and her team at Morgan &amp; Claypool Publishers have provided valuable guidance and editorial help},
booktitle = {Frontiers of Multimedia Research},
pages = {xi–xv}
}

@inproceedings{10.1145/3210713.3210737,
author = {Shi, Shumin and Huang, Heyan and Li, Kan},
title = {The construction and practice of integration cultivation mechanism for innovative talents in CS discipline},
year = {2018},
isbn = {9781450364157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210713.3210737},
doi = {10.1145/3210713.3210737},
abstract = {Along with the imperatives of Chinese education strategy and the practical requirements of our university (Beijing Institute of Technology, BIT)'s Double-First Class development blueprint, the cultivation of innovative talents becomes even more important. Since more than a decade of professional accumulation and talent training, school of computer science &amp; technology has explored and gradually practiced an Integration Cultivation Mechanism for Innovative Talents in CS Discipline, namely ICM4IT. This paper introduces ICM4IT from the five aspects in details. That is, idea (Outcomes-Based Education, OBE), mechanism (Bachelor-Master-PhD penetration cultivation), mode (Multi-layer/mode/platform internationalization), practice (MOOC/SPOC-based online &amp; offline teaching), support (research &amp; education coordination). Education data shows that ICM4IT in CS-discipline at BIT has achieved remarkable achievements. CS ranking enters A-Class according to results of Discipline Evaluation in National Colleges and Universities announced on Dec 28, 2017. Furthermore, authors also share the experiences of above five parts during the implementation processes, as well as their thinking of solving emerging issues under the New Engineering Discipline.},
booktitle = {Proceedings of ACM Turing Celebration Conference - China},
pages = {110–114},
numpages = {5},
keywords = {OBE, innovative talents, internationalization cultivation, online/offline teaching, penetration cultivation, research &amp; education coordination},
location = {Shanghai, China},
series = {ACM TURC '18}
}

@article{10.1109/TNET.2023.3329005,
author = {Liu, Jun and Liu, Jianchun and Xu, Hongli and Liao, Yunming and Wang, Zhiyuan and Ma, Qianpiao},
title = {YOGA: Adaptive Layer-Wise Model Aggregation for Decentralized Federated Learning},
year = {2023},
issue_date = {April 2024},
publisher = {IEEE Press},
volume = {32},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3329005},
doi = {10.1109/TNET.2023.3329005},
abstract = {Traditional Federated Learning (FL) is a promising paradigm that enables massive edge clients to collaboratively train deep neural network (DNN) models without exposing raw data to the parameter server (PS). To avoid the bottleneck on the PS, Decentralized Federated Learning (DFL), which utilizes peer-to-peer (P2P) communication without maintaining a global model, has been proposed. Nevertheless, DFL still faces two critical challenges, i.e., limited communication bandwidth and not independent and identically distributed (non-IID) local data, thus hindering efficient model training. Existing works commonly assume full model aggregation at periodic intervals, i.e., clients periodically collect models from peers. To reduce the communication cost, these methods allow clients to collect model(s) from selected peers, but often result in a significant degradation of model accuracy when dealing with non-IID data. Alternatively, the layer-wise aggregation mechanism has been proposed to alleviate communication overhead under the PS architecture, but its potential in DFL remains rarely explored yet. To this end, we propose an efficient DFL framework YOGA that adaptively performs layer-wise model aggregation and training. Specifically, YOGA first generates the ranking of layers in the model according to the learning speed and layer-wise divergence. Combining with the layer ranking and peers’ status information (i.e., data distribution and communication capability), we propose the max-match (MM) algorithm to generate the proper layer-wise model aggregation policy for the clients. Extensive experiments on DNN models and datasets show that YOGA saves communication cost by about 45% without sacrificing the model performance compared with the baselines, and provides 1.53-&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$3.5times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; speedup on the physical platform.},
journal = {IEEE/ACM Trans. Netw.},
month = {nov},
pages = {1768–1780},
numpages = {13}
}

@inproceedings{10.1145/3384943.3409418,
author = {Li, Zhenzhen and Xia, Wei and Cui, Mingxin and Fu, Peipei and Gou, Gaopeng and Xiong, Gang},
title = {Mining the Characteristics of the Ethereum P2P Network},
year = {2020},
isbn = {9781450376105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384943.3409418},
doi = {10.1145/3384943.3409418},
abstract = {Ethereum is the typical representative of the second generation of Blockchain technology. To achieve anonymity and decentralization, Ethereum utilizes a structured P2P network for data broadcasting, replication, and synchronization. The Ethereum P2P network topology construction profoundly affects the overall Ethereum system performance and reliability. However, there are few studies on the Ethereum underlying P2P overlay network topology. In this paper, we perform a preliminary investigation on this extensive Ethereum P2P system with a comprehensive measurement method combining active and passive methods. We gather Ethereum nodes information by active method for nodes crawling and verification, and we use the passive network traffic monitoring for routing data gathering. After a period of data collection for more than one month, we get a massive data set that has more than 50 million routing data entries. This data set allows us to gain knowledge on the Ethereum network scale and to derive mathematical characteristics of overlay network structure. By analyzing the data set comprehensively, we find that the Ethereum network exhibits both small-world and scale-free characteristics. There are a few critical nodes that have a significant impact on the robustness and vulnerability of the whole system. The Ethereum network also has a very high concentration in terms of network routing, which is very vulnerable to routing attacks.},
booktitle = {Proceedings of the 2nd ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {20–30},
numpages = {11},
keywords = {Ethereum, blockchain, complex network theory, measurement, network traffic, peer-to-peer network},
location = {Taipei, Taiwan},
series = {BSCI '20}
}

@article{10.1145/3411838,
author = {Zhang, Lefan and He, Weijia and Morkved, Olivia and Zhao, Valerie and Littman, Michael L. and Lu, Shan and Ur, Blase},
title = {Trace2TAP: Synthesizing Trigger-Action Programs from Traces of Behavior},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3411838},
doi = {10.1145/3411838},
abstract = {Two common approaches for automating IoT smart spaces are having users write rules using trigger-action programming (TAP) or training machine learning models based on observed actions. In this paper, we unite these approaches. We introduce and evaluate Trace2TAP, a novel method for automatically synthesizing TAP rules from traces (time-stamped logs of sensor readings and manual actuations of devices). We present a novel algorithm that uses symbolic reasoning and SAT-solving to synthesize TAP rules from traces. Compared to prior approaches, our algorithm synthesizes generalizable rules more comprehensively and fully handles nuances like out-of-order events. Trace2TAP also iteratively proposes modified TAP rules when users manually revert automations. We implemented our approach on Samsung SmartThings. Through formative deployments in ten offices, we developed a clustering/ranking system and visualization interface to intelligibly present the synthesized rules to users. We evaluated Trace2TAP through a field study in seven additional offices. Participants frequently selected rules ranked highly by our clustering/ranking system. Participants varied in their automation priorities, and they sometimes chose rules that would seem less desirable by traditional metrics like precision and recall. Trace2TAP supports these differing priorities by comprehensively synthesizing TAP rules and bringing humans into the loop during automation.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {104},
numpages = {26},
keywords = {IFTTT, Internet of Things, IoT, Trigger-action programming, end-user programming, smart environment, symbolic reasoning}
}

@inproceedings{10.1145/2612733.2612741,
author = {Priano, F\'{e}lix Herrera and Guerra, Cristina Fajardo},
title = {A framework for measuring smart cities},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2612741},
doi = {10.1145/2612733.2612741},
abstract = {Smart cities are an international phenomenon. Many cities are actively working to build or transform their models toward that of a Smart City. There is constant research and reports devoted to measuring the intelligence of cities through establishing specific methodologies and indicators (grouped by various criteria).We believe the subject lacks a certain uniformity, which we aim to redress in this paper by suggesting a framework for properly measuring the smart level of a city.Cities are complex and heterogeneous structures, which complicates comparisons between them. To address this we propose an N--dimensional measurement framework where each level or dimension supplies information of interest that is evaluated independently. As a result, the measure of a city's intelligence is the result of the evaluations obtained for each of these levels.To this end, we have typified the transformation (city to smart city) and the measurement (smart city ranking) processes.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {44–54},
numpages = {11},
keywords = {framework, measurement, model, planning, project, ranking, smart city},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@inproceedings{10.1145/3508546.3508548,
author = {Deng, Xin and Chen, Songjian and Chen, Yifan and Xu, Jie-Fang},
title = {Multi-level Convolutional Transformer with Adaptive Ranking for Semi-supervised Crowd Counting},
year = {2022},
isbn = {9781450385053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508546.3508548},
doi = {10.1145/3508546.3508548},
abstract = {Most existing crowd counting methods have focused on pure convolutional neural network based supervised algorithms. Although these methods have attained good results on some datasets, they still encounter several common problems. The cost of labeling annotations for supervised methods is huge and the shortage of labeled datasets limits the further development of supervised algorithms for crowd counting. Meanwhile, pure CNN-based algorithms have certain limitations in building the connections among these features. To overcome those problems, we proposed a semi-supervised crowd counting algorithm that is a mixture model of CNN and transformer. Specifically, our method consists of two parts Multi-Level Convolutional Transformer (MLCT) and Adaptive Scale Module (ASM). MLCT is the counting branch, with its front end and back end being the CNN and the transformer, respectively. ASM outputs an adaptive scale factor for the unlabeled crowd images. We generate a ranking list based on this factor, which is fed into the MLCT and computes loss by the order of the list. Different from most crowd counting methods, we use a region-level regression target for labeled images, which is a weaker regression approach than the location regression. Furthermore, We train the entire model using a novel loss function that combines L1 loss and ranking loss. Experimental results on the three challenging datasets ShanghaiTech Part A, ShanghaiTech Part B, and UCF-QNRF have all demonstrated the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {2},
numpages = {7},
keywords = {convolution, crowd counting, ranking, transformer},
location = {Sanya, China},
series = {ACAI '21}
}

@inproceedings{10.1145/2740908.2741753,
author = {Butt, Anila Sahar},
title = {Ontology Search: Finding the Right Ontologies on the Web},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2741753},
doi = {10.1145/2740908.2741753},
abstract = {With the recent growth of Linked Data on the Web, there is an increased need for knowledge engineers to find ontologies to describe their data. Only limited work exists that addresses the problem of searching and ranking ontologies based on keyword queries. In this proposal we introduce the main challenges to find appropriate ontologies, and preliminary solutions to address these challenges. Our evaluation shows that the proposed solution performs significantly better than existing solutions on a benchmark ontology collection for the majority of the sample queries defined in the benchmark.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {487–491},
numpages = {5},
keywords = {ontology benchmark, ontology ranking, ontology search},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3539618.3591676,
author = {He, Qiang and Tan, Siyu and Chen, Feifei and Xu, Xiaolong and Qi, Lianyong and Hei, Xinhong and Jin, Hai and Yang, Yun},
title = {EDIndex: Enabling Fast Data Queries in Edge Storage Systems},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591676},
doi = {10.1145/3539618.3591676},
abstract = {In an edge storage system, popular data can be stored on edge servers to enable low-latency data retrieval for nearby users. Suffering from constrained storage capacities, edge servers must process users' data requests collaboratively. For sourcing data, it is essential to find out which edge servers in the system have the requested data. In this paper, we make the first attempt to study this edge data query (EDQ) problem and present EDIndex, a distributed Edge Data Indexing system to enable fast data queries at the edge. First, we introduce a new index structure named Counting Bloom Filter (CBF) tree for facilitating edge data queries. Then, to improve query performance, we enhance EDIndex with a novel index structure named hierarchical Counting Bloom Filter (HCBF) tree. In EDIndex, each edge server maintains an HCBF tree that indexes the data stored on nearby edge servers to facilitate data sourcing between edge servers at the edge. The results of extensive experiments conducted on an edge storage system comprised of 90 edge servers demonstrate that EDIndex 1) takes up to 8.8x less time to answer edge data queries compared with state-of-the-art edge indexing systems; and 2) can be implemented in practice with a high query accuracy at low initialization and maintenance overheads.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {675–685},
numpages = {11},
keywords = {data retrieval, edge computing, edge data query},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3150226,
author = {Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.},
title = {Multimedia Big Data Analytics: A Survey},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3150226},
doi = {10.1145/3150226},
abstract = {With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {10},
numpages = {34},
keywords = {5V challenges, Big data analytics, data mining, indexing, machine learning, mobile multimedia, multimedia analysis, multimedia databases, retrieval, survey}
}

@inproceedings{10.1145/3195970.3195991,
author = {Zhang, Boyu and Maga\~{n}a, Jonathon Crandall and Davoodi, Azadeh},
title = {Analysis of security of split manufacturing using machine learning},
year = {2018},
isbn = {9781450357005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195970.3195991},
doi = {10.1145/3195970.3195991},
abstract = {This work is the first to analyze the security of split manufacturing using machine learning, based on data collected from layouts provided by industry, with 8 routing metal layers, and significant variation in wire size and routing congestion across the layers. We consider many types of layout features for machine learning including those obtained from placement, routing, and cell sizes. For the top split layer, we demonstrate dramatically better results in proximity attack compared to a recent prior work. We analyze the ranking of the features used by machine learning and show the importance of how features vary when moving to the lower layers. Since the runtime of our basic machine learning becomes prohibitively large for lower layers, we propose novel techniques to make it scalable with little sacrifice in effectiveness of the attack.},
booktitle = {Proceedings of the 55th Annual Design Automation Conference},
articleno = {141},
numpages = {6},
keywords = {machine learning, reverse engineering, split manufacturing},
location = {San Francisco, California},
series = {DAC '18}
}

@inproceedings{10.1145/3653081.3653163,
author = {Li, Renjie and Peng, Xin},
title = {Analysis and Design of Multiple Constraint Indexes in Path Planning},
year = {2024},
isbn = {9798400716485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653081.3653163},
doi = {10.1145/3653081.3653163},
abstract = {Aiming at the complex constraints in the path planning problem on cruise ships, this paper, after considering the needs of various groups of people such as tourists and crew on cruise ships, extracts the attributes that are required by various groups of people on the road. It. proposes passenger attributes that reflect the needs of different groups of people. In addition, this paper explores the constraints of route planning on cruise ships and the travel goals of passengers, including the availability of road sections for certain groups of people, congestion problems, potential preferences of tourists, safety, etc. Based on the data model of the road section on the cruise ship, this paper establishes a hierarchical structure of road weights with multiple dimensionless attributes. The relative importance of each preference requirement is determined by the importance ranking of the input. The relative importance of congestion and safety is determined according to the type of input personnel. Finally, the weight value of each road section is obtained by AHP. Dijkstra's algorithm is used to search for the optimal path under different constraints, and the rationality and effectiveness of the multi-constraint index design are verified.},
booktitle = {Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence},
pages = {492–497},
numpages = {6},
location = {Nanchang, China},
series = {IoTAAI '23}
}

@inproceedings{10.1145/2820783.2820843,
author = {Wang, Ran and Chow, Chi-Yin and Lyu, Yan and Lee, Victor C. S. and Kwong, Sam and Li, Yanhua and Zeng, Jia},
title = {TaxiRec: recommending road clusters to taxi drivers using ranking-based extreme learning machines},
year = {2015},
isbn = {9781450339674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820783.2820843},
doi = {10.1145/2820783.2820843},
abstract = {Utilizing large-scale GPS data to improve taxi services becomes a popular research problem in the areas of data mining, intelligent transportation, and the Internet of Things. In this paper, we utilize a large-scale GPS data set generated by over 7,000 taxis in a period of one month in Nanjing, China, and propose TaxiRec; a framework for discovering the passenger-finding potentials of road clusters, which is incorporated into a recommender system for taxi drivers to hunt passengers. In TaxiRec, we first construct the road network by defining the nodes and road segments. Then, the road network is divided into a number of road clusters through a clustering process on the mid points of the road segments. Afterwards, a set of features for each road cluster is extracted from real-life data sets, and a ranking-based extreme learning machine (ELM) model is proposed to evaluate the passenger-finding potential of each road cluster. Experimental results demonstrate the feasibility and effectiveness of the proposed framework.},
booktitle = {Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {53},
numpages = {4},
keywords = {extreme learning machine, passenger-finding potential, recommender system, taxi trajectory data},
location = {Seattle, Washington},
series = {SIGSPATIAL '15}
}

@inproceedings{10.1145/3532640.3532652,
author = {WANG, JIE and GE, LINA},
title = {Consensus Algorithm of Proof-of-Stake Based on Credit Model},
year = {2022},
isbn = {9781450395762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532640.3532652},
doi = {10.1145/3532640.3532652},
abstract = {Blockchain has attracted widespread attention from scholars due to its decentralization characteristics. Consensus algorithm is one of the core technologies of blockchain. The proposal of the Proof-of-Stake consensus algorithm solves the problem of wasted hashrate in Proof-of-Work. Aiming at the problem of interest centralization in Proof-of-Stake we propose an improved PoS scheme based on credit model. Design the credit evaluation model, using the credit evaluation mechanism to quantify the credit status of each node in the network, and adjust the hash difficulty according to the credit ranking. Introduce the concept of Gini coefficient to analyze the centralization of stake of the algorithm. Simulation experiments show that the PoS improvement scheme based on the credit model effectively relieves the power centralization, guarantees the block chain decentralization.},
booktitle = {Proceedings of the 2022 4th International Conference on Blockchain Technology},
pages = {88–94},
numpages = {7},
location = {Shanghai, China},
series = {ICBCT '22}
}

@inproceedings{10.1145/3321408.3321578,
author = {Zhang, Rui and Guo, Jiming and Zhou, Yueqi and Jiang, Hongbo and Wang, Chen},
title = {traj2bits: indexing trajectory data for efficient query},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3321578},
doi = {10.1145/3321408.3321578},
abstract = {With the popularity of mobile devices and the rapid development of position acquisition technology, the amount of trajectory data has soared dramatically. It is time-consuming to manage and mine massive trajectory data, because we need to access different trajectory samples or different parts of a trajectory for multiple times. Therefore, it is necessary to devise an efficient data management technology to fast retrieve the desired trajectories. In general, building indexes is a basic step for solving query problems. However, traditional spatial indexing technologies are mostly designed for moving objects, and thus are unable to achieve fast trajectory data query and efficient computing analysis. In this regard, we propose traj2bits, a bitmap-based trajectory data encoding schema, to convert trajectories into binary strings. Based on traj2bits, we also design a trajectory query method. Experiments on two real datasets have shown that traj2bits improves the spatio-temporal efficiency of trajectory query. Compared with other schemes, traj2bitsuery encoding occupies less than 1/10 of the disk space, its encoding efficiency is at least four times faster and its range query time is reduced by at least 65%.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {17},
numpages = {5},
keywords = {bitmap, index, query, trajectory data},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3178212.3178223,
author = {Setiawan, Foni Agus and Budiardjo, Eko K. and Basaruddin, T. and Aminah, Siti},
title = {A Systematic Literature Review on Combining Ontology with Bayesian Network to Support Logical and Probabilistic Reasoning},
year = {2017},
isbn = {9781450354882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178212.3178223},
doi = {10.1145/3178212.3178223},
abstract = {Reasoning in ontology is currently limited to logical reasoning. It is because ontology does not have a standard for probabilistic reasoning. Various approaches have been made by researchers to add the ability for ontological reasoner to do probabilistic reasoning. The approach is done by combining ontology with Bayesian network that does have probabilistic reasoning abilities. This study mapped out various approaches performed in combining ontologies with Bayesian networks to realize logical and probabilistic reasoning simultaneously. We use a systematic literature review method to identify the primary studies on combining ontology with Bayesian network following a predefined review protocol. We searched from four indexing services (SCOPUS, IEEE Xplore, ACM Digital Library, and SpringerLink) and got the result of 74 papers accepted for the review. We extracted properties from these studies and found 8 motivations of the studies, 5 contexts, 4 factors involved, and 5 techniques used in combining ontology with Bayesian network. The aim and the context are clearly stated in most of the studies, while most of the authors did not completely discuss the threats in their papers. As for the method and findings, most of the studies describe and discuss it moderately.},
booktitle = {Proceedings of the 2017 International Conference on Software and E-Business},
pages = {1–12},
numpages = {12},
keywords = {Bayesian network, Logical reasoning, Ontology, Ontology-based application, Probabilistic reasoning},
location = {Hong Kong, Hong Kong},
series = {ICSEB '17}
}

@inproceedings{10.1145/3318464.3389701,
author = {Tahboub, Ruby Y. and Rompf, Tiark},
title = {Architecting a Query Compiler for Spatial Workloads},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389701},
doi = {10.1145/3318464.3389701},
abstract = {Modern location-based applications rely extensively on the efficient processing of spatial data and queries. Spatial query engines are commonly engineered as an extension to a relational database or a cluster-computing framework. Large parts of the spatial processing runtime is spent on evaluating spatial predicates and traversing spatial indexing structures. Typical high-level implementations of these spatial structures incur significant interpretive overhead, which increases latency and lowers throughput. A promising idea to improve the performance of spatial workloads is to leverage native code generation techniques that have become popular in relational query engines. However, architecting a spatial query compiler is challenging since spatial processing has fundamentally different execution characteristics from relational workloads in terms of data dimensionality, indexing structures, and predicate evaluation. In this paper, we discuss the underlying reasons why standard query compilation techniques are not fully effective when applied to spatial workloads, and we demonstrate how a particular style of query compilation based on techniques borrowed from partial evaluation and generative programming manages to avoid most of these difficulties by extending the scope of custom code generation into the data structures layer. We extend the LB2 main-memory query compiler, a relational engine developed in this style, with spatial data types, predicates, indexing structures, and operators. We show that the spatial extension matches the performance of specialized library code and outperforms relational and map-reduce extensions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2103–2118},
numpages = {16},
keywords = {code generation, extensible compilers, futamura projections, query compilation, spatial query processing, staging},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1145/3662735,
author = {Zhang, Tiangmeng and Chen, Renhui and Li, Zijing and Gao, Congming and Wang, Chengke and Shu, Jiwu},
title = {Design and Implementation of Deduplication on F2FS},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1553-3077},
url = {https://doi.org/10.1145/3662735},
doi = {10.1145/3662735},
abstract = {Data deduplication technology has gained popularity in modern file systems due to its ability to eliminate redundant writes and improve storage space efficiency. In recent years, the flash-friendly file system (F2FS) has been widely adopted in flash memory based storage devices, including smartphones, fast-speed servers and Internet of Things. In this paper, we propose F2DFS (deduplication-based F2FS), which introduces three main design contributions. First, F2DFS integrates inline and offline hybrid deduplication. Inline deduplication eliminates redundant writes and enhances flash device endurance, while offline deduplication mitigates the negative I/O performance impact and saves more storage space. Second, F2DFS follows the file system coupling design principle, effectively leveraging the potentials and benefits of both deduplication and native F2FS. Also, with the aid of this principle, F2DFS achieves high-performance and space-efficient incremental deduplication. Third, F2DFS adopts virtual indexing to mitigate deduplication-induced many-to-one mapping updates during the segment cleaning. We conducted comprehensive experimental comparisons between F2DFS, native F2FS, and other state-of-the-art deduplication schemes, using both synthetic and real-world workloads. For inline deduplication, F2DFS outperforms SmartDedup, Dmdedup, and ZFS, in terms of both I/O bandwidth performance and deduplication rates. And for offline deduplication, compared to SmartDedup, XFS and BtrFS, F2DFS shows higher execution efficiency, lower resource usage and greater storage space savings. Moreover, F2DFS demonstrates more efficient segment cleanings than native F2FS.},
note = {Just Accepted},
journal = {ACM Trans. Storage},
month = {apr},
keywords = {deduplication, F2FS, file system, storage system}
}

@article{10.14778/3415478.3415547,
author = {Potharaju, Rahul and Kim, Terry and Wu, Wentao and Acharya, Vidip and Suh, Steve and Fogarty, Andrew and Dave, Apoorve and Ramanujam, Sinduja and Talius, Tomas and Novik, Lev and Ramakrishnan, Raghu},
title = {Helios: hyperscale indexing for the cloud &amp; edge},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415547},
doi = {10.14778/3415478.3415547},
abstract = {Helios is a distributed, highly-scalable system used at Microsoft for flexible ingestion, indexing, and aggregation of large streams of real-time data that is designed to plug into relational engines. The system collects close to a quadrillion events indexing approximately 16 trillion search keys per day from hundreds of thousands of machines across tens of data centers around the world. Helios use cases within Microsoft include debugging/diagnostics in both public and government clouds, workload characterization, cluster health monitoring, deriving business insights and performing impact analysis of incidents in other large-scale systems such as Azure Data Lake and Cosmos. Helios also serves as a reference blueprint for other large-scale systems within Microsoft. We present the simple data model behind Helios, which offers great flexibility and control over costs, and enables the system to asynchronously index massive streams of data. We also present our experiences in building and operating Helios over the last five years at Microsoft.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3231–3244},
numpages = {14}
}

@inproceedings{10.1145/3514221.3517888,
author = {Alghamdi, Noura S. and Zhang, Liang and Rundensteiner, Elke A. and Eltabakh, Mohamed Y.},
title = {Scalable Time Series Compound Infrastructure},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517888},
doi = {10.1145/3514221.3517888},
abstract = {Objects ranging from a patient's history of medical tests to an IoT device's series of sensor maintenance records leave digital traces in the form of big time series. These time series objects do not only span exceedingly long time periods (sometimes years), but are also characterized by intermittent yet interrelated time series measurements punctuated by long gaps of silence. This prevalent data type, which we refer to as Time Series Compound objects (or, TSC), has been largely overlooked in the literature. Unique challenges arise when managing, querying and analyzing repositories of these big TSC objects. These include appropriate similarity semantics with time misalignment resiliency, efficient storage of excessively long and complex objects, and TSC-holistic indexing. We demonstrate that state-of-the-art time series systems, although effective at indexing and searching regular time series data, fail to support such big TSC data. In this work, we introduce the first comprehensive solution for managing TSC objects as first class citizen. We introduce new similarity-match semantics as well as a compact misalignment-resilient representation for TSCs. Upon this foundation, we then design a TSC-aware distributed indexing infrastructure Sloth that supports scalable storage, indexing and querying of TB-scale TSC datasets. Our experimental study demonstrates that for TB-scale datasets, the query response time of Sloth is up to one order of magnitude faster than that of existing systems, while the mean average precision (mAP) for approximate kNN similarity match query results by Sloth is 70% more accurate than existing solutions.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1685–1698},
numpages = {14},
keywords = {KNN approximate query, distributed indexing, similarity search, sloth, time series compound},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3194133.3194142,
author = {Edwards, Ross and Bencomo, Nelly},
title = {DeSiRE: further understanding nuances of degrees of satisfaction of non-functional requirements trade-off},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194142},
doi = {10.1145/3194133.3194142},
abstract = {[&lt;u&gt;Context/Motivation&lt;/u&gt;] Self-adaptive systems (SAS) are being deployed in environments of increasing uncertainty, in which they must adapt reconfiguring themselves in such a way as to continuously fulfil multiple objectives according to changes in the environment. The trade-offs between a system's non-functional requirements (NFRs) need to be done to maximise a system's utility (or equity) with regards to the NFRs, and are key drivers of the adaptation process. Decision-making for multiple objective scenarios frequently uses utility functions as measures of satisfaction of both individual and sets of NFRs, usually resulting in a weighted sum of the different objectives. [&lt;u&gt;Questions/Problems&lt;/u&gt;] However, while adaptations are performed autonomously, the methods for choosing an adaptation are based on the criteria of human expert(s), who are susceptible to bias, subjectivity and/or lack of quantitativeness in their judgements. Thus, there is a need for a non-subjective and quantitative approach to reason about NFR satisfaction in multi-objective self-adaptation without relying on human expertise. Furthermore, human biases can also apply to the relationships between two or more NFRs (e.g. how much the satisfaction of one NFR affects the satisfaction of another), resulting in emergent inaccuracies affecting the decision(s) chosen. [&lt;u&gt;Principal ideas/ results&lt;/u&gt;] This paper presents DeSiRE (Degrees of Satisfaction of NFRs), a purely automated objective statistical approach to quantifying the extent that a requirement is violated or satisfied, and its application to further explore the trade-offs between NFRs in decision making. Experiments using case studies have positive results showing the identification of a Pareto optimal set of candidate solutions, in addition to a ranking of these configurations by their satisfaction of each NFR.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {12–18},
numpages = {7},
keywords = {multi-criteria decison making, non-functional requirements, self-adaptative systems},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@inproceedings{10.1145/3493287.3493294,
author = {Kovacs, Mate and Buryakov, Daniil and Kryssanov, Victor},
title = {An Unsupervised Approach for Customer Need Assessment in E-commerce: A Case Study of Japanese Customer Reviews},
year = {2021},
isbn = {9781450389877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493287.3493294},
doi = {10.1145/3493287.3493294},
abstract = {Developing business solutions based on customer needs is a major challenge for companies. Attributes of products often have to be modified, and services and marketing strategies must be adjusted accordingly to meet customer requirements. The immense amount of feedback that companies receive on e-commerce platforms makes it difficult to obtain useful information about the needs of the customers. In this study, an unsupervised approach is developed to extract customer requirements from online product reviews and to establish a ranking among them. Experiments were conducted on a very large collection of Japanese customer reviews, and results obtained indicate that the approach proposed could be used to enhance need assessment activities in e-commerce.},
booktitle = {Proceedings of the 2021 6th International Conference on Cloud Computing and Internet of Things},
pages = {41–48},
numpages = {8},
keywords = {Customer needs, Data mining, Online reviews, Requirement discovery},
location = {Okinawa, Japan},
series = {CCIOT '21}
}

@inproceedings{10.1145/3508352.3549435,
author = {Parvathy, Aradhana Mohan and Krithivasan, Sarada and Sen, Sanchari and Raghunathan, Anand},
title = {Seprox: Sequence-Based Approximations for Compressing Ultra-Low Precision Deep Neural Networks},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549435},
doi = {10.1145/3508352.3549435},
abstract = {Compression techniques such as quantization and pruning are indispensable for deploying state-of-the-art Deep Neural Networks (DNNs) on resource-constrained edge devices. Quantization is widely used in practice - many commercial platforms already support 8-bits, with recent trends towards ultra-low precision (4-bits and below). Pruning, which increases network sparsity (incidence of zero-valued weights), enables compression by storing only the nonzero weights and their indices. Unfortunately, the compression benefits of pruning deteriorate or even vanish in ultra-low precision DNNs. This is due to (i) the unfavorable tradeoff between the number of bits needed to store a weight (which reduces with lower precision) and the number of bits needed to encode an index (which remains unchanged), and (ii) the lower sparsity levels that are achievable at lower precisions.We propose Seprox, a new compression scheme that overcomes the aforementioned challenges by exploiting two key observations about ultra-low precision DNNs. First, with lower precision, fewer weight values are possible, leading to increased incidence of frequently-occurring weights and weight sequences. Second, some weight values occur rarely and can be eliminated by replacing them with similar values. Leveraging these insights, Seprox encodes frequently-occurring weight sequences (as opposed to individual weights) while using the eliminated weight values to encode them, thereby avoiding indexing overheads and achieving higher compression. Additionally, Seprox uses approximation techniques to increase the frequencies of the encoded sequences. Across six ultra-low precision DNNs trained on the Cifar10 and ImageNet datasets, Seprox achieves model compressions, energy improvements and speed-ups of up to 35.2%, 14.8% and 18.2% respectively.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {153},
numpages = {9},
location = {San Diego, California},
series = {ICCAD '22}
}

@article{10.1145/3314575,
author = {Metwalli, Sara Ayman and Hara-Azumi, Yuko},
title = {SSA-AC: Static Significance Analysis for Approximate Computing},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/3314575},
doi = {10.1145/3314575},
abstract = {Recently, the quest to reduce energy consumption in digital systems has been the subject of a number of ongoing studies. One of the most researched focuses is approximate computing (AC). AC is a new computing paradigm in both hardware and software designs that aim to achieve energy-efficient digital systems. Although a variety of AC techniques have been studied so far, the main question, “How (in which section) can a program or a circuit be approximated?,” has not been answered yet. This work addresses the above issue by developing a software framework Static Significance Analysis for Approximate Computing (SSA-AC) to analyze the target application program and guide the designers to identify parts of the program to which approximation can or cannot be applied. SSA-AC statically analyzes the significance of variables in the precise version of the program and thus needs no trial-and-error evaluation or specific test data. Experimental results show that SSA-AC can successfully extract the significance ranking of inputs/variables to be approximated in much shorter time than existing statistical works that are inevitably data dependent.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {apr},
articleno = {34},
numpages = {17},
keywords = {Approximate computing, framework, path extraction, significance analysis}
}

@article{10.1145/3648373,
author = {Xiao, Lijun and Han, Dezhi and Li, Kuan-Ching and Khan, Muhammad Khurram},
title = {UETOPSIS: A Data-Driven Intelligence Approach to Security Decisions for Edge Computing in Smart Cities},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3648373},
doi = {10.1145/3648373},
abstract = {Despite considerable technological advances for smart cities, they still face problems such as instability of cloud server connection, insecurity during data transmission, and slight deficiencies in TCP/IP network architecture. To address such issues, we propose a data-driven intelligence approach to security decisions under Named Data Networking (NDN) architecture for edge computing, taking into consideration factors that impact device entry in smart cities, such as device performance, load, Bluetooth signal strength, and scan frequency. Despite existing techniques for Order Preference by Similarity to Ideal Solution (TOPSIS)-based on entropy weights methods are improved and applied, there exist unstable decision results. Due to this, we propose a technique for Order Preference by Similarity to Ideal Solution (TOPSIS)-based on utility function and entropy weights, named UETOPSIS, where the corresponding utility function is applied according to the influence of each attribute on the decision, ensuring the stability of the ranking of decision results. We rely on an entropy-based weights mechanism to select a suitable master controller for the design of the multi-control protocol in the smart city system, and utilize a utility function to calculate the attribute values and then combine the normalized attribute values of utility numbers, starting by analyzing the main work of the controllers. Lastly, a prototype is developed for performance evaluation purposes. Experimental evaluation and analysis show that the proposed work has better authenticity and reliability than existing works and can reduce the workload of edge computing devices when forwarding data, with stability 24.7% higher than TOPSIS, significantly improving the performance and stability of system fault tolerance and reliability in smart cities, as the second-ranked controller can efficiently take over the work when a central controller fails or damaged.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = {feb},
keywords = {Edge Computing, Data-Driven, Named Data Networking (NDN), Smart Cities, Utility Function}
}

@inproceedings{10.1145/3603781.3603932,
author = {Liu, Jian and Gou, Huanshu},
title = {Deep Multifaceted Highlight Network for Multi-objective Ranking in Trigger-Induced Recommendation},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603932},
doi = {10.1145/3603781.3603932},
abstract = {Recommender Systems have been proven to be of great business value in e-commerce platforms and recommendation algorithms play an important role in it. E-commerce platforms provide entrances for customers to enter channels that can meet their specific shopping needs. Trigger items displayed on entrance icons are useful to attract more entering. User instant interest can be explicitly induced by the trigger items and follow-up related target items are recommended accordingly, this recommendation scenario is called trigger-induced recommendation. However, existing trigger-induced recommendation algorithms usually optimize a single task (e.g., click-through rate prediction) based on users’ historical click sequences, and do not consider modeling the similarity between the trigger item and the target item. In this paper, we propose a novel recommendation method named Deep Multifaceted Highlight Network (DMHN) that can model users’ multiple types of behavior sequences simultaneously for multi-objective ranking in trigger-induced recommendation. DMHN utilizes the Multi-Interest Module to model user’s various behavior sequences to capture the different latent user interests, the Similarity Module to focus on the trigger item by modeling the similarity between the target item and the trigger item, and the Progressive Layered Extraction to optimize multiple objectives. Experiments on JD.com real production dataset demonstrate the superiority of DMHN over state-of-the-art methods.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {858–865},
numpages = {8},
keywords = {Click-Through Rate Prediction, Conversation Rate Prediction, Multi-task Learning, Recommender System, Trigger-Induced Recommendation},
location = {Xiamen, China},
series = {CNIOT '23}
}

@article{10.14778/2850469.2850472,
author = {Li, Zhenguo and Fang, Yixiang and Liu, Qin and Cheng, Jiefeng and Cheng, Reynold and Lui, John C. S.},
title = {Walking in the cloud: parallel SimRank at scale},
year = {2015},
issue_date = {September 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/2850469.2850472},
doi = {10.14778/2850469.2850472},
abstract = {Despite its popularity, SimRank is computationally costly, in both time and space. In particular, its recursive nature poses a great challenge in using modern distributed computing power, and also prevents querying similarities individually. Existing solutions suffer greatly from these practical issues. In this paper, we break such dependency for maximum efficiency possible. Our method consists of offline and online phases. In offline phase, a length-n indexing vector is derived by solving a linear system in parallel. At online query time, the similarities are computed instantly from the index vector. Throughout, the Monte Carlo method is used to maximally reduce time and space. Our algorithm, called CloudWalker, is highly parallelizable, with only linear time and space. Remarkably, it responses to both single-pair and single-source queries in constant time. CloudWalker is orders of magnitude more efficient and scalable than existing solutions for large-scale problems. Implemented on Spark with 10 machines and tested on the web-scale clue-web graph with 1 billion nodes and 43 billion edges, it takes 110 hours for offline indexing, 64 seconds for a single-pair query, and 188 seconds for a single-source query. To the best of our knowledge, our work is the first to report results on clue-web, which is 10x larger than the largest graph ever reported for SimRank computation.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {24–35},
numpages = {12}
}

@inproceedings{10.1145/3085504.3085522,
author = {Ding, Yifei and Keogh, Eamonn},
title = {Query Suggestion to allow Intuitive Interactive Search in Multidimensional Time Series},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085504.3085522},
doi = {10.1145/3085504.3085522},
abstract = {In recent years, the research community, inspired by its success in dealing with single-dimensional time series, has turned its attention to dealing with multidimensional time series. There are now a plethora of techniques for indexing, classification, and clustering of multidimensional time series. However, we argue that the difficulty of exploratory search in large multidimensional time series remains underappreciated. In essence, the problem reduces to the "chicken-and-egg" paradox that it is difficult to produce a meaningful query without knowing the best subset of dimensions to use, but finding the best subset of dimensions is itself query dependent. In this work we propose a solution to this problem. We introduce an algorithm that runs in the background, observing the user's search interactions. When appropriate, our algorithm suggests to the user a dimension that could be added or deleted to improve the user's satisfaction with the query. These query dependent suggestions may be useful to the user, even if she does not act on them (by reissuing the query), as they can hint at unexpected relationships or redundancies between the dimensions of the data. We evaluate our algorithm on several real-world datasets in medical, human activity, and industrial domains, showing that it produces subjectively sensible and objectively superior results.},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {18},
numpages = {11},
keywords = {Multidimensional Time Series, Query Suggestion},
location = {Chicago, IL, USA},
series = {SSDBM '17}
}

@article{10.1145/3390890,
author = {Huang, Zhiyi and Kang, Ning and Tang, Zhihao Gavin and Wu, Xiaowei and Zhang, Yuhao and Zhu, Xue},
title = {Fully Online Matching},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/3390890},
doi = {10.1145/3390890},
abstract = {We introduce a fully online model of maximum cardinality matching in which all vertices arrive online. On the arrival of a vertex, its incident edges to previously arrived vertices are revealed. Each vertex has a deadline that is after all its neighbors’ arrivals. If a vertex remains unmatched until its deadline, then the algorithm must irrevocably either match it to an unmatched neighbor or leave it unmatched. The model generalizes the existing one-sided online model and is motivated by applications including ride-sharing platforms, real-estate agency, and so on.We show that the Ranking algorithm by Karp et al. (STOC 1990) is 0.5211-competitive in our fully online model for general graphs. Our analysis brings a novel charging mechanic into the randomized primal dual technique by Devanur et al. (SODA 2013), allowing a vertex other than the two endpoints of a matched edge to share the gain. To our knowledge, this is the first analysis of Ranking that beats 0.5 on general graphs in an online matching problem, a first step toward solving the open problem by Karp et al. (STOC 1990) about the optimality of Ranking on general graphs. If the graph is bipartite, then we show a tight competitive ratio ≈0.5671 of Ranking. Finally, we prove that the fully online model is strictly harder than the previous model as no online algorithm can be 0.6317 &lt; 1- 1/e-competitive in our model, even for bipartite graphs.},
journal = {J. ACM},
month = {may},
articleno = {17},
numpages = {25},
keywords = {Online matching, ranking}
}

@article{10.1145/3586073,
author = {Du, Haohua and Wang, Yue and Xu, Xiaoya and Liu, Mingsheng},
title = {Niffler: Real-time Device-level Anomalies Detection in Smart Home},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/3586073},
doi = {10.1145/3586073},
abstract = {Device-level security has become a major concern in smart home systems. Detecting problems in smart home sytems strives to increase accuracy in near real time without hampering the regular tasks of the smart home. The current state of the art in detecting anomalies in smart home devices is mainly focused on the app level, which provides a basic level of security by assuming that the devices are functioning correctly. However, this approach is insufficient for ensuring the overall security of the system, as it overlooks the possibility of anomalies occurring at the lower layers such as the devices. In this article, we propose a novel notion, correlated graph, and with the aid of that, we develop our system to detect misbehaving devices without modifying the existing system. Our correlated graphs explicitly represent the contextual correlations among smart devices with little knowledge about the system. We further propose a linkage path model and a sensitivity ranking method to assist in detecting the abnormalities. We implement a semi-automatic prototype of our approach, evaluate it in real-world settings, and demonstrate its efficiency, which achieves an accuracy of around 90% in near real time.},
journal = {ACM Trans. Web},
month = {may},
articleno = {16},
numpages = {27},
keywords = {Misbehaving device detection, streaming graphs, smart homes}
}

@article{10.14778/3342263.3342648,
author = {Paparrizos, John and Franklin, Michael J.},
title = {GRAIL: efficient time-series representation learning},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342648},
doi = {10.14778/3342263.3342648},
abstract = {The analysis of time series is becoming increasingly prevalent across scientific disciplines and industrial applications. The effectiveness and the scalability of time-series mining techniques critically depend on design choices for three components responsible for (i) representing; (ii) comparing; and (iii) indexing time series. Unfortunately, these components have to date been investigated and developed independently, often resulting in mutually incompatible methods. The lack of a unified approach has hindered progress towards fast and accurate analytics over massive time-series collections. To address this major drawback, we present GRAIL, a generic framework to learn compact time-series representations that preserve the properties of a user-specified comparison function. Given the comparison function, GRAIL (i) extracts landmark time series using clustering; (ii) optimizes necessary parameters; and (iii) exploits approximations for kernel methods to construct representations in linear time and space by expressing each time series as a combination of the landmark time series. We extensively evaluate GRAIL for querying, classification, clustering, sampling, and visualization of time series. For these tasks, methods leveraging GRAIL's representations are significantly faster and at least as accurate as state-of-the-art methods operating over the raw time series. GRAIL shows promise as a new primitive for highly accurate, yet scalable, time-series analysis.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1762–1777},
numpages = {16}
}

@article{10.1109/TCBB.2022.3198284,
author = {Cai, Wentian and Xie, Linsen and Yang, Weixian and Li, Yijiang and Gao, Ying and Wang, Tingting},
title = {DFTNet: Dual-Path Feature Transfer Network for Weakly Supervised Medical Image Segmentation},
year = {2022},
issue_date = {July-Aug. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3198284},
doi = {10.1109/TCBB.2022.3198284},
abstract = {Medical image segmentation has long suffered from the problem of expensive labels. Acquiring pixel-level annotations is time-consuming, labor-intensive, and relies on extensive expert knowledge. Bounding box annotations, in contrast, are relatively easy to acquire. Thus, in this paper, we explore to segment images through a novel Dual-path Feature Transfer design with only bounding box annotations. Specifically, a Target-aware Reconstructor is proposed to extract target-related features by reconstructing the pixels within the bounding box through the channel and spatial attention module. Then, a sliding Feature Fusion and Transfer Module (FFTM) fuses the extracted features from Reconstructor and transfers them to guide the Segmentor for segmentation. Finally, we present the Confidence Ranking Loss (CRLoss) which dynamically assigns weights to the loss of each pixel based on the network's confidence. CRLoss mitigates the impact of inaccurate pseudo-labels on performance. Extensive experiments demonstrate that our proposed model achieves state-of-the-art performance on the Medical Segmentation Decathlon (MSD) Brain Tumour and PROMISE12 datasets.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {aug},
pages = {2530–2540},
numpages = {11}
}

@article{10.1145/3342357,
author = {Seidemann, Marc and Glombiewski, Nikolaus and K\"{o}rber, Michael and Seeger, Bernhard},
title = {ChronicleDB: A High-Performance Event Store},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/3342357},
doi = {10.1145/3342357},
abstract = {Reactive security monitoring, self-driving cars, the Internet of Things (IoT), and many other novel applications require systems for both writing events arriving at very high and fluctuating rates to persistent storage as well as supporting analytical ad hoc queries. As standard database systems are not capable of delivering the required write performance, log-based systems, key-value stores, and other write-optimized data stores have emerged recently. However, the drawbacks of these systems are a fair query performance and the lack of suitable instant recovery mechanisms in case of system failures.In this article, we present ChronicleDB, a novel database system with a storage layout tailored for high write performance under fluctuating data rates and powerful indexing capabilities to support a variety of queries. In addition, ChronicleDB offers low-cost fault tolerance and instant recovery within milliseconds. Unlike previous work, ChronicleDB is designed either as a serverless library to be tightly integrated in an application or as a standalone database server. Our results of an experimental evaluation with real and synthetic data reveal that ChronicleDB clearly outperforms competing systems with respect to both write and query performance.},
journal = {ACM Trans. Database Syst.},
month = {oct},
articleno = {13},
numpages = {45},
keywords = {Event processing, aggregation queries, indexing, recovery, storage layout, time travel queries}
}

@inproceedings{10.1145/3274895.3274949,
author = {Chatzigeorgakidis, Georgios and Patroumpas, Kostas and Skoutas, Dimitrios and Athanasiou, Spiros and Skiadopoulos, Spiros},
title = {Scalable hybrid similarity join over geolocated time series},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274949},
doi = {10.1145/3274895.3274949},
abstract = {A geolocated time series is a sequence of values associated with a geolocation, such as measurements provided by a sensor installed at a certain location. In this paper, we address the problem of hybrid similarity joins over such geolocated time series. This operation returns all pairs of geolocated time series that exhibit similar behavior in the time series domain while also being closely located in space. First, we propose algorithms for performing such join operations using different types of indices, including spatial-only, time series-only, and hybrid indices. Such centralized indexing schemes can cope well with moderate data volumes but they face scalability issues when the dataset size increases significantly. To overcome this problem, we present a MapReduce-based processing scheme with space-driven partitioning. Our parallel and distributed algorithm leverages our hybrid index for geolocated time series to efficiently execute similarity joins locally within each partition and minimize the amount of data that needs to be shuffled between processing nodes. An extensive experimental evaluation confirms that our approach can efficiently compute all matching pairs even for datasets containing millions of geolocated time series.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {119–128},
numpages = {10},
keywords = {big data, similarity joins, spatial indices, time series},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1145/3320435.3320457,
author = {Musto, Cataldo and Lops, Pasquale and de Gemmis, Marco and Semeraro, Giovanni},
title = {Justifying Recommendations through Aspect-based Sentiment Analysis of Users Reviews},
year = {2019},
isbn = {9781450360210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320435.3320457},
doi = {10.1145/3320435.3320457},
abstract = {In this paper we present a methodology to justify the suggestions generated by a recommendation algorithm through the identification of relevant and distinguishing characteristics of the recommended item, automatically extracted by mining users' reviews. Our approach relies on a combination ofnatural language processing and sentiment analysis techniques, and is based on the following steps: (1) a set of users' reviews discussing the recommended item is gathered and analyzed; (2) the distinguishing aspects that characterize the item are extracted and a ranking function is used to identify the most relevant ones; (3) excerpts of the reviews discussing such aspects are extracted and a natural language template is filled in through the aggregation of these sentences. This represents the final output of the algorithm, which is provided to the user as justification of the recommendation she received. In the experimental evaluation, we carried out a user study (N=296, 73.6% male) aiming to investigate the effectiveness of our methodology in two different domains, as movies and books. Results showed that our technique can provide users with rich and satisfying justifications. Moreover, our experiment also showed that the users prefer review-based justifications to other explanation strategies, and this finding further confirmed the effectiveness of the approach.},
booktitle = {Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {4–12},
numpages = {9},
keywords = {explanation, natural language processing, opinion mining, recommender systems, sentiment analysis, user study},
location = {Larnaca, Cyprus},
series = {UMAP '19}
}

@article{10.1109/TASLP.2022.3192664,
author = {Mill\'{a}n-Castillo, Roberto San and Martino, Luca and Morgado, Eduardo and Llorente, Fernando},
title = {An Exhaustive Variable Selection Study for Linear Models of Soundscape Emotions: Rankings and Gibbs Analysis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3192664},
doi = {10.1109/TASLP.2022.3192664},
abstract = {In the last decade, soundscapes have become one of the most active topics in Acoustics, providing a holistic approach to the acoustic environment, which involves human perception and context. Soundscapes-elicited emotions are central and substantially subtle and unnoticed (compared to speech or music). Currently, soundscape emotion recognition is a very active topic in the literature. We provide an exhaustive variable selection study (i.e., a selection of the soundscapes indicators) to a well-known dataset (emo-soundscapes). We consider linear soundscape emotion models for two soundscapes descriptors: arousal and valence. Several ranking schemes and procedures for selecting the number of variables are applied. We have also performed an alternating optimization scheme for obtaining the best sequences keeping fixed a certain number of features. Furthermore, we have designed a novel technique based on Gibbs sampling, which provides a more complete and clear view of the relevance of each variable. Finally, we have also compared our results with the analysis obtained by the classical methods based on p-values. As a result of our study, we suggest two simple and parsimonious linear models of only 7 and 16 variables (within the 122 possible features) for the two outputs (arousal and valence), respectively. The suggested linear models provide very good and competitive performance, with &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$R^{2}&gt;0.86$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$R^{2}&gt;0.63$&lt;/tex-math&gt;&lt;/inline-formula&gt; (values obtained after a cross-validation procedure), respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2460–2474},
numpages = {15}
}

@article{10.1145/3610885,
author = {Wang, Juexing and Wang, Guangjing and Zhang, Xiao and Liu, Li and Zeng, Huacheng and Xiao, Li and Cao, Zhichao and Gu, Lin and Li, Tianxing},
title = {PATCH: A Plug-in Framework of Non-blocking Inference for Distributed Multimodal System},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610885},
doi = {10.1145/3610885},
abstract = {Recent advancements in deep learning have shown that multimodal inference can be particularly useful in tasks like autonomous driving, human health, and production line monitoring. However, deploying state-of-the-art multimodal models in distributed IoT systems poses unique challenges since the sensor data from low-cost edge devices can get corrupted, lost, or delayed before reaching the cloud. These problems are magnified in the presence of asymmetric data generation rates from different sensor modalities, wireless network dynamics, or unpredictable sensor behavior, leading to either increased latency or degradation in inference accuracy, which could affect the normal operation of the system with severe consequences like human injury or car accident. In this paper, we propose PATCH, a framework of speculative inference to adapt to these complex scenarios. PATCH serves as a plug-in module in the existing multimodal models, and it enables speculative inference of these off-the-shelf deep learning models. PATCH consists of 1) a Masked-AutoEncoder-based cross-modality imputation module to impute missing data using partially-available sensor data, 2) a lightweight feature pair ranking module that effectively limits the searching space for the optimal imputation configuration with low computation overhead, and 3) a data alignment module that aligns multimodal heterogeneous data streams without using accurate timestamp or external synchronization mechanisms. We implement PATCH in nine popular multimodal models using five public datasets and one self-collected dataset. The experimental results show that PATCH achieves up to 13% mean accuracy improvement over the state-of-art method while only using 10% of training data and reducing the training overhead by 73% compared to the original cost of retraining the model.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {130},
numpages = {24},
keywords = {Multi-task Learning, Multimodal Learning, Neural Networks, Non-blocking Inference}
}

@inproceedings{10.1145/3400903.3400927,
author = {Arseneau, Yoann and Gautam, Saransh and Nickerson, Bradford and Ray, Suprio},
title = {STILT: Unifying Spatial, Temporal and Textual Search using a Generalized Multi-dimensional Index},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3400927},
doi = {10.1145/3400903.3400927},
abstract = {The proliferation of location-enabled sensors, smart phones, and the power of digital messaging combined with social media platforms is producing a deluge of multi-dimensional data. Novel index structures are needed to efficiently process massive amounts of geo-tagged data, and to promptly answer queries with textual, spatial, and temporal components. Existing approaches to spatio-temporal text indexing lack a unified index supporting efficient range and top-k search on any combination of location, time, or text. We introduce a generalized, multi-dimensional index called Spatio-temporal Textual InterLeaved Trie (STILT), which unifies spatial, textual, and temporal components within a single structure. STILT is a general-purpose index supporting subset searches (e.g. spatio-textual, spatio-temporal, spatial, textual), as well as full spatio-temporal textual searches. STILT uses a binary trie-based index interleaving text, location, and time information in a space-efficient manner. STILT supports parallel building of the index and concurrent execution of spatio-temporal textual queries, including top-k and range search queries. With extensive evaluation, we demonstrate that STILT is significantly faster than state-of-the-art approaches in terms of index construction time and search latency.},
booktitle = {Proceedings of the 32nd International Conference on Scientific and Statistical Database Management},
articleno = {11},
numpages = {12},
keywords = {multi-dimensional index, spatial, spatio-temporal, spatio-textual},
location = {Vienna, Austria},
series = {SSDBM '20}
}

@article{10.14778/3436905.3436926,
author = {Jiang, Lin and Qiu, Junqiao and Zhao, Zhijia},
title = {Scalable structural index construction for JSON analytics},
year = {2020},
issue_date = {December 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3436905.3436926},
doi = {10.14778/3436905.3436926},
abstract = {JavaScript Object Notation (JSON) and its variants have gained great popularity in recent years. Unfortunately, the performance of their analytics is often dragged down by the expensive JSON parsing. To address this, recent work has shown that building bitwise indices on JSON data, called structural indices, can greatly accelerate querying. Despite its promise, the existing structural index construction does not scale well as records become larger and more complex, due to its (inherently) sequential construction process and the involvement of costly memory copies that grow as the nesting level increases.To address the above issues, this work introduces Pison - a more memory-efficient structural index constructor with supports of intra-record parallelism. First, Pison features a redesign of the bottleneck step in the existing solution. The new design is not only simpler but more memory-efficient. More importantly, Pison is able to build structural indices for a single bulky record in parallel, enabled by a group of customized parallelization techniques. Finally, Pison is also optimized for better data locality, which is especially critical in the scenario of bulky record processing. Our evaluation using real-world JSON datasets shows that Pison achieves 9.8X speedup (on average) over the existing structural index construction solution for bulky records and 4.6X speedup (on average) of end-to-end performance (indexing plus querying) over a state-of-the-art SIMD-based JSON parser on a 16-core machine.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {694–707},
numpages = {14}
}

@article{10.1145/2786758,
author = {That, Dai Hai Ton and Popa, Iulian Sandu and Zeitouni, Karine},
title = {TRIFL: A Generic Trajectory Index for Flash Storage},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2374-0353},
url = {https://doi.org/10.1145/2786758},
doi = {10.1145/2786758},
abstract = {Due to several important features, such as high performance, low power consumption, and shock resistance, NAND flash has become a very popular stable storage medium for embedded mobile devices, personal computers, and even enterprise servers. However, the peculiar characteristics of flash memory require redesigning the existing data storage and indexing techniques that were devised for magnetic hard disks.In this article, we propose TRIFL, an efficient and generic TRajectory Index for FLash. TRIFL is designed around the key requirements of trajectory indexing and flash storage. TRIFL is generic in the sense that it is efficient for both simple flash storage devices such as SD cards and more powerful devices such as solid state drives. In addition, TRIFL is supplied with an online self-tuning algorithm that allows adapting the index structure to the workload and the technical specifications of the flash storage device to maximize the index performance. Moreover, TRIFL achieves good performance with relatively low memory requirements, which makes the index appropriate for many application scenarios. The experimental evaluation shows that TRIFL outperforms the representative indexing methods on magnetic disks and flash disks.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = {jul},
articleno = {6},
numpages = {44},
keywords = {Flash memory, partitioning, range queries, trajectory indexing}
}

@article{10.14778/2733004.2733021,
author = {Liu, Yue and Hu, Songlin and Rabl, Tilmann and Liu, Wantao and Jacobsen, Hans-Arno and Wu, Kaifeng and Chen, Jian and Li, Jintao},
title = {DGFIndex for smart grid: enhancing hive with a cost-effective multidimensional range index},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733021},
doi = {10.14778/2733004.2733021},
abstract = {In Smart Grid applications, as the number of deployed electric smart meters increases, massive amounts of valuable meter data is generated and collected every day. To enable reliable data collection and make business decisions fast, high throughput storage and high-performance analysis of massive meter data become crucial for grid companies. Considering the advantage of high efficiency, fault tolerance, and price-performance of Hadoop and Hive systems, they are frequently deployed as underlying platform for big data processing. However, in real business use cases, these data analysis applications typically involve multidimensional range queries (MDRQ) as well as batch reading and statistics on the meter data. While Hive is high-performance at complex data batch reading and analysis, it lacks efficient indexing techniques for MDRQ.In this paper, we propose DGFIndex, an index structure for Hive that efficiently supports MDRQ for massive meter data. DGFIndex divides the data space into cubes using the grid file technique. Unlike the existing indexes in Hive, which stores all combinations of multiple dimensions, DGFIndex only stores the information of cubes. This leads to smaller index size and faster query processing. Furthermore, with pre-computing user-defined aggregations of each cube, DGFIndex only needs to access the boundary region for aggregation query. Our comprehensive experiments show that DGFIndex can save significant disk space in comparison with the existing indexes in Hive and the query performance with DGFIndex is 2-50 times faster than existing indexes in Hive and HadoopDB for aggregation query, 2-5 times faster than both for non-aggregation query, 2-75 times faster than scanning the whole table in different query selectivity.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1496–1507},
numpages = {12}
}

@article{10.1145/3507911,
author = {Pavlopoulou, Niki and Curry, Edward},
title = {PoSSUM: An Entity-centric Publish/Subscribe System for Diverse Summarization in Internet of Things},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3507911},
doi = {10.1145/3507911},
abstract = {Users are interested in entity information provided by multiple sensors in the Internet of Things. The challenges regarding this environment span from data-centric ones due to data integration, heterogeneity, and enrichment, to user-centric ones due to the need for high-level data interpretation and usability for non-expert users, to system-centric ones due to resource constraints. Publish/Subscribe systems (PSSs) are suitable schemes for large-scale applications, but they are limited in dealing with the data and user challenges. In this article, we propose PoSSUM, a novel entity-centric PSS that provides entity summaries for user-friendly subscriptions through data integration, a novel Density-Based VARiance Clustering (DBVARC) for diverse entity summarization that is parameter-free and partly incremental, reasoning rules, and a novel Triple2Rank scoring for top-k filtering based on importance, informativeness, and diversity. We introduce a novel evaluation methodology that creates ground truths and metrics that capture the quality of entity summaries. We compare our approach with a previous dynamic approach and a static diverse entity summarization approach that we adapted to dynamic environments. The evaluation results for two use cases, Healthcare and Smart Cities, show that when users are provided with less information, their data diversity desire could reach up to 80%. Summarization approaches achieve from 80% to 99% message reduction, with PoSSUM having the best-ranking quality for more than half of the entities by a significant margin. PoSSUM has the highest conceptual clustering F-score, ranging from 0.69 to 0.83, and a redundancy-aware F-score up to 0.95, with cases, where it is almost two times better than the other approaches. PoSSUM takes 50% or less clustering processing time and it performs scoring significantly faster for larger windows. It also has comparable end-to-end latency and throughput values, and it occupies a third of the memory compared to the second-best approach.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {73},
numpages = {30},
keywords = {Diverse entity summarization, triple ranking, Top-k filtering, Internet of Things}
}

@inproceedings{10.1145/3433210.3437533,
author = {Ji, Yuede and Cui, Lei and Huang, H. Howie},
title = {BugGraph: Differentiating Source-Binary Code Similarity with Graph Triplet-Loss Network},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3437533},
doi = {10.1145/3433210.3437533},
abstract = {Binary code similarity detection, which answers whether two pieces of binary code are similar, has been used in a number of applications,such as vulnerability detection and automatic patching. Existing approaches face two hurdles in their efforts to achieve high accuracy and coverage: (1) the problem of source-binary code similarity detection, where the target code to be analyzed is in the binary format while the comparing code (with ground truth) is in source code format. Meanwhile, the source code is compiled to the comparing binary code with either a random or fixed configuration (e.g.,architecture, compiler family, compiler version, and optimization level), which significantly increases the difficulty of code similarity detection; and (2) the existence of different degrees of code similarity. Less similar code is known to be more, if not equally, important in various applications such as binary vulnerability study. To address these challenges, we design BugGraph, which performs source-binary code similarity detection in two steps. First, BugGraph identifies the compilation provenance of the target binary and compiles the comparing source code to a binary with the same provenance.Second, BugGraph utilizes a new graph triplet-loss network on the attributed control flow graph to produce a similarity ranking. The experiments on four real-world datasets show that BugGraph achieves 90% and 75% true positive rate for syntax equivalent and similar code, respectively, an improvement of 16% and 24% overstate-of-the-art methods. Moreover, BugGraph is able to identify 140 vulnerabilities in six commercial firmware.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {702–715},
numpages = {14},
keywords = {binary code, code similarity, graph embedding, vulnerability},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@article{10.1109/TNET.2016.2549017,
author = {Cheng, Long and Niu, Jianwei and Gu, Yu and Luo, Chengwen and He, Tian},
title = {Achieving Efficient Reliable Flooding in Low-Duty-Cycle Wireless Sensor Networks},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2549017},
doi = {10.1109/TNET.2016.2549017},
abstract = {Reliable flooding in wireless sensor networks WSNs is desirable for a broad range of applications and network operations. However, relatively little work has been done for reliable flooding in low-duty-cycle WSNs with unreliable wireless links. It is a challenging problem to efficiently ensure 100% flooding coverage considering the combined effects of low-duty-cycle operation and unreliable wireless transmission. In this paper, we propose a novel dynamic switching-based reliable flooding DSRF framework, which is designed as an enhancement layer to provide efficient and reliable delivery for a variety of existing flooding tree structures in low-duty-cycle WSNs. The key novelty of DSRF lies in the dynamic switching decision making when encountering a transmission failure, where a flooding tree structure is dynamically adjusted based on the packet reception results for energy saving and delay reduction. DSRF distinguishes itself from the existing works in that it explores both poor links and good links on demand. In addition, we define the optimal wakeup schedule-ranking problem in order to maximize the switching gain in DSRF. We prove the NP-completeness of this problem and present a heuristic algorithm with a low computational complexity. Through comprehensive performance comparisons, including the simulation of large-scale scenarios and small-scale experiments on a WSN testbed, we demonstrate that compared with the flooding protocol without DSRF enhancement, the DSRF effectively reduces the flooding delay and the total number of packet transmission by  $12%{sim }25%$  and  $10%{sim }15%$ , respectively. Remarkably, the achieved performance is close to the theoretical lower bound.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {3676–3689},
numpages = {14}
}

@article{10.1145/3656348,
author = {Casella, Enrico and Silvestri, Simone and Baker, D. A. and Das, Sajal K.},
title = {A Human-Centered Power Conservation Framework based on Reverse Auction Theory and Machine Learning},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2378-962X},
url = {https://doi.org/10.1145/3656348},
doi = {10.1145/3656348},
abstract = {Extreme outside temperatures resulting from heat waves, winter storms, and similar weather-related events trigger the Heating Ventilation and Air Conditioning (HVAC) systems, resulting in challenging, and potentially catastrophic, peak loads. As a consequence, such extreme outside temperatures put a strain on power grids and may thus lead to blackouts. In order to avoid the financial and personal repercussions of peak loads, demand response and power conservation represent promising solutions. Despite numerous efforts, it has been shown that the current state-of-the-art fails to consider: i) the complexity of human behavior when interacting with power conservation systems; and ii) realistic home-level power dynamics. As a consequence, this leads to approaches that are i) ineffective due to poor long-term user engagement; and ii) too abstract to be used in real-world settings. In this paper, we propose an auction-theory-based power conservation framework for HVAC designed to address such individual human component through a three-fold approach: personalized preferences of power conservation, models of realistic user behavior, and realistic home-level power dynamics. In our framework, the System Operator (SO) sends Load Serving Entities (LSEs) the required power saving to tackle peak loads at the residential distribution feeder. Each LSE then prompts its users to provide bids, i.e., personalized preferences of thermostat temperature adjustments, along with corresponding financial compensations. We employ models of realistic user behavior by means of online surveys to gather user bids and evaluate user interaction with such system. Realistic home-level power dynamics are implemented by our machine-learning-based Power Saving Predictions (PSP) algorithm, calculating the individual power savings in each user’s home resulting from such bids. A machine-learning-based Power Saving Predictions (PSP) algorithm is executed by the users’ Smart Energy Management System (SEMS). PSP translates temperature adjustments into the corresponding power savings. Then, the SEMS sends bids back to the LSE, which selects the auction winners through an optimization problem called POwer Conservation Optimization (POCO). We prove that POCO is NP-hard, and thus provide two approaches to solve this problem. One approach is an optimal pseudo-polynomial algorithm called DYnamic programming Power Saving (DYPS), while the second is a heuristic polynomial-time algorithm called Greedy Ranking Allocation (GRAN). EnergyPlus, the high-fidelity and gold-standard energy simulator funded by the U.S. Department of Energy, was used to validate our experiments, as well as to collect data to train PSP. We further evaluate the results of the auctions across several scenarios, showing that, as expected, DYPS finds the optimal solution, while GRAN outperforms recent state-of-the-art approaches.},
note = {Just Accepted},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {apr},
keywords = {HVAC power conservation, machine learning power saving predictions, reverse auctions, cyber-physical pervasive computing, human-centered cyber-physical systems}
}

@inproceedings{10.1145/75334.75355,
author = {Maarek, Y. S. and Smadja, F. Z.},
title = {Full text indexing based on lexical relations an application: software libraries},
year = {1989},
isbn = {0897913213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/75334.75355},
doi = {10.1145/75334.75355},
abstract = {In contrast to other kinds of libraries, software libraries need to be conceptually organized. When looking for a component, the main concern of users is the functionality of the desired component; implementation details are secondary. Software reuse would be enhanced with conceptually organized large libraries of software components. In this paper, we present GURU, a tool that allows automatical building of such large software libraries from documented software components. We focus here on GURU's indexing component which extracts conceptual attributes from natural language documentation. This indexing method is based on words' co-occurrences. It first uses EXTRACT, a co-occurrence knowledge compiler for extracting potential attributes from textual documents. Conceptually relevant collocations are then selected according to their resolving power, which scales down the noise due to context words. This fully automated indexing tool thus goes further than keyword-based tools in the understanding of a document without the brittleness of knowledge based tools. The indexing component of GURU is fully implemented, and some results are given in the paper.},
booktitle = {Proceedings of the 12th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {198–206},
numpages = {9},
location = {Cambridge, Massachusetts, USA},
series = {SIGIR '89}
}

@inproceedings{10.1145/3583780.3615109,
author = {Wang, Tianle and Wang, Zihan and Liu, Weitang and Shang, Jingbo},
title = {WOT-Class: Weakly Supervised Open-world Text Classification},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615109},
doi = {10.1145/3583780.3615109},
abstract = {State-of-the-art weakly supervised text classification methods, while significantly reduced the required human supervision, still requires the supervision to cover all the classes of interest. This is never easy to meet in practice when human explore new, large corpora without complete pictures. In this paper, we work on a novel yet important problem of weakly supervised open-world text classification, where supervision is only needed for a few examples from a few known classes and the machine should handle both known and unknown classes in test time. General open-world classification has been studied mostly using image classification; however, existing methods typically assume the availability of sufficient known-class supervision and strong unknown-class prior knowledge (e.g., the number and/or data distribution). We propose a novel framework \o{}ur that lifts those strong assumptions. Specifically, it follows an iterative process of (a) clustering text to new classes, (b) mining and ranking indicative words for each class, and (c) merging redundant classes by using the overlapped indicative words as a bridge. Extensive experiments on 7 popular text classification datasets demonstrate that \o{}ur outperforms strong baselines consistently with a large margin, attaining 23.33% greater average absolute macro-F1 over existing approaches across all datasets. Such competent accuracy illuminates the practical potential of further reducing human effort for text classification.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2666–2675},
numpages = {10},
keywords = {open-world learning, text classification, weak supervision},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.5555/1287369.1287453,
author = {Ilyas, Ihab F. and Aref, Walid G. and Elmagarmid, Ahmed K.},
title = {Joining ranked inputs in practice},
year = {2002},
publisher = {VLDB Endowment},
abstract = {Joining ranked inputs is an essential requirement for many database applications, such as ranking search results from multiple search engines and answering multi-feature queries for multimedia retrieval systems. We introduce a new practical pipelined query operator, termed NRA-RJ, that produces a global rank from input ranked streams based on a score function. The output of NRA-RJ can serve as a valid input to other NRA-RJ operators in the query pipeline. Hence, the NRA-RJ operator can support a hierarchy of join operations and can be easily integrated in query processing engines of commercial database systems. The NRA-RJ operator bridges Fagin's optimal aggregation algorithm into a practical implementation and contains several optimizations that address performance issues. We compare the performance of NRA-RJ against recent rank join algorithms. Experimental results demonstrate the performance trade-offs among these algorithms. The experimental results are based on an empirical study applied to a medical video application on top of a prototype database system. The study reveals important design options and shows that the NRA-RJ operator outperforms other pipelined rank join operators when the join condition is an equi-join on key attributes.},
booktitle = {Proceedings of the 28th International Conference on Very Large Data Bases},
pages = {950–961},
numpages = {12},
location = {Hong Kong, China},
series = {VLDB '02}
}

@inproceedings{10.1145/3426020.3426083,
author = {Choi, In Hyeok and Kim, Yang Sok and Lee, Choong Kwon},
title = {A Study of the Classification of IT Jobs Using LSTM and LIME},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426083},
doi = {10.1145/3426020.3426083},
abstract = {This study aims to suggest a new approach that finds important job skill terms using deep learning and eXplainable Artificial Intelligence (XAI) algorithms. A total of 52,190 job advertisements were collected from a job posting website using web crawling technique. The job advertisements were classified into specific job roles using Deep Learning-based Bidirectional LSTM(Long Short Term Memory) and Bidirectional LSTM Attention. Finally, the best performing Bidirectional LSTM Attention model was used to extract important terms from the selected job advertisements by using Local Interpretable Model-agnostic Explanations (LIME), one of the XAI techniques, and compared them with those selected by term frequency. The results show that these two sets are significantly different in some cases, even when one set is more reasonable compared to other set and vice versa. Although this research cannot conclude the LIME is better than the frequency-based approach for identifying important skills, at least we found that LIME could guide researchers to a new path for this task.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {248–252},
numpages = {5},
keywords = {LIME, LSTM, Text Mining, XAI},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@article{10.1145/3604611,
author = {Yang, Shouguo and Dong, Chaopeng and Xiao, Yang and Cheng, Yiran and Shi, Zhiqiang and Li, Zhi and Sun, Limin},
title = {Asteria-Pro: Enhancing Deep Learning-based Binary Code Similarity Detection by Incorporating Domain Knowledge},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3604611},
doi = {10.1145/3604611},
abstract = {Widespread code reuse allows vulnerabilities to proliferate among a vast variety of firmware. There is an urgent need to detect these vulnerable codes effectively and efficiently. By measuring code similarities, AI-based binary code similarity detection is applied to detecting vulnerable code at scale. Existing studies have proposed various function features to capture the commonality for similarity detection. Nevertheless, the significant code syntactic variability induced by the diversity of IoT hardware architectures diminishes the accuracy of binary code similarity detection. In our earlier study and the tool Asteria, we adopted a Tree-LSTM network to summarize function semantics as function commonality, and the evaluation result indicates an advanced performance. However, it still has utility concerns due to excessive time costs and inadequate precision while searching for large-scale firmware bugs.To this end, we propose a novel deep learning-enhancement architecture by incorporating domain knowledge-based pre-filtration and re-ranking modules, and we develop a prototype named Asteria-Pro based on Asteria. The pre-filtration module eliminates dissimilar functions, thus reducing the subsequent deep learning-model calculations. The re-ranking module boosts the rankings of vulnerable functions among candidates generated by the deep learning model. Our evaluation indicates that the pre-filtration module cuts the calculation time by 96.9%, and the re-ranking module improves MRR and Recall by 23.71% and 36.4%, respectively. By incorporating these modules, Asteria-Pro outperforms existing state-of-the-art approaches in the bug search task by a significant margin. Furthermore, our evaluation shows that embedding baseline methods with pre-filtration and re-ranking modules significantly improves their precision. We conduct a large-scale real-world firmware bug search, and Asteria-Pro manages to detect 1,482 vulnerable functions with a high precision 91.65%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {1},
numpages = {40},
keywords = {Binary code similarity detection, pre-fitering, re-ranking, abstract syntactic tree, graph neural network}
}

@inproceedings{10.1145/2837185.2837261,
author = {Qiu, Robin G. and Ravi, Ramya R. and Qiu, Lawrence L.},
title = {Aggregating and visualizing public opinions and sentiment trends on the US higher education},
year = {2015},
isbn = {9781450334914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837185.2837261},
doi = {10.1145/2837185.2837261},
abstract = {Through leveraging big data technologies and a quantitative and model-driven approach, we previously built an education monitoring and ranking system (eduMRS) that allowed us to change ranking factors that are used to assess quality of higher education. Taking a systems perspective, this short paper introduces an enhanced eduMRS -- II that can effectively aggregate and visualize public opinions and sentiment trends on higher education services. Public opinions can be selectively retrieved from online media including tweets. This enhanced service system demonstrates that a big data based, real time, and interactive computer application possesses promising potential of facilitating decision-making of addressing the needs of customers in the service industry.},
booktitle = {Proceedings of the 17th International Conference on Information Integration and Web-Based Applications &amp; Services},
articleno = {33},
numpages = {5},
keywords = {big data, higher education, information system design, prototyping, ranking, ranking system, sentiment analysis, visualization},
location = {Brussels, Belgium},
series = {iiWAS '15}
}

@inproceedings{10.1145/2413176.2413213,
author = {Rahman, Md Sazzadur and Huang, Ting-Kai and Madhyastha, Harsha V. and Faloutsos, Michalis},
title = {FRAppE: detecting malicious facebook applications},
year = {2012},
isbn = {9781450317757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2413176.2413213},
doi = {10.1145/2413176.2413213},
abstract = {With 20 million installs a day, third-party apps are a major reason for the popularity and addictiveness of Facebook. Unfortunately, hackers have realized the potential of using apps for spreading malware and spam. The problem is already significant, as we find that at least 13% of apps in our dataset are malicious. So far, the research community has focused on detecting malicious posts and campaigns.In this paper, we ask the question: given a Facebook application, can we determine if it is malicious? Our key contribution is in developing FRAppE---Facebook's Rigorous Application Evaluator---arguably the first tool focused on detecting malicious apps on Facebook. To develop FRAppE, we use information gathered by observing the posting behavior of 111K Facebook apps seen across 2.2 million users on Facebook. First, we identify a set of features that help us distinguish malicious apps from benign ones. For example, we find that malicious apps often share names with other apps, and they typically request fewer permissions than benign apps.Second, leveraging these distinguishing features, we show that FRAppE can detect malicious apps with 99.5% accuracy, with no false positives and a low false negative rate (4.1%). Finally, we explore the ecosystem of malicious Facebook apps and identify mechanisms that these apps use to propagate. Interestingly, we find that many apps collude and support each other; in our dataset, we find 1,584 apps enabling the viral propagation of 3,723 other apps through their posts. Long-term, we see FRAppE as a step towards creating an independent watchdog for app assessment and ranking, so as to warn Facebook users before installing apps.},
booktitle = {Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies},
pages = {313–324},
numpages = {12},
keywords = {facebook apps, online social networks, osn, security},
location = {Nice, France},
series = {CoNEXT '12}
}

@article{10.14778/3570690.3570704,
author = {Ma, Chaohong and Yu, Xiaohui and Li, Yifan and Meng, Xiaofeng and Maoliniyazi, Aishan},
title = {FILM: A Fully Learned Index for Larger-Than-Memory Databases},
year = {2022},
issue_date = {November 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3570690.3570704},
doi = {10.14778/3570690.3570704},
abstract = {As modern applications generate data at an unprecedented speed and often require the querying/analysis of data spanning a large duration, it is crucial to develop indexing techniques that cater to larger-than-memory databases, where data reside on heterogeneous storage devices (such as memory and disk), and support fast data insertion and query processing. In this paper, we propose FILM, a Fully learned Index for Larger-than-Memory databases. FILM is a learned tree structure that uses simple approximation models to index data spanning different storage devices. Compared with existing techniques for larger-than-memory databases, such as anti-caching, FILM allows for more efficient query processing at significantly lower main-memory overhead. FILM is also designed to effectively address one of the bottlenecks in existing methods for indexing larger-than-memory databases that is caused by data swapping between memory and disk. More specifically, updating the LRU (for Least Recently Used) structure employed by existing methods for cold data identification (determining the data to be evicted to disk when the available memory runs out) often incurs significant delay to query processing. FILM takes a drastically different approach by proposing an adaptive LRU structure and piggybacking its update onto query processing with minimal overhead. We thoroughly study the performance of FILM and its components on a variety of datasets and workloads, and the experimental results demonstrate its superiority in improving query processing performance and reducing index storage overhead (by orders of magnitudes) compared with applicable baselines.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {561–573},
numpages = {13}
}

@article{10.1109/TNET.2014.2385831,
author = {Rahman, Sazzadur and Huang, Ting-Kai and Madhyastha, Harsha V. and Faloutsos, Michalis},
title = {Detecting malicious facebook applications},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2385831},
doi = {10.1109/TNET.2014.2385831},
abstract = {With 20 million installs a day [1], third-party apps are a major reason for the popularity and addictiveness of Facebook. Unfortunately, hackers have realized the potential of using apps for spreading malware and spam. The problem is already significant, as we find that at least 13% of apps in our dataset are malicious. So far, the research community has focused on detecting malicious posts and campaigns. In this paper, we ask the question: Given a Facebook application, can we determine if it is malicious? Our key contribution is in developing FRAppE---Facebook's Rigorous Application Evaluator---arguably the first tool focused on detecting malicious apps on Facebook. To develop FRAppE, we use information gathered by observing the posting behavior of 111K Facebook apps seen across 2.2 million users on Facebook. First, we identify a set of features that help us distinguish malicious apps from benign ones. For example, we find that malicious apps often share names with other apps, and they typically request fewer permissions than benign apps. Second, leveraging these distinguishing features, we show that FRAppE can detect malicious apps with 99.5% accuracy, with no false positives and a high true positive rate (95.9%). Finally, we explore the ecosystem of malicious Facebook apps and identify mechanisms that these apps use to propagate. Interestingly, we find that many apps collude and support each other; in our dataset, we find 1584 apps enabling the viral propagation of 3723 other apps through their posts. Long term, we see FRAppE as a step toward creating an independent watchdog for app assessment and ranking, so as to warn Facebook users before installing apps.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {773–787},
numpages = {15},
keywords = {Facebook apps, malicious, online social networks, spam}
}

@inproceedings{10.1145/3514221.3517840,
author = {Hu, Bo and Guo, Peizhen and Hu, Wenjun},
title = {Video-zilla: An Indexing Layer for Large-Scale Video Analytics},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517840},
doi = {10.1145/3514221.3517840},
abstract = {Pervasive deployment of surveillance cameras today poses enormous scalability challenges to video analytics systems operating over many camera feeds. Currently, there are few indexing tools to organize video feeds beyond what is provided by a standard file system. Recent video analytic systems implement application-specific frame profiling and sampling techniques to reduce the number of raw videos processed, leveraging frame-level redundancy or manually labeled spatial-temporal correlation between cameras.This paper presents Video-zilla, a standalone indexing layer between video query systems and a video store to organize video data. We propose a video data unit abstraction, semantic video stream (SVS), based on a notion of distance between objects in the video. SVS implicitly captures scenes, which is missing from current video content characterization and a middle ground between individual frames and an entire camera feed. We then build a hierarchical index that exposes the semantic similarity both within and across camera feeds, such that Video-zilla can quickly cluster video feeds based on their content semantics without manual labeling. We implement and evaluate Video-zilla in three use cases: object identification queries, clustering for training specialized DNNs, and archival services. In all three cases, Video-zilla reduces the time complexity of inter-camera video analytics from linear with the number of cameras to sublinear, and reduces query resource usage by up to 14\texttimes{} compared to using frame-level or spatial-temporal similarity built into existing query systems.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1905–1919},
numpages = {15},
keywords = {edge computing, video analytics},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3524053.3542741,
author = {Song, Weijia and Yang, Yuting and Liu, Thompson and Merlina, Andrea and Garrett, Thiago and Vitenberg, Roman and Rosa, Lorenzo and Awatramani, Aahil and Wang, Zheng and Birman, Ken},
title = {Cascade: An Edge Computing Platform for Real-time Machine Intelligence},
year = {2022},
isbn = {9781450392808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524053.3542741},
doi = {10.1145/3524053.3542741},
abstract = {Intelligent IoT is a prerequisite for societal priorities such as a smart power grid, smart urban infrastructures and smart highways. These applications bring requirements such as real-time guarantees, data and action consistency, fault-tolerance, high availability, temporal data indexing, scalability, and even self-organization and self-stabilization. Existing platforms are oriented towards asynchronous, out of band upload of data to the cloud: Important functionality, but not enough to address the need. Cornell's Cascade project seeks to close the gap by creating a new platform for hosting ML and AI, optimized to achieve sharply lower delay and substantially higher bandwidth than in any existing platform. At the same time, Cascade introduces much stronger guarantees - a mix that we believe will be particularly appealing in applications where events should trigger a quick and trustworthy response. This short paper is intended as a brief overview of the effort, with details to be published elsewhere.},
booktitle = {Proceedings of the 2022 Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating Algorithms for Distributed Systems},
pages = {2–6},
numpages = {5},
keywords = {IoT, cloud computing, consistency, edge intelligence, time-sensitive computing},
location = {Salerno, Italy},
series = {ApPLIED '22}
}

@inproceedings{10.1145/3448016.3452764,
author = {Maroulis, Stavros and Bikakis, Nikos and Papastefanatos, George and Vassiliadis, Panos and Vassiliou, Yannis},
title = {RawVis: A System for Efficient In-situ Visual Analytics},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452764},
doi = {10.1145/3448016.3452764},
abstract = {In-situ processing has received a great deal of attention in recent years. In in-situ scenarios, big raw data files which do not fit in main memory, must be efficiently handled on-the-fly using commodity hardware, without the overhead of a preprocessing phase or the loading of data into a database system. This paper presents RawVis, an open source data visualization system for in-situ visual exploration and analytics over big raw data. RawVis implements novel indexing schemes and adaptive processing techniques allowing users to perform efficient visual and analytics operations directly over the data files. RawVis provides real-time interaction, reporting low response time, over large data files, using commodity hardware.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2760–2764},
numpages = {5},
keywords = {adaptive and progressive indexing, big data visualization, big raw data, data exploration, rawvis, visual analytics, visualization tool},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1145/3371927.3371933,
author = {Claffy, K C and Clark, David},
title = {The 11th workshop on active internet measurements (AIMS-11) workshop report},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/3371927.3371933},
doi = {10.1145/3371927.3371933},
abstract = {On 16-17 April 2018, CAIDA hosted its eleventh Workshop on Active Internet Measurements (AIMS-11). This workshop series provides a forum for stakeholders in Internet active measurement projects to communicate their interests and concerns, and explore cooperative approaches to maximizing the collective benefit of deployed infrastructure and gathered data. An overarching theme this year was scaling the storage, indexing, annotation, and usage of Internet measurements. We discussed tradeoffs in use of commercial cloud services to to make measurement results more accessible and informative to researchers in various disciplines. Other agenda topics included status updates on recent measurement infrastructures and community feedback; measurement of poorly configured infrastructure; and recent successes and approaches to evolving challenges in geolocation, topology, route hijacking, and performance measurement. We review highlights of discussions of the talks. This report does not cover each topic discussed; for more details examine workshop presentations linked from the workshop web page:http://www.caida.org/workshops/aims/1904/.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {nov},
pages = {39–43},
numpages = {5},
keywords = {active Internet measurement}
}

@inproceedings{10.1145/3532577.3532578,
author = {Newton, Benjamin and Scoggin, Michael and Ganti, Anand and Onunkwo, Uzoma and Hietala, Vincent},
title = {Clipping for Faster Wireless Network Simulation in ns-3},
year = {2022},
isbn = {9781450396516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532577.3532578},
doi = {10.1145/3532577.3532578},
abstract = {In this work we present the concept of ‘clipping’, scheduling receive events for wireless transmissions only on receivers within some distance of the transmitter. Combined with spatial indexing, this technique enables faster simulation of large-scale wireless networks containing tens of thousands or even hundreds of thousands of wireless nodes. We detail our additions and changes to ns-3 to implement this feature, demonstrate how it yields a 2 \texttimes{} speedup for a complex 5G scenario with minimal impact on simulation fidelity, and show how under special circumstances a speedup of over 40 \texttimes{} is achievable while producing identical results.},
booktitle = {Proceedings of the 2022 Workshop on Ns-3},
pages = {65–72},
numpages = {8},
keywords = {5G, clipping, high-fidelity, large-scale, network simulation, ns-3, spatial indexing, wireless},
location = {Virtual Event, USA},
series = {WNS3 '22}
}

@inproceedings{10.1145/3560905.3568534,
author = {Li, Shuheng and Shang, Jingbo and Gupta, Rajesh K. and Hong, Dezhi},
title = {SQEE: A Machine Perception Approach to Sensing Quality Evaluation at the Edge by Uncertainty Quantification},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568534},
doi = {10.1145/3560905.3568534},
abstract = {Cyber-physical systems are starting to adopt neural network (NN) models for a variety of smart sensing applications. While several efforts seek better NN architectures for system performance improvement, few attempts have been made to study the deployment of these systems in the field. Proper deployment of these systems is critical to achieving ideal performance, but the current practice is largely empirical via trials and errors, lacking a measure of quality. Sensing quality should reflect the impact on the performance of NN models that drive machine perception tasks. However, traditional approaches either evaluate statistical difference that exists objectively, or model the quality subjectively via human perception.In this work, we propose an efficient sensing quality measure requiring limited data samples using smart voice sensing system as an example. We adopt recent techniques in uncertainty evaluation for NN to estimate audio sensing quality. Intuitively, a deployment at better sensing location should lead to less uncertainty in NN predictions. We design SQEE, Sensing Quality Evaluation at the Edge for NN models, which constructs a model ensemble through Monte-Carlo dropout and estimates posterior total uncertainty via average conditional entropy. We collected data from three indoor environments, with a total of 148 transmitting-receiving (t-r) locations experimented and more than 7,000 examples tested. SQEE achieves the best performance in terms of the top-1 ranking accuracy---whether the measure finds the best spot for deployment, in comparison with other uncertainty strategies. We implemented SQEE on a ReSpeaker to study SQEE's real-world efficacy. Experimental result shows that SQEE can effectively evaluate the data collected from each t-r location pair within 30 seconds and achieve an average top-3 ranking accuracy of over 94%. We further discuss generalization of our framework to other sensing schemes.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {277–290},
numpages = {14},
keywords = {sensing quality evaluation, speech sensing, uncertainty quantification},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@proceedings{10.1145/3557915,
title = {SIGSPATIAL '22: Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The conference started as a series of workshops and symposia back in 1993 with the aim of promoting interdisciplinary discussions among researchers, developers, users, and practitioners and fostering research in all aspects of geographic information systems, especially in relation to novel systems based on geospatial data and knowledge. It continues to provide a forum for original research contributions covering all conceptual, design and implementation aspects of geospatial data ranging from applications, user interfaces and visualization, to data storage, query processing, indexing, machine learning and data mining. The conference is the premier annual event of the ACM Special Interest Group on Spatial Information (ACM SIGSPATIAL).},
location = {Seattle, Washington}
}

@inproceedings{10.1145/3429789.3429830,
author = {William, George and Anthony, Randy and Purnama, James},
title = {Development of NodeJS based Backend System with Multiple Storefronts for Batik Online Store},
year = {2020},
isbn = {9781450387712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429789.3429830},
doi = {10.1145/3429789.3429830},
abstract = {Inibatikita is an online clothing store company which sells its products through multiple online stores, each of which is associated with a partner influencer. Currently, there exist no ecommerce solution which can accommodate this business process. This research aims to develop a system which facilitates this business process while addressing scalability for up to 50000 users. After the requirements are gathered, a system analysis is performed to compare existing ecommerce solutions with a custom solution. Post analysis, system design takes place and is swiftly followed by development and testing; finishing with deployment of the system. The results of system analysis indicate that existing ecommerce solutions aren't able to accommodate a multiple store approach, while a custom NodeJS solution is capable of doing so. Development and testing then proves that a NodeJS solution is able to fulfill the business process while accommodating 50000 users through indexing and caching. The results show that a custom NodeJS solution addresses the business requirements and scalability of inibatikita, when existing solutions could not.},
booktitle = {Proceedings of the 2020 International Conference on Engineering and Information Technology for Sustainable Industry},
articleno = {40},
numpages = {6},
keywords = {Batik, Ecommerce, Multi Store, MySQL, NodeJS},
location = {Tangerang, Indonesia},
series = {ICONETSI '20}
}

@inproceedings{10.1145/3167132.3167216,
author = {Dilli, Renato and Argou, Amanda and Pilla, Mauricio and Pernas, Ana Marilza and Reiser, Renata and Yamin, Adenauer},
title = {Fuzzy logic and MCDA in IoT resources classification},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167216},
doi = {10.1145/3167132.3167216},
abstract = {Internet of Things is characterized by a lot of resources connected to the Internet, many simultaneously requesting and providing services. The adequate selection of resources that best meets the demands of users with a broad range of options has been a relevant and current research challenge. Based on the non-functional parameters of QoS, it plays a significant role in the ranking of these resources according to the services they offer. This work presents a proposal to classify and select the most appropriate resource to the client's request, using fuzzy logic to treat uncertainties in the definition of ideal weights for QoS attributes. It also proposes to add fuzzy logic in the pre-classification of resources, to reduce the computational cost generated by MCDA algorithms. The accuracy of the proposed model in the pre-classification of the best resources is presented, and the results obtained are promising and indicate the continuity of the research.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {761–766},
numpages = {6},
keywords = {IoT, MCDA, fuzzy logic, resources ranking},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3299869.3319859,
author = {Perdacher, Martin and Plant, Claudia and B\"{o}hm, Christian},
title = {Cache-oblivious High-performance Similarity Join},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3319859},
doi = {10.1145/3299869.3319859},
abstract = {A similarity join combines vectors based on a distance condition. Typically, such algorithms apply a filter step (by indexing or sorting) and then refine pairs of candidate vectors. In this paper, we propose to refine the pairs in an order defined by a space-filling curve which dramatically improves data locality. Modern multi-core microprocessors are supported by a deep memory hierarchy including RAM, various levels of cache, and registers. The space-filling curve makes our proposed algorithm cache-oblivious to fully exploit the memory hierarchy and to reach the possible peak performance of a multi-core processor. Our novel space-filling curve called Fast General Form (FGF) Hilbert solves a number of limitations of well-known approaches: it is non-recursive, it is not restricted to traverse squares, and it has a constant time and space complexity. As we demonstrate the easy transformation from conventional into cache-oblivious loops we believe that many algorithms for complex joins and other database operators could be transformed systematically into cache-oblivious SIMD and MIMD parallel algorithms.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {87–104},
numpages = {18},
keywords = {cache-oblivious, epsilon grid order, hilbert-curve, similarity join, space-filling curve},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@proceedings{10.1145/3589132,
title = {SIGSPATIAL '23: Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The conference started as a series of workshops and symposia back in 1993 with the aim of promoting interdisciplinary discussions among researchers, developers, users, and practitioners and fostering research in all aspects of geographic information systems, especially in relation to novel systems based on geospatial data and knowledge. It continues to provide a forum for original research contributions covering all conceptual, design and implementation aspects of geospatial data ranging from applications, user interfaces and visualization, to data storage, query processing, indexing, machine learning and data mining. The conference is the premier annual event of the ACM Special Interest Group on Spatial Information (ACM SIGSPATIAL).},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/3284869.3284877,
author = {Furini, Marco and Galli, Giovanna and Martini, Maria Cristiana},
title = {ONELab: Online Education with Minimal Human Supervision},
year = {2018},
isbn = {9781450365819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284869.3284877},
doi = {10.1145/3284869.3284877},
abstract = {Many educational institutes are recording classroom lessons to improve the students' learning process. Unfortunately, a simple recording does not necessarily imply benefits. Indeed, when producing video lectures, there are different challenges that must be faced: production costs, accessibility, usability, video indexing, just to name a few. In this paper, we share our experience building ONELab, a system designed to capture, record, edit and stream video lectures. ONELab was designed to be scalable and to have low-cost implementation and maintenance. The system has been used in the 2017-18 Academic Year to manage the 49 courses offered by the five degrees available at our Department. In numbers, it supported 1,054 freshman students and produced 1,376 video lectures (for a total of 2,064 hours). The usage analysis showed that students appreciated the system and a comparative analysis between students who used the system and students who did not use the system, showed that the former passed more exams (+97.8%), had better grades (+8%) and acquired more credits (+105%).},
booktitle = {Proceedings of the 4th EAI International Conference on Smart Objects and Technologies for Social Good},
pages = {88–93},
numpages = {6},
keywords = {On-line learning, learning process, video lectures},
location = {Bologna, Italy},
series = {Goodtechs '18}
}

@inproceedings{10.1145/3361525.3361532,
author = {Liu, Beichen and Olivier, Pierre and Ravindran, Binoy},
title = {SlimGuard: A Secure and Memory-Efficient Heap Allocator},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361532},
doi = {10.1145/3361525.3361532},
abstract = {Attacks on the heap are an increasingly severe threat. State-of-the-art secure dynamic memory allocators can offer protection, however their memory footprint is high, making them suboptimal in many situations. We introduce Slim-Guard, a secure allocator whose design is driven by memory efficiency. Among other features, SlimGuard uses an efficient fine-grain size classes indexing mechanism and implements a novel dynamic canary scheme. It offers a low memory overhead due its size classes optimized for canary usage, its on-demand metadata allocation, and the combination of randomized allocations and over-provisioning into a single memory efficient security feature. SlimGuard protects against widespread heap-related attacks such as overflows, over-reads, double/invalid free, and use-after-free. Evaluation over a wide range of applications shows that it offers a significant reduction in memory consumption compared to the state-of-the-art secure allocator (up to 2x in macro-benchmarks), while offering similar or better security guarantees and good performance.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {1–13},
numpages = {13},
keywords = {dynamic memory allocation, memory safety},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/3318464.3389752,
author = {Kristo, Ani and Vaidya, Kapil and \c{C}etintemel, Ugur and Misra, Sanchit and Kraska, Tim},
title = {The Case for a Learned Sorting Algorithm},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389752},
doi = {10.1145/3318464.3389752},
abstract = {Sorting is one of the most fundamental algorithms in Computer Science and a common operation in databases not just for sorting query results but also as part of joins (i.e., sort-merge-join) or indexing. In this work, we introduce a new type of distribution sort that leverages a learned model of the empirical CDF of the data. Our algorithm uses a model to efficiently get an approximation of the scaled empirical CDF for each record key and map it to the corresponding position in the output array. We then apply a deterministic sorting algorithm that works well on nearly-sorted arrays (e.g., Insertion Sort) to establish a totally sorted order. We compared this algorithm against common sorting approaches and measured its performance for up to 1 billion normally-distributed double-precision keys. The results show that our approach yields an average 3.38x performance improvement over C++ STL sort, which is an optimized Quicksort hybrid, 1.49x improvement over sequential Radix Sort, and 5.54x improvement over a C++ implementation of Timsort, which is the default sorting function for Java and Python.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1001–1016},
numpages = {16},
keywords = {CDF, ML for systems, RMI, learned algorithm, linear interpolation, linear models, sorting, sorting algorithm},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3331076.3331118,
author = {Riegger, Christian and Vin\c{c}on, Tobias and Petrov, Ilia},
title = {Indexing large updatable datasets in multi-version database management systems},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331118},
doi = {10.1145/3331076.3331118},
abstract = {Database Management Systems (DBMS) need to handle large updatable datasets in on-line transaction processing (OLTP) workloads. Most modern DBMS provide snapshots of data in multi-version concurrency control (MVCC) transaction management scheme. Each transaction operates on a snapshot of the database, which is calculated from a set of tuple versions. High parallelism and resource-efficient append-only data placement on secondary storage is enabled. One major issue in indexing tuple versions on modern hardware technologies is the high write amplification for tree-indexes.Partitioned B-Trees (PBT) [5] is based on the structure of the ubiquitous B+-Tree [8]. They achieve a near optimal write amplification and beneficial sequential writes on secondary storage. Yet they have not been implemented in a MVCC enabled DBMS to date.In this paper we present the implementation of PBTs in PostgreSQL extended with SIAS. Compared to PostgreSQL's B+-Trees PBTs have 50% better transaction throughput under TPC-C and a 30% improvement to standard PostgreSQL with Heap-Only Tuples.},
booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
articleno = {36},
numpages = {5},
keywords = {MVCC, indexing structure, modern storage hardware},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.1145/3512576.3512665,
author = {zhang, shuqin and jiang, wei and yang, ning and long, bing and zhu, jianan and zhou, yi},
title = {Time elasticity evaluation of flight turnaround ground service process},
year = {2022},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512665},
doi = {10.1145/3512576.3512665},
abstract = {Based on time elasticity definitions for passengers, ground staff and crew, we propose one evaluation method, which screens out airport ground service process with the maximum time elasticity for transit flight, to reduce arrival time of passengers at airports, increase the flexibility of ground services and improve the connectivity between arrivals and departures for crew. Firstly, timed delay Petri net is used to elicit all feasible service processes of one transit flight, and then the optimal service process is obtained by Analytic Hierarchy Process and fuzzy comprehensive evaluation method. Finally, one concrete example results show that the method can obtain time elasticity ranking of all feasible service processes which demonstrates its effectiveness.},
booktitle = {Proceedings of the 2021 9th International Conference on Information Technology: IoT and Smart City},
pages = {519–524},
numpages = {6},
keywords = {AHP, FCEM, flight turnaround ground service, time elasticity, timed delay Petri net},
location = {Guangzhou, China},
series = {ICIT '21}
}

@inproceedings{10.1145/3018896.3018936,
author = {Essafi, Hassane and H\`{e}de, Patrick},
title = {FRAMSTIM: framework for large scale multimedia content feature extraction based on MPI one-sided communication},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018936},
doi = {10.1145/3018896.3018936},
abstract = {Every day a large number of images are made available throw social networks and different IoT embedded sensors. R&amp;D devoted to the development of applications based on visual pattern recognition has attracted a large population of researchers in both side academic and industry. Extraction of relevant features is challenging and known to be one of the key issues in many applications where the visual pattern recognition is applied (object recognition and tracking, image identification, multimedia document categorization, indexing and retrieval, deep learning based visual feature coding, video surveillance, robotic, activity recognition). Furthermore the extraction features from a big volume of image and video data is time and resources consuming. In the context of the ITEA2 project H4H/PerfCloud ( Performance in the Cloud) we have developed parallel OpenMP threads video engine search. To scale the extraction of visual features from a large volume of streaming visual content, we have developed a framework based on OpenMP and MPI one-sided communication where the computation and communication are overlapped thanks to the RDMA approach.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {41},
numpages = {6},
keywords = {MPI, RMA, concept construction, feature extraction, multimedia engine, one-sided communication, visual data characterization and representation},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3307650.3322263,
author = {Zhang, Jiaqi and Chen, Xiangru and Song, Mingcong and Li, Tao},
title = {Eager pruning: algorithm and architecture support for fast training of deep neural networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322263},
doi = {10.1145/3307650.3322263},
abstract = {Today's big and fast data and the changing circumstance require fast training of Deep Neural Networks (DNN) in various applications. However, training a DNN with tons of parameters involves intensive computation. Enlightened by the fact that redundancy exists in DNNs and the observation that the ranking of the significance of the weights changes slightly during training, we propose Eager Pruning, which speeds up DNN training by moving pruning to an early stage.Eager Pruning is supported by an algorithm and architecture co-design. The proposed algorithm dictates the architecture to identify and prune insignificant weights during training without accuracy loss. A novel architecture is designed to transform the reduced training computation into performance improvement. Our proposed Eager Pruning system gains an average of 1.91x speedup over state-of-the-art hardware accelerator and 6.31x energy-efficiency over Nvidia GPUs.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {292–303},
numpages = {12},
keywords = {neural network pruning, neural network training, software-hardware co-design},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@article{10.1145/3639280,
author = {Chen, Ziling and Guan, Haoquan and Song, Shaoxu and Huang, Xiangdong and Wang, Chen and Wang, Jianmin},
title = {Determining Exact Quantiles with Randomized Summaries},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639280},
doi = {10.1145/3639280},
abstract = {Quantiles are fundamental statistics in various data science tasks, but costly to compute, e.g., by loading the entire data in memory for ranking. With limited memory space, prevalent in end devices or databases with heavy loads, it needs to scan the data in multiple passes. The idea is to gradually shrink the range of the queried quantile till it is small enough to fit in memory for ranking the result. Existing methods use deterministic sketches to determine the exact range of quantile, known as deterministic filter, which could be inefficient in range shrinking. In this study, we propose to shrink the ranges more aggressively, using randomized summaries such as KLL sketch. That is, with a high probability the quantile lies in a smaller range, namely probabilistic filter, determined by the randomized sketch. Specifically, we estimate the expected passes for determining the exact quantiles with probabilistic filters, and select a proper probability that can minimize the expected passes. Analyses show that our exact quantile determination method can terminate in P passes with 1-δ confidence, storing O(N 1/P logP-1/2P (1/δ)) items, close to the lower bound \O{}mega(N1/P) for a fixed δ. The approach has been deployed as a function in an LSM-tree based time-series database Apache IoTDB. Remarkably, the randomized sketches can be pre-computed for the immutable SSTables in LSM-tree. Moreover, multiple quantile queries could share the data passes for probabilistic filters in range estimation. Extensive experiments on real and synthetic datasets demonstrate the superiority of our proposal compared to the existing methods with deterministic filters. On average, our method takes 0.48 fewer passes and 18% of the time compared with the state-of-the-art deterministic sketch (GK sketch).},
journal = {Proc. ACM Manag. Data},
month = {mar},
articleno = {25},
numpages = {26},
keywords = {data stream, quantile, sketches}
}

@article{10.1145/3095021,
author = {Lee, Woosuk and Lee, Wonchan and Kang, Dongok and Heo, Kihong and Oh, Hakjoo and Yi, Kwangkeun},
title = {Sound Non-Statistical Clustering of Static Analysis Alarms},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/3095021},
doi = {10.1145/3095021},
abstract = {We present a sound method for clustering alarms from static analyzers. Our method clusters alarms by discovering sound dependencies between them such that if the dominant alarms of a cluster turns out to be false, all the other alarms in the same cluster are guaranteed to be false. We have implemented our clustering algorithm on top of a realistic buffer-overflow analyzer and proved that our method reduces 45% of alarm reports. Our framework is applicable to any abstract interpretation-based static analysis and orthogonal to abstraction refinements and statistical ranking schemes.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {aug},
articleno = {16},
numpages = {35},
keywords = {Static analysis, abstract interpretation, false alarms}
}

@article{10.14778/3425879.3425885,
author = {Tran, Luan and Mun, Min Y. and Shahabi, Cyrus},
title = {Real-time distance-based outlier detection in data streams},
year = {2020},
issue_date = {October 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3425879.3425885},
doi = {10.14778/3425879.3425885},
abstract = {Real-time outlier detection in data streams has drawn much attention recently as many applications need to be able to detect abnormal behaviors as soon as they occur. The arrival and departure of streaming data on edge devices impose new challenges to process the data quickly in real-time due to memory and CPU limitations of these devices. Existing methods are slow and not memory efficient as they mostly focus on quick detection of inliers and pay less attention to expediting neighbor searches for outlier candidates. In this study, we propose a new algorithm, CPOD, to improve the efficiency of outlier detections while reducing its memory requirements. CPOD uses a unique data structure called "core point" with multi-distance indexing to both quickly identify inliers and reduce neighbor search spaces for outlier candidates. We show that with six real-world and one synthetic dataset, CPOD is, on average, 10, 19, and 73 times faster than M_MCOD, NETS, and MCOD, respectively, while consuming low memory.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {141–153},
numpages = {13}
}

@article{10.14778/3476311.3476382,
author = {Potharaju, Rahul and Kim, Terry and Song, Eunjin and Wu, Wentao and Novik, Lev and Dave, Apoorve and Fogarty, Andrew and Pirzadeh, Pouria and Acharya, Vidip and Dhody, Gurleen and Li, Jiying and Ramanujam, Sinduja and Bruno, Nicolas and Galindo-Legaria, C\'{e}sar A. and Narasayya, Vivek and Chaudhuri, Surajit and Nori, Anil K. and Talius, Tomas and Ramakrishnan, Raghu},
title = {Hyperspace: the indexing subsystem of azure synapse},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476382},
doi = {10.14778/3476311.3476382},
abstract = {Microsoft recently introduced Azure Synapse Analytics, which offers an integrated experience across data ingestion, storage, and querying in Apache Spark and T-SQL over data in the lake, including files and warehouse tables. In this paper, we present our experiences with designing and implementing Hyperspace, the indexing subsystem underlying Synapse. Hyperspace enables users to build multiple types of secondary indexes on their data, maintain them through a multi-user concurrency model, and leverage them automatically---without any change to their application code---for query/workload acceleration. Many requirements of Hyperspace are based on feedback from several enterprise customers. We present the details of Hyperspace's underlying design, the user-facing APIs, its concurrency control protocol for index access, its index-aware query processing techniques, and its maintenance mechanisms for handling index updates. Evaluations over standard industry benchmarks and real customer workloads show that Hyperspace can accelerate query execution by up to 10x and in certain real-world workloads, even up to two orders of magnitude.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3043–3055},
numpages = {13}
}

@article{10.14778/3368289.3368303,
author = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis and Benbrahim, Houda},
title = {Return of the Lernaean Hydra: experimental evaluation of data series approximate similarity search},
year = {2019},
issue_date = {November 2019},
publisher = {VLDB Endowment},
volume = {13},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3368289.3368303},
doi = {10.14778/3368289.3368303},
abstract = {Data series are a special type of multidimensional data present in numerous domains, where similarity search is a key operation that has been extensively studied in the data series literature. In parallel, the multidimensional community has studied approximate similarity search techniques. We propose a taxonomy of similarity search techniques that reconciles the terminology used in these two domains, we describe modifications to data series indexing techniques enabling them to answer approximate similarity queries with quality guarantees, and we conduct a thorough experimental evaluation to compare approximate similarity search techniques under a unified framework, on synthetic and real datasets in memory and on disk. Although data series differ from generic multidimensional vectors (series usually exhibit correlation between neighboring values), our results show that data series techniques answer approximate queries with strong guarantees and an excellent empirical performance, on data series and vectors alike. These techniques outperform the state-of-the-art approximate techniques for vectors when operating on disk, and remain competitive in memory.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {403–420},
numpages = {18}
}

@inproceedings{10.1145/3550355.3552461,
author = {L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Rubei, Riccardo and Cuadrado, Jes\'{u}s S\'{a}nchez and di Ruscio, Davide},
title = {Machine learning methods for model classification: a comparative study},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552461},
doi = {10.1145/3550355.3552461},
abstract = {In the quest to reuse modeling artifacts, academics and industry have proposed several model repositories over the last decade. Different storage and indexing techniques have been conceived to facilitate searching capabilities to help users find reusable artifacts that might fit the situation at hand. In this respect, machine learning (ML) techniques have been proposed to categorize and group large sets of modeling artifacts automatically. This paper reports the results of a comparative study of different ML classification techniques employed to automatically label models stored in model repositories. We have built a framework to systematically compare different ML models (feed-forward neural networks, graph neural networks, k-nearest neighbors, support version machines, etc.) with varying model encodings (TF-IDF, word embeddings, graphs and paths). We apply this framework to two datasets of about 5,000 Ecore and 5,000 UML models. We show that specific ML models and encodings perform better than others depending on the characteristics of the available datasets (e.g., the presence of duplicates) and on the goals to be achieved.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {165–175},
numpages = {11},
keywords = {machine learning, model classification, model-driven engineering},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@article{10.1145/3569455,
author = {Yang, Gangqiang and Shi, Zhengyuan and Chen, Cheng and Xiong, Hailiang and Li, Fudong and Hu, Honggang and Wan, Zhiguo},
title = {Hardware Optimizations of Fruit-80 Stream Cipher: Smaller than Grain},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3569455},
doi = {10.1145/3569455},
abstract = {Fruit-80, which emerged as an ultra-lightweight stream cipher with 80-bit secret key, is oriented toward resource-constrained devices in the Internet of Things. In this article, we propose area and speed optimization architectures of Fruit-80 on FPGAs. Our implementations include both serial and parallel structure and optimize area, power, speed, and throughput, respectively. The area optimization architecture aims to achieve the most suitable ratio of look-up-tables and flip-flops to fully utilize the reconfigurable unit. It also reuses NFSR and LFSR feedback functions to save resources for high throughput. The speed optimization architecture adopts a hybrid approach for parallelization and reduces the latency of long data paths by pre-generating primary feedback and inserting flip-flops. Besides, we recommend using the round key function to optimize serial or parallel implementations for Fruit-80 and using indexing and shifting methods for different throughput. In conclusion, our results show that the area optimization architecture occupies up to 35 slices on Xilinx Spartan-3 FPGA and 18 slices on Xilinx 7 series FPGA, smaller than that of Grain and other common stream ciphers. The optimal throughput/area ratio of the speed optimization architecture is 7.74 Mbps/slice, better than that of Grain v1, which is 5.98 Mbps/slice. The serial implementation of Fruit-80 with round key function occupies only 75 slices on Spartan-3 FPGA. To the best of our knowledge, the result sets a new record of the minimum area in lightweight cipher implementation on FPGA.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {mar},
articleno = {22},
numpages = {32},
keywords = {Hardware optimization, lightweight cryptography, parallelism, Fruit-80}
}

@inproceedings{10.1145/3282373.3282402,
author = {Riegger, Christian and Vin\c{c}on, Tobias and Petrov, Ilia},
title = {Efficient Data and Indexing Structure for Blockchains in Enterprise Systems},
year = {2018},
isbn = {9781450364799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282373.3282402},
doi = {10.1145/3282373.3282402},
abstract = {Blockchains yield to new workloads in database management systems and K/V-Stores. Distributed Ledger Technology (DLT) is a technique for managing transactions in 'trustless' distributed systems. Yet, clients of nodes in blockchain networks are backed by 'trustworthy' K/V-Stores, like LevelDB or RocksDB in Ethereum, which are based on Log-Structured Merge Trees (LSM-Trees). However, LSM-Trees do not fully match the properties of blockchains and enterprise workloads.In this paper, we claim that Partitioned B-Trees (PBT) fit the properties of this DLT: uniformly distributed hash keys, immutability, consensus, invalid blocks, unspent and off-chain transactions, reorganization and data state / version ordering in a distributed log-structure. PBT can locate records of newly inserted key-value pairs, as well as data of unspent transactions, in separate partitions in main memory. Once several blocks acquire consensus, PBTs evict a whole partition, which becomes immutable, to secondary storage. This behavior minimizes write amplification and enables a beneficial sequential write pattern on modern hardware. Furthermore, DLT implicate some type of log-based versioning. PBTs can serve as MV-Store for data storage of logical blocks and indexing in multi-version concurrency control (MVCC) transaction processing.},
booktitle = {Proceedings of the 20th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {173–182},
numpages = {10},
keywords = {Blockchain, Data Structure, Enterprise Workload, K/V-Store},
location = {Yogyakarta, Indonesia},
series = {iiWAS2018}
}

@inproceedings{10.1145/3335783.3335793,
author = {Mytilinis, Ioannis and Tsoumakos, Dimitrios and Koziris, Nectarios},
title = {Maintaining Wavelet Synopses for Sliding-Window Aggregates},
year = {2019},
isbn = {9781450362160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335783.3335793},
doi = {10.1145/3335783.3335793},
abstract = {The IoT era has brought forth a computing paradigm shift from traditional high-end servers to "edge" devices of limited processing and memory capabilities. These devices, together with sensors, regularly produce very high data volumes nowadays. For many real-time applications, storing and indexing an unbounded stream may not be an option. Thus, it is important that we design algorithms and systems that can both work at the edge of the network and be able to answer queries on distributed, streaming data. Moreover, in many streaming scenarios, fresh data tend to be prioritized. A sliding-window model is an important case of stream processing, where only the most recent elements remain active and the rest are discarded. In this work, we study the problem of maintaining basic aggregate statistics over a sliding-window data stream under the constraint of limited memory. As in IoT scenarios the available memory is typically much less than the window size, queries are answered from compact synopses that are maintained in an online fashion. For the efficient construction of such synopses, in this work, we propose wavelet-based algorithms that provide deterministic guarantees and produce almost exact results. Our algorithms can work on any kind of numerical data and do not have the positive-numbers constraint of techniques such as the exponential histograms. Our experimental evaluation indicates that, in terms of accuracy and space-efficiency, our solution outperforms the exponential histograms and deterministic waves techniques.},
booktitle = {Proceedings of the 31st International Conference on Scientific and Statistical Database Management},
pages = {73–84},
numpages = {12},
location = {Santa Cruz, CA, USA},
series = {SSDBM '19}
}

@article{10.14778/3484224.3484225,
author = {Liu, Jian and Wang, Kefei and Chen, Feng},
title = {TSCache: an efficient flash-based caching scheme for time-series data workloads},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484225},
doi = {10.14778/3484224.3484225},
abstract = {Time-series databases are becoming an indispensable component in today's data centers. In order to manage the rapidly growing time-series data, we need an effective and efficient system solution to handle the huge traffic of time-series data queries. A promising solution is to deploy a high-speed, large-capacity cache system to relieve the burden on the backend time-series databases and accelerate query processing. However, time-series data is drastically different from other traditional data workloads, bringing both challenges and opportunities. In this paper, we present a flash-based cache system design for time-series data, called TSCache. By exploiting the unique properties of time-series data, we have developed a set of optimization schemes, such as a slab-based data management, a two-layered data indexing structure, an adaptive time-aware caching policy, and a low-cost compaction process. We have implemented a prototype based on Twitter's Fatcache. Our experimental results show that TSCache can significantly improve client query performance, effectively increasing the bandwidth by a factor of up to 6.7 and reducing the latency by up to 84.2%.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3253–3266},
numpages = {14}
}

@inproceedings{10.1145/1989323.1989386,
author = {Jin, Wen and Patel, Jignesh M.},
title = {Efficient and generic evaluation of ranked queries},
year = {2011},
isbn = {9781450306614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989323.1989386},
doi = {10.1145/1989323.1989386},
abstract = {An important feature of the existing methods for ranked top-k processing is to avoid searching all the objects in the underlying dataset, and limiting the number of random accesses to the data. However, the performance of these methods degrades rapidly as the number of random accesses increases. In this paper, we propose a novel and general sequential access scheme for top-k query evaluation, which outperforms existing methods. We extend this scheme to efficiently answer top-k queries in subspace and on dynamic data. We also study the "dual" form of top-k queries called "ranking" queries, which returns the rank of a specified record/object, and propose an exact as well as two approximate solutions. An extensive empirical evaluation validates the robustness and efficiency of our techniques.},
booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
pages = {601–612},
numpages = {12},
keywords = {ranked queries},
location = {Athens, Greece},
series = {SIGMOD '11}
}

@inproceedings{10.1145/3653724.3653776,
author = {Duan, Junyi},
title = {Data feature analysis for blast furnace temperature prediction from machine learning perspective},
year = {2024},
isbn = {9798400716973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653724.3653776},
doi = {10.1145/3653724.3653776},
abstract = {Modern blast furnace ironmaking technology mainly uses the thermal state of the furnace cylinder to reflect the furnace temperature conditions. However, due to the complexity of the blast furnace smelting process, it is very difficult to modelling and control this process effectively. Therefore, it is important to carry out research on blast furnace temperature prediction modelling in order to realize early warning of furnace health condition in production systems, where nowadays more and more attentions are paid to technologies of machine learning and deep learning. Taking the actual application scenario of a large iron and steel production enterprise as a case study, this paper focuses on the lack of robustness when training machine learning models, due to the noise in the original collected business data. The experimental results show that the application of feature engineering, including feature construction, key feature analysis, and feature ranking within different data analysis stages, is able to improve the quality of the collected raw business data, consequently it is helpful in solving practical engineering problems from machine learning perspective.},
booktitle = {Proceedings of the International Conference on Mathematics and Machine Learning},
pages = {298–303},
numpages = {6},
location = {Nanjing, China},
series = {ICMML '23}
}

@inproceedings{10.1145/2912160.2912167,
author = {Kubler, Sylvain and Robert, J\'{e}r\'{e}my and Le Traon, Yves and Umbrich, J\"{u}rgen and Neumaier, Sebastian},
title = {Open Data Portal Quality Comparison using AHP},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912167},
doi = {10.1145/2912160.2912167},
abstract = {During recent years, more and more Open Data becomes available and used as part of the Open Data movement. However, there are reported issues with the quality of the metadata in data portals and the data itself. This is a serious risk that could disrupt the Open Data project, as well as e-government initiatives since the data quality needs to be managed to guarantee the reliability of e-government to the public. First quality assessment frameworks emerge to evaluate the quality for a given dataset or portal along various dimensions (e.g., information completeness). Nonetheless, a common problem with such frameworks is to provide meaningful ranking mechanisms that are able to integrate several quality dimensions and user preferences (e.g., a portal provider is likely to have different quality preferences than a portal consumer). To address this multi-criteria decision making problem, our research work applies AHP (Analytic Hierarchy Process), which compares 146 active Open Data portals across 44 countries, powered by the CKAN software.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {397–407},
numpages = {11},
keywords = {Analytic Hierarchy Process, Data Quality, E-government, Multi-Criteria Decision Making, Open Data},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.1145/3486183.3491000,
author = {Evagorou, Giannis and Ghosh, Abhirup and Heinis, Thomas},
title = {HYPO: skew-resilient partitioning for trajectory datasets},
year = {2021},
isbn = {9781450391009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486183.3491000},
doi = {10.1145/3486183.3491000},
abstract = {The rapid increase of GPS-enabled devices has led to immense amounts of trajectory data being collected and analyzed. To provide insight into these datasets, a number of spatio-temporal queries need to be executed efficiently and at scale. One such important query is the Query by Path, which given a series of road segments and a time interval, retrieves all trajectories that have passed through the road segments within a given time interval. The Query by Path finds application in many areas, including traffic management, transportation planning and fleet monitoring.In this paper we develop an approach to partition and distribute trajectories across a cluster and execute queries by path at scale. At the center of our approach is the partitioning of the entire dataset and indexing each partition with a Trie. We develop a basic set of partitioning approaches and show that each can be rendered inefficient by skew in the dataset. We consequently propose a HYbrid PartitiOning algorithm (HYPO) that performs robustly in face of skew. We also provide the cost models to configure HYPO. Finally we assess its performance extensively using both real and synthetic datasets to demonstrate that it scales well in face of skew.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
articleno = {5},
numpages = {10},
keywords = {network-constrained indexing, path queries, skewed trajectory distributions, spatio-temporal indexing, trajectory indexing},
location = {Beijing, China},
series = {LocalRec '21}
}

@inproceedings{10.1145/2882903.2882962,
author = {Ogden, Peter and Thomas, David and Pietzuch, Peter},
title = {AT-GIS: Highly Parallel Spatial Query Processing with Associative Transducers},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882962},
doi = {10.1145/2882903.2882962},
abstract = {Users in many domains, including urban planning, transportation, and environmental science want to execute analytical queries over continuously updated spatial datasets. Current solutions for large-scale spatial query processing either rely on extensions to RDBMS, which entails expensive loading and indexing phases when the data changes, or distributed map/reduce frameworks, running on resource-hungry compute clusters. Both solutions struggle with the sequential bottleneck of parsing complex, hierarchical spatial data formats, which frequently dominates query execution time. Our goal is to fully exploit the parallelism offered by modern multi-core CPUs for parsing and query execution, thus providing the performance of a cluster with the resources of a single machine. We describe AT-GIS, a highly-parallel spatial query processing system that scales linearly to a large number of CPU cores. AT-GIS integrates the parsing and querying of spatial data using a new computational abstraction called associative transducers (ATs). ATs can form a single data-parallel pipeline for computation without requiring the spatial input data to be split into logically independent blocks. Using ATs, AT-GIS can execute, in parallel, spatial query operators on the raw input data in multiple formats, without any pre-processing. On a single 64-core machine, AT-GIT provides 3x the performance of an 8-node Hadoop cluster with 192 cores for containment queries, and 10x for aggregation queries.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1041–1054},
numpages = {14},
keywords = {JSON, NODB, XML, multi-core CPUs, parallel automata, spatial query processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3560107.3560179,
author = {Sarv, Lill and Soe, Ralf-Martin},
title = {Piloting Smart City Solutions in Very Small, Small and Medium-sized Municipalities. The Estonian case study.},
year = {2022},
isbn = {9781450396356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560107.3560179},
doi = {10.1145/3560107.3560179},
abstract = {This paper focuses on a question of how to make innovative digital smart city solutions available for the very small, small and medium-size municipalities. The paper introduces as one potential approach, for reaching this goal, a Smart City (SC) idea competition and piloting initiative carried out in Estonia, involving very small towns, small cities and medium-sized cities. The article describes the process of identifying urban challenges and ranking them in collaboration with the representatives of municipalities, crowdsourcing innovative solutions to the identified most pressing problems through open international smart city (SC) idea competition and the selection of the winning piloting ideas. The results of the SC idea competition are analysed in the context of mission-oriented-innovation (MOI) and public value (PV) theory framework, in order to evaluate the winning SC pilot plans potential to have a positive impact and added PV for the local communities, improving their quality of life and sustainability.},
booktitle = {Proceedings of the 15th International Conference on Theory and Practice of Electronic Governance},
pages = {475–482},
numpages = {8},
location = {Guimar\~{a}es, Portugal},
series = {ICEGOV '22}
}

@inproceedings{10.1145/3503162.3503164,
author = {Biswas, Chandan and Ganguly, Debasis and Bhattacharya, Ujjwal},
title = {Approximate Nearest Neighbour Search on Privacy-aware Encoding of User Locations to Identify Susceptible Infections in Simulated Epidemics},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503164},
doi = {10.1145/3503162.3503164},
abstract = {Amidst an increasing number of infected cases during the Covid-19 pandemic, it is essential to trace, as early as possible, the susceptible people who might have been infected by the disease due to their close proximity with people who were tested positive for the virus. This early contact tracing is likely to limit the rate of spread of the infection within a locality. In this paper, we investigate how effectively and efficiently can such a list of susceptible people be found given a list of infected persons and their locations. By using the locations of the given list of infected persons as queries, we investigate the feasibility of applying approximate nearest neighbour (ANN) based indexing and retrieval approaches to obtain a list of top-k suspected users in real-time. Since leveraging information from true user location data can lead to privacy concerns, we also investigate the effectiveness of the ANN methods on privacy-aware encoding of the input data. Experiments conducted on real and synthetic datasets demonstrate that the top-k susceptible users retrieved with existing ANN approaches (KD-tree and HNSW) yield satisfactory recall values and achieves up to 21000 \texttimes{} speed-gain compared to exhaustive search, thus indicating that ANN approaches can potentially be applied, in practice, to facilitate real-time contact tracing even under the presence of imposed privacy constraints.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {35–42},
numpages = {8},
location = {Virtual Event, India},
series = {FIRE '21}
}

@article{10.14778/3457390.3457406,
author = {Kulkarni, Chinmay and Chandramouli, Badrish and Stutsman, Ryan},
title = {Achieving high throughput and elasticity in a larger-than-memory store},
year = {2021},
issue_date = {April 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3457390.3457406},
doi = {10.14778/3457390.3457406},
abstract = {Millions of sensors, mobile applications and machines now generate billions of events. Specialized many-core key-value stores (KVSs) can ingest and index these events at high rates (over 100 Mops/s on one machine) if events are generated on the same machine; however, to be practical and cost-effective they must ingest events over the network and scale across cloud resources elastically.We present Shadowfax, a new distributed KVS based on FASTER, that transparently spans DRAM, SSDs, and cloud blob storage while serving 130 Mops/s/VM over commodity Azure VMs using conventional Linux TCP. Beyond high single-VM performance, Shadowfax uses a unique approach to distributed reconfiguration that avoids any server-side key ownership checks or cross-core coordination both during normal operation and migration. Hence, Shadowfax can shift load in 17 s to improve system throughput by 10 Mops/s with little disruption. Compared to the state-of-the-art, it has 8x better throughput (than Seastar+memcached) and avoids costly I/O to move cold data during migration. On 12 machines, Shadowfax retains its high throughput to perform 930 Mops/s, which, to the best of our knowledge, is the highest reported throughput for a distributed KVS used for large-scale data ingestion and indexing.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {1427–1440},
numpages = {14}
}

@inproceedings{10.1145/3591106.3592289,
author = {Zhang, Ying and Zheng, Lilei and Thing, Vrizlynn L.L. and Zimmermann, Roger and Guo, Bin and Yu, Zhiwen},
title = {FaceLivePlus: A Unified System for Face Liveness Detection and Face Verification},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592289},
doi = {10.1145/3591106.3592289},
abstract = {Face verification is a trending way to verify someone’s identity in broad applications. But such systems are vulnerable to face spoofing attacks via, for example, a fraudulent copy of a photo, making it necessary to include face liveness detection as an additional safeguard. Among most existing studies, the face liveness detection is realized in a separate machine learning model in addition to the model for face verification. Such a two-model configuration may face challenges when deployed onto platforms with limited computation power and storage (e.g. mobile phone, IoT devices), especially considering each model may have millions of parameters. Inspired by the fact that humans can verify a person’s identity and liveness at a single glance from a face, we develop a novel system, named FaceLivePlus, to learn a single and universal face descriptor for the two tasks (face verification and liveness detection) so that the computational workload and storage space can be halved. To achieve this, we formulate the underlying relationship between the two tasks, and seamlessly embed this relationship in a distance ranking deep model. The model directly works on features rather than classification labels, which makes the system well generalized on unseen data. Extensive experiments show that our average half total error rate (HTER) has at least 15% and 8% improvement from the state-of-the-arts on two benchmark datasets. We anticipate this approach could become a new direction for face authentication.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {144–152},
numpages = {9},
keywords = {Biometrics, Face authentication, Face liveness detection, Multimedia Forensics, Multimedia Security, Multimedia System},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3588340.3588372,
author = {ming, du Hua and Liu, Yu and fang, zeng Ju and Hu, Chuan Cong and bao, yao Li},
title = {Exploration of Blockchain Technology in Trusted atmospheric Environmental Data},
year = {2023},
isbn = {9781450399548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588340.3588372},
doi = {10.1145/3588340.3588372},
abstract = {In the process of atmospheric environment data monitoring and governance, due to the assessment and ranking of the main governance departments, environmental data are often affected by many objective and subjective factors, resulting in data fraud or loss. Previously, after finding such phenomena through manual comparison, the management department often took the worst value of the environmental data in the day as the environmental data on the assessed area, so as to assess and punish. Although this method can play a certain deterrent effect, it seriously affects the authenticity of data and the fairness of assessment, in the long run, it will bring unpredictable challenges to the authenticity of environmental data. Based on blockchain technology, this paper combines residual network, forward Euler method, and convolutional neural network to calculate the mapping relationship between different monitoring stations through historical environmental data. Based on the training set, the minimum deviation value of distorted data or lost data is obtained, so as to provide key technical support for the trusted management of atmospheric environmental data, realize data trusted management, and ensure the integrity, authenticity and traceability of atmospheric environmental data..},
booktitle = {Proceedings of the 2022 International Conference on Big Data, IoT, and Cloud Computing},
articleno = {2},
numpages = {9},
keywords = {Air quality, Blockchain, Convolutional Neural Networks, Graph of relational networks, Residual Network, Trust Management},
location = {Chengdu, China},
series = {ICBICC '22}
}

@inproceedings{10.1145/3600100.3625681,
author = {Gokhale, Gargya and Tiben, Niels and Verwee, Marie-Sophie and Lahariya, Manu and Claessens, Bert and Develder, Chris},
title = {Real-World Implementation of Reinforcement Learning Based Energy Coordination for a Cluster of Households},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3625681},
doi = {10.1145/3600100.3625681},
abstract = {Given its substantial contribution of 40% to global power consumption, the built environment has received increasing attention to serve as a source of flexibility to assist the modern power grid. In that respect, previous research mainly focused on energy management of individual buildings. In contrast, in this paper, we focus on aggregated control of a set of residential buildings, to provide grid supporting services, that eventually should include ancillary services. In particular, we present a real-life pilot study that studies the effectiveness of reinforcement-learning (RL) in coordinating the power consumption of 8 residential buildings to jointly track a target power signal. Our RL approach relies solely on observed data from individual households and does not require any explicit building models or simulators, making it practical to implement and easy to scale. We show the feasibility of our proposed RL-based coordination strategy in a real-world setting. In a 4-week case study, we demonstrate a hierarchical control system, relying on an RL-based ranking system to select which households to activate flex assets from, and a real-time PI control-based power dispatch mechanism to control the selected assets. Our results demonstrate satisfactory power tracking, and the effectiveness of the RL-based ranks which are learnt in a purely data-driven manner.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {347–351},
numpages = {5},
keywords = {Advantage function, Building Cluster, Coordination, Demand Response, Reinforcement Learning},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@inproceedings{10.1145/1806338.1806362,
author = {Tomassen, Stein L. and Strasunskas, Darijus},
title = {Relating ontology and web terminologies by feature vectors: unsupervised construction and experimental validation},
year = {2009},
isbn = {9781605586601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806338.1806362},
doi = {10.1145/1806338.1806362},
abstract = {Search is among the most frequent activities on the Web. However, the search activity still requires extra efforts in order to get satisfactory results. One of the reasons is heterogeneous information resources and exponential growth of information. In this paper we try to tackle these issues. We elaborate on an approach to construction of semantic-linguistic feature vectors (FV) that are used in search. These FVs are built based on domain semantics encoded in an ontology and enhanced by relevant terminology from Web documents. The value of this approach is twofold. First, it captures relevant semantics from an ontology, and, second, it accounts for statistically significant collocations of other terms and phrases in relation to the ontology entities. In this paper, we elaborate on the extended FV construction process and evaluate the FV quality with respect to a set of heterogeneous ontologies. The evaluation shows that ranking of entities is significant neither for FV quality nor FV construction process. However, the results demonstrate that the construction process is most sensitive to taxonomy type of ontologies while usage of advanced and rich ontologies produces better quality FVs.},
booktitle = {Proceedings of the 11th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {88–95},
numpages = {8},
keywords = {evaluation, feature vector construction, ontology},
location = {Kuala Lumpur, Malaysia},
series = {iiWAS '09}
}

@inproceedings{10.1145/3269206.3272013,
author = {Lin, Ying and Chen, Zhengzhang and Cao, Cheng and Tang, Lu-An and Zhang, Kai and Cheng, Wei and Li, Zhichun},
title = {Collaborative Alert Ranking for Anomaly Detection},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3272013},
doi = {10.1145/3269206.3272013},
abstract = {Given a large number of low-quality heterogeneous categorical alerts collected from an anomaly detection system, how to characterize the complex relationships between different alerts and deliver trustworthy rankings to end users? While existing techniques focus on either mining alert patterns or filtering out false positive alerts, it can be more advantageous to consider the two perspectives simultaneously in order to improve detection accuracy and better understand abnormal system behaviors. In this paper, we propose CAR, a collaborative alert ranking framework that exploits both temporal and content correlations from heterogeneous categorical alerts. CAR first builds a hierarchical Bayesian model to capture both short-term and long-term dependencies in each alert sequence. Then, an entity embedding-based model is proposed to learn the content correlations between alerts via their heterogeneous categorical attributes. Finally, by incorporating both temporal and content dependencies into a unified optimization framework, CAR ranks both alerts and their corresponding alert patterns. Our experiments-using both synthetic and real-world enterprise security alert data-show that CAR can accurately identify true positive alerts and successfully reconstruct the attack scenarios at the same time.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1987–1995},
numpages = {9},
keywords = {alert ranking, anomaly detection, content dependency modeling, enterprise security system, entity embedding, temporal dependency modeling},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3339252.3339267,
author = {Iliou, Christos and Kostoulas, Theodoros and Tsikrika, Theodora and Katos, Vasilis and Vrochidis, Stefanos and Kompatsiaris, Yiannis},
title = {Towards a framework for detecting advanced Web bots},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3339267},
doi = {10.1145/3339252.3339267},
abstract = {Automated programs (bots) are responsible for a large percentage of website traffic. These bots can either be used for benign purposes, such as Web indexing, Website monitoring (validation of hyperlinks and HTML code), feed fetching Web content and data extraction for commercial use or for malicious ones, including, but not limited to, content scraping, vulnerability scanning, account takeover, distributed denial of service attacks, marketing fraud, carding and spam. To ensure their security, Web servers try to identify bot sessions and apply special rules to them, such as throttling their requests or delivering different content. The methods currently used for the identification of bots are based either purely on rule-based bot detection techniques or a combination of rule-based and machine learning techniques. While current research has developed highly adequate methods for Web bot detection, these methods' adequacy when faced with Web bots that try to remain undetected hasn't been studied. For this reason, we created and evaluated a Web bot detection framework on its ability to detect conspicuous bots separately from its ability to detect advanced Web bots. We assessed the proposed framework performance using real HTTP traffic from a public Web server. Our experimental results show that the proposed framework has significant ability to detect Web bots that do not try to hide their bot identity using HTTP Web logs (balanced accuracy in a false-positive intolerant server &gt; 95%). However, detecting advanced Web bots that present a browser fingerprint and may present a humanlike behaviour as well is considerably more difficult.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {18},
numpages = {10},
keywords = {Advanced Web bots, Evasive Web bots, Web bot detection, humanlike behaviour},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3527049.3527086,
author = {Tikhomirov, Anton and Dukeov, Igor and Kudryavtseva, Tatiana and Shneider, Aleksandra},
title = {Cross-border SME cooperation and prospects Fourth industrial revolution},
year = {2022},
isbn = {9781450386944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527049.3527086},
doi = {10.1145/3527049.3527086},
abstract = {The development of cross-border cooperation between small and medium-sized enterprises of Russia and Finland in the context of the Fourth Industrial Revolution and digital transformation is considered. Both literature data and the results of a survey of entrepreneurs of the two countries, conducted in 2020-2021 as part of the INCROBB project, are used. The sample included 168 Russian and 76 Finnish enterprises in St. Petersburg, the Leningrad region, the Republic of Karelia, and in the regions of South-East Finland. The study indicates the presence of certain problems in cross-border cooperation between Russian and Finnish entrepreneurs, which can significantly complicate the further development of cooperation in the context of digital business transformation. These include, for example, insufficient funding that has been noted by other researchers. In addition, the effectiveness of cooperation can be affected by a significant difference in the level of development of digital technologies in the national economies of Russia and Finland. Finland occupies one of the leading places in the European Union's ranking of the level of development of the digital economy, and the Russian Federation is inferior even to many countries of the former USSR. Another problem can be considered the lack of specialists with professional competencies required for digital transformation. To solve this problem, it is proposed to use the potential of "hidden experts" - qualified specialists in their field, information about which, for whatever reason, is unavailable, "hidden". The INCROBB project provides for the creation of a digital platform that allows entrepreneurs and hidden experts to interact in the implementation of various projects, execution of work and provision of services. In addition to new opportunities for expanding cross-border cooperation of small and medium enterprises, the use of hidden experts can have positive consequences in the social sphere, increasing employment and stimulating entrepreneurship.},
booktitle = {Proceedings of the 3rd International Scientific Conference on Innovations in Digital Economy},
pages = {139–145},
numpages = {7},
keywords = {cross-border cooperation, digital transformation, hidden experts, small and medium enterprises},
location = {Saint - Petersburg, Russian Federation},
series = {SPBPU IDE '21}
}

@inproceedings{10.1145/3038912.3052605,
author = {Kowald, Dominik and Pujari, Subhash Chandra and Lex, Elisabeth},
title = {Temporal Effects on Hashtag Reuse in Twitter: A Cognitive-Inspired Hashtag Recommendation Approach},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052605},
doi = {10.1145/3038912.3052605},
abstract = {Hashtags have become a powerful tool in social platforms such as Twitter to categorize and search for content, and to spread short messages across members of the social network. In this paper, we study temporal hashtag usage practices in Twitter with the aim of designing a cognitive-inspired hashtag recommendation algorithm we call BLLi,s. Our main idea is to incorporate the effect of time on (i) individual hashtag reuse (i.e., reusing own hashtags), and (ii) social hashtag reuse (i.e., reusing hashtags, which has been previously used by a followee) into a predictive model. For this, we turn to the Base-Level Learning (BLL) equation from the cognitive architecture ACT-R, which accounts for the time-dependent decay of item exposure in human memory. We validate BLLI,S using two crawled Twitter datasets in two evaluation scenarios. Firstly, only temporal usage patterns of past hashtag assignments are utilized and secondly, these patterns are combined with a content-based analysis of the current tweet. In both evaluation scenarios, we find not only that temporal effects play an important role for both individual and social hashtag reuse but also that our BLLI,S approach provides significantly better prediction accuracy and ranking results than current state-of-the-art hashtag recommendation methods.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1401–1410},
numpages = {10},
keywords = {act-r, bll equation, hashtag recommendation, hashtag reuse prediction, hashtag usage recency, hashtags, recommender systems, temporal dynamics, tf-idf, twitter},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3450267.3450544,
author = {Harder, Aron and Ranjit, Jaspreet and Behl, Madhur},
title = {Scenario2Vector: scenario description language based embeddings for traffic situations},
year = {2021},
isbn = {9781450383530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450267.3450544},
doi = {10.1145/3450267.3450544},
abstract = {A popular metric for measuring progress in autonomous driving has been the "miles per intervention". This is nowhere near a sufficient metric and it does not allow for a fair comparison between the capabilities of two autonomous vehicles (AVs). In this paper we propose Scenario2Vector - a Scenario Description Language (SDL) based embedding for traffic situations that allows us to automatically search for similar traffic situations from large AV data-sets. Our SDL embedding distills a traffic situation experienced by an AV into its canonical components - actors, actions, and the traffic scene. We can then use this embedding to evaluate similarity of different traffic situations in vector space. We have also created a first of its kind, Traffic Scenario Similarity (TSS) dataset which contains human ranking annotations for the similarity between traffic scenarios. Using the TSS data, we compare our SDL embedding -with textual caption based search methods such as Sentence2Vector. We find that Scenario2Vector outperforms Sentence2Vector by 13% ; and is a promising step towards enabling fair comparisons among AVs by inspecting how they perform in similar traffic situations. We hope that Scenario2Vector can have a similar impact to the AV community that Word2Vec/Sent2Vec have had in Natural Language Processing datasets.},
booktitle = {Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems},
pages = {167–176},
numpages = {10},
location = {Nashville, Tennessee},
series = {ICCPS '21}
}

@article{10.1145/3639290,
author = {Rui, Lei and Huang, Xiangdong and Song, Shaoxu and Kang, Yuyuan and Wang, Chen and Wang, Jianmin},
title = {Time Series Representation for Visualization in Apache IoTDB},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639290},
doi = {10.1145/3639290},
abstract = {When analyzing time series, often interactively, the analysts frequently demand to visualize instantly large-scale data stored in databases. M4 visualization selects the first, last, bottom and top data points in each pixel column to ensure pixel-perfectness of the two-color line chart visualization. While M4 already shows its preciseness of encasing time series in different scales into a fixed size of pixels, how to efficiently support M4 representation in a time series native database is still absent. It is worth noting that, to enable fast writes, the commodity time series database systems, such as Apache IoTDB or InfluxDB, employ LSM-Tree based storage. That is, a time series is segmented and stored in a number of chunks, with possibly out-of-order arrivals, i.e., disordered on timestamps. To implement M4, a natural idea is to merge online the chunks as a whole series, with costly merge sort on timestamps, and then perform M4 representation as in relational databases. In this study, we propose a novel chunk merge free approach called M4-LSM to accelerate M4 representation and visualization. In particular, we utilize the metadata of chunks to prune and avoid the costly merging of any chunk. Moreover, intra-chunk indexing and pruning are enabled for efficiently accessing the representation points, referring to the special properties of time series. Remarkably, the time series database native operator M4-LSM has been implemented in Apache IoTDB, an open-source time series database, and deployed in companies across various industries. In the experiments over real-world datasets, the proposed M4-LSM operator demonstrates high efficiency without sacrificing preciseness.},
journal = {Proc. ACM Manag. Data},
month = {mar},
articleno = {35},
numpages = {26},
keywords = {database query processing, time series visualization}
}

@inproceedings{10.1145/3485768.3485769,
author = {Meepung, Tippawan and Nilsook, Prachayanan and Wannapiroon, Panita},
title = {Conceptual Framework for High Performance Digital Entrepreneurial University},
year = {2021},
isbn = {9781450390156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485768.3485769},
doi = {10.1145/3485768.3485769},
abstract = {Digital transformation with university context to create a new performance for educational opportunities for Thais to access new knowledge without restrictions on time – location, digital transformation in the academic sector. It has partnered with all key stakeholders within the institution to make decisions about software and infrastructure and make decisions about changes, transitions, customizations, and upgrades that will drive the best long-term value. High Performance Organization is an organization of excellence. Everyone plays an important role in the drive forward, happening on a personal level. A group or department contributes to achieving the organization's goals. Have a pronounced ability to establish excellent standards of work. For superior competitiveness. The purpose of this research the content divided into two Phases: Phase 1: Synthesize the element of High Performance Digital Entrepreneurial University, Step1) Synthesis from documents, theories, and related research of Digital Transformation Framework, Step 2) Study element of Digital Transformation for High Performance Digital Entrepreneurial University and Step 3) Synthesize the High Performance Organization framework. Phase 2: Develop the conceptual framework of High Performance Digital Entrepreneurial University. The research results as follows; 1. Results Synthesis of Digital Transformation Framework for High Performance Digital Entrepreneurial University, consisting of 1) Strategy, 2) Business Process, 3) Digital Technologies, 4) Organizational, 5) Culture, 6) Stakeholder, and 7) Employee. 2. Results Synthesis of the High Performance Organization framework in The criteria for ranking include 11 areas : 1) Academic reputation, 2) Employer attitude towards graduates, 3) Proportion of teachers/staff per student, 4) International research network, 5) Proportion of research that has been referenced, 6) Proportion of research published, 7) Personnel with Ph.D. qualifications, 8) Proportion of foreign teachers, 9) Proportion of foreign students, 10) Proportion of international students exchanged, and 11) Proportion of foreign students who exchange.},
booktitle = {2021 5th International Conference on E-Society, E-Education and E-Technology},
pages = {47–53},
numpages = {7},
location = {Taipei, Taiwan},
series = {ICSET 2021}
}

@inproceedings{10.1145/3512576.3512642,
author = {Fernando Raguro, Ma. Corazon and Lagman, Ace Carpio and Juanatas, Ronaldo},
title = {Technology Management Framework for Smart University System in the Philippines},
year = {2022},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512642},
doi = {10.1145/3512576.3512642},
abstract = {In this study, the Smart University came out of the concept of smart cities by applying the principles of smart cities to the operation of university. The Smart University is a vision where the University, as a platform, provides foundational context data to deliver the university of the future. To deal with this reality, the researcher developed a framework for transforming a traditional university into a Smart University, which is more efficient, effective, and has more involvement from both students and teachers, all working together to accomplish the shared goal of improved learning. To achieve this objective, the researchers propose to develop a Technology Management Framework that will address issues and challenges to be considered in establishing and adopting a Smart University in the Philippines. The study will also measure the level of preparedness on Smart Universities among the students, teachers, university administrators, and IT consultant and practitioners that serve in different government agencies. Finally, the paper will also address adaptability of universities and to measure the Level of Smartness of Universities in Metro Manila.This study adopts a mixed-method approach that combines qualitative and quantitative research approaches. The authors also used the sequential exploratory design method. The researchers applied the Nominal Group Technique to acquire knowledge from the group of experts for the qualitative portion of the study. Cronbach's alpha is used by the authors for the designed survey instruments that are taken from the Nominal group. Cronbach's alpha is a tool used to evaluate the internal consistency, or reliability, of a collection of scale or test items. The researchers used the Friedman non-parametric hypothesis test for the Level of Smartness ranking.The outcome of the survey instrument uncovers the strength and weaknesses based on the Level of Smartness every university participant. The authors concluded that the developed Technology Management Framework for Smart University System in the Philippines is aligned with the literature and the available systems and applications. These standards are clustered in the three pillars and five criteria, which will guide the academic institutions on establishing Smart University. The three pillars are Technology Infrastructure, Educational Pedagogy, and Government Collaboration together with the five major criteria (Learning Environment, Educational Technology, Academic System, Governance, Health Services) and the application of a roadmap will support the success rate of Smart University in the Philippines. This leads to increasing the efficiency and responsiveness of the academic institution, holistic educational system, dynamic &amp; safer academic community, and exceptional student experience.},
booktitle = {Proceedings of the 2021 9th International Conference on Information Technology: IoT and Smart City},
pages = {372–380},
numpages = {9},
keywords = {Academic System, Cronbach Alpha, Educational Pedagogy, Educational Technology, Governance, Government Collaboration, Health Services, Learning Environment, Nominal Group Technique, Smart University, Smart University Road Map, Smartness Level of Universities, Technology Infrastructure, Technology Management Framework},
location = {Guangzhou, China},
series = {ICIT '21}
}

@inproceedings{10.1145/3356250.3360019,
author = {Li, Zhengxiong and Chen, Baicheng and Yang, Zhuolin and Li, Huining and Xu, Chenhan and Chen, Xingyu and Wang, Kun and Xu, Wenyao},
title = {FerroTag: a paper-based mmWave-scannable tagging infrastructure},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360019},
doi = {10.1145/3356250.3360019},
abstract = {Inventory management is pivotal in the supply chain to supervise the non-capitalized products and stock items. Item counting, indexing and identification are the major jobs of inventory management. Currently, the most adopted inventory technologies in product counting/identification are using either the laser-scannable barcode or the radio-frequency identification (RFID). However, the laser-scannable barcode is entangled by an alignment issue (i.e., the laser reader must align with one barcode in line-of-sight), and the RFID is economically and environmentally unfriendly (i.e., high-cost and not naturally disposable). To this end, we propose FerroTag which is a paper-based mmWave-scannable tagging infrastructure for the next generation inventory management system, featuring ultra-low cost, environment-friendly, battery-free and in-situ (i.e., multiple tags can be simultaneously processed outside the line-of-sight). FerroTag is developed on top of the FerroRF effects. Specifically, the magnetic nanoparticles within the ferrofluidic ink reply to probing mmWave with classifiable features (i.e., the FerroRF response). By designating the ink pattern and hence the location of particles, the related FerroRF response can be modified. Thus, a specifically designated ferrofluidic ink printed pattern, which is associated with a unique FerroRF response, is a remotely retrievable (a.k.a., mmWave-scannable) identity. Furthermore, we augment FerroTag by designing a high capacity pattern system and a fine-grained identification protocol such that the capacity and robustness of FerroTag can be systematically improved in mass product management in inventory. Last but not least, we evaluate the performance of FerroTag with 201 different tag design patterns. Results show that FerroTag can identify tags with an accuracy of more than 99% in a controlled lab setup. Moreover, we examine the reliability, robustness and performance of FerroTag under various real-world circumstances, where FerroTag maintains the accuracy over 97%. Therefore, FerroTag is a promising tagging infrastructure for the applications in inventory management systems.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {324–337},
numpages = {14},
location = {New York, New York},
series = {SenSys '19}
}

@article{10.1145/3559104,
author = {Bitton, Ron and Maman, Nadav and Singh, Inderjeet and Momiyama, Satoru and Elovici, Yuval and Shabtai, Asaf},
title = {Evaluating the Cybersecurity Risk of Real-world, Machine Learning Production Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3559104},
doi = {10.1145/3559104},
abstract = {Although cyberattacks on machine learning (ML) production systems can be harmful, today, security practitioners are ill-equipped, lacking methodologies and tactical tools that would allow them to analyze the security risks of their ML-based systems. In this article, we perform a comprehensive threat analysis of ML production systems. In this analysis, we follow the ontology presented by NIST for evaluating enterprise network security risk and apply it to ML-based production systems. Specifically, we (1) enumerate the assets of a typical ML production system, (2) describe the threat model (i.e., potential adversaries, their capabilities, and their main goal), (3) identify the various threats to ML systems, and (4) review a large number of attacks, demonstrated in previous studies, which can realize these threats. To quantify the risk posed by adversarial machine learning (AML) threat, we introduce a novel scoring system that assigns a severity score to different AML attacks. The proposed scoring system utilizes the analytic hierarchy process (AHP) for ranking—with the assistance of security experts—various attributes of the attacks. Finally, we developed an extension to the MulVAL attack graph generation and analysis framework to incorporate cyberattacks on ML production systems. Using this extension, security practitioners can apply attack graph analysis methods in environments that include ML components thus providing security practitioners with a methodological and practical tool for both evaluating the impact and quantifying the risk of a cyberattack targeting ML production systems.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {183},
numpages = {36},
keywords = {Adversarial machine learning, attack graphs, threat analysis, risk assessment}
}

@inproceedings{10.1145/3576841.3585925,
author = {Gao, Qitong and Schmidt, Stephen L. and Chowdhury, Afsana and Feng, Guangyu and Peters, Jennifer J. and Genty, Katherine and Grill, Warren M. and Turner, Dennis A. and Pajic, Miroslav},
title = {Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment},
year = {2023},
isbn = {9798400700361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576841.3585925},
doi = {10.1145/3576841.3585925},
abstract = {Deep brain stimulation (DBS) has shown great promise toward treating motor symptoms caused by Parkinson's disease (PD), by delivering electrical pulses to the Basal Ganglia (BG) region of the brain. However, DBS devices approved by the U.S. Food and Drug Administration (FDA) can only deliver continuous DBS (cDBS) at a fixed amplitude; this energy inefficient operation reduces battery lifetime of the device, cannot adapt treatment dynamically for activity, and may cause significant side-effects (e.g., gait impairment). In this work, we introduce an offline reinforcement learning (RL) framework, allowing the use of past clinical data to train an RL policy to adjust the stimulation amplitude in real time, with the goal of reducing energy use while maintaining the same level of treatment (i.e., control) efficacy as cDBS. Moreover, clinical protocols require the safety and performance of such RL controllers to be demonstrated ahead of deployments in patients. Thus, we also introduce an offline policy evaluation (OPE) method to estimate the performance of RL policies using historical data, before deploying them on patients. We evaluated our framework on four PD patients equipped with the RC+S DBS system, employing the RL controllers during monthly clinical visits, with the overall control efficacy evaluated by severity of symptoms (i.e., bradykinesia and tremor), changes in PD biomakers (i.e., local field potentials), and patient ratings. The results from clinical experiments show that our RL-based controller maintains the same level of control efficacy as cDBS, but with significantly reduced stimulation energy. Further, the OPE method is shown effective in accurately estimating and ranking the expected returns of RL controllers.},
booktitle = {Proceedings of the ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2023)},
pages = {44–55},
numpages = {12},
keywords = {deep brain stimulation, offline reinforcement learning, offline policy evaluation},
location = {San Antonio, TX, USA},
series = {ICCPS '23}
}

@inproceedings{10.1145/3210259.3210261,
author = {Deshpande, Amol},
title = {In situ graph querying and analytics with graphgen: extended abstract},
year = {2018},
isbn = {9781450356954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210259.3210261},
doi = {10.1145/3210259.3210261},
abstract = {After several decades of research but limited adoption in practice, graph querying and analytics are finally starting to gain a foothold in the data management landscape. This is driven to a large degree by the increasing desire to model and query the key entities and the interconnections between them explicitly; and the observation that the use of network science, graph algorithms, and graph mining can lead to crucial new insights that are not accessible without reasoning about those interconnections in a holistic and collective manner. In addition to the traditional application domains like social media, Web, biological networks, and RDF knowledge bases, graph data models are a natural fit for new and important application domains like personal data management, provenance, metadata management, machine learning, and others. Even data that is not naturally graph-structured is increasingly viewed from the graph lense, examples being shopping transaction data, healthcare data, source code repositories, parcel shipment data, etc.This increased attention has led to much recent work on specialized graph databases (e.g., Neo4j, Titan, OrientDB, Dgraph, Blaze-graph, Amazon Neptune), and graph analysis frameworks (e.g., Giraph, GraphLab, Ligra, GraphX, and numerous others [?]). Several established relational databases have added support for limited forms of graph querying and analytics including SAP HANA, Oracle, Aster Data, and SQL Server. However, there is no clear unifying or overarching theme that connects these systems and other research on graph querying and analytics, resulting in a highly fragmented landscape. We contend that the key reasons for this are two-fold. First, &lt;u&gt;graph querying/analytics is typically a small part of the overall data management process&lt;/u&gt;, and many of the solutions (especially graph databases) require complete buy-in so that the data can be stored in an appropriate graph-aware format and appropriate indexes can be built. Second, the &lt;u&gt;querying or analysis workloads typically considered to be within the scope of graph data management are highly varied&lt;/u&gt;, and include point queries (e.g., pattern matching, reachability, shortest paths), network science (e.g., community detection, centrality), graph mining (e.g., influence propagation, similarity-based ranking), graph algorithms (e.g., bipartite matching, min-cut), temporal analytics (e.g., network evolution), "what-if" analytics (e.g., vulnerability analysis), among others. As a result, many users prefer crafting custom solutions to solve specific problems in the context of their environments, that use custom data structures that are not reusable or standardized.In this talk, I will present our vision for an in situ graph querying and analytics framework, called GraphGen, that uses the familiar abstraction of "views" or "data virtualization" to: (a) construct graphs by combining data from a number of heterogeneous data sources, and (b) query and analyze them using powerful, flexible APIs. I will discuss how this framework can serve as a unifying abstraction that covers the spectrum of use cases and approaches above. Our focus in the work done so far has been on the common scenario where the data originally resides in an RDBMS [? ? ? ]; I will describe our prototype that enables users to declaratively specify graph extraction tasks over such data, visually explore the extracted graphs, and write and execute graph algorithms over them, either directly or using existing graph libraries like the NetworkX Python library. GraphGen has a fundamentally different goal from recent work on using RDBMSs to store graph data through "shredding". Instead, GraphGen is intended to analyze "hidden" graphs that are present in existing databases (relational or not). GraphGen attempts to utilize the underlying systems to the full extent possible by pushing down computation, uses a novel condensed representation to handle graphs that may be too large to extract in their entirety, allows writing programs using a general subgraph-centric API, and features several optimizations for efficient extraction and querying of large graphs. I will conclude with a discussion of new optimization opportunities and open challenges.},
booktitle = {Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {2},
numpages = {2},
keywords = {graph analytics, graph databases},
location = {Houston, Texas},
series = {GRADES-NDA '18}
}

